GPU available: True
Carga datasets
	Tiny Shakespeare
	Wikitext-2
Longitud corpus train: 896135 caracteres
Longitud corpus valid: 100448 caracteres
Longitud corpus test: 111570 caracteres
Tokenización del corpus de train, test y validation
Número total de tokens en el corpus: 265566
Tokenización del corpus de train, test y validation
Número total de tokens en el corpus: 30318
Preparando DataLoader
Se invoca el modelo
TRAIN
Iteracciones: 4146
  [Batch 100] Loss: 6.2514 | LR: 1.499995e-03
  [Batch 200] Loss: 6.2496 | LR: 1.499978e-03
  [Batch 300] Loss: 6.2453 | LR: 1.499952e-03
  [Batch 400] Loss: 6.2425 | LR: 1.499914e-03
  [Batch 500] Loss: 6.2250 | LR: 1.499865e-03
  [Batch 600] Loss: 6.2409 | LR: 1.499806e-03
  [Batch 700] Loss: 6.2378 | LR: 1.499736e-03
  [Batch 800] Loss: 6.2279 | LR: 1.499656e-03
  [Batch 900] Loss: 6.2173 | LR: 1.499564e-03
  [Batch 1000] Loss: 6.2434 | LR: 1.499462e-03
  [Batch 1100] Loss: 6.2465 | LR: 1.499349e-03
  [Batch 1200] Loss: 6.2322 | LR: 1.499225e-03
  [Batch 1300] Loss: 6.2064 | LR: 1.499090e-03
  [Batch 1400] Loss: 6.2087 | LR: 1.498945e-03
  [Batch 1500] Loss: 6.2241 | LR: 1.498789e-03
  [Batch 1600] Loss: 6.2287 | LR: 1.498622e-03
  [Batch 1700] Loss: 6.2101 | LR: 1.498445e-03
  [Batch 1800] Loss: 6.1939 | LR: 1.498257e-03
  [Batch 1900] Loss: 6.2343 | LR: 1.498058e-03
  [Batch 2000] Loss: 6.2260 | LR: 1.497848e-03
  [Batch 2100] Loss: 6.2177 | LR: 1.497627e-03
  [Batch 2200] Loss: 6.2388 | LR: 1.497396e-03
  [Batch 2300] Loss: 6.2040 | LR: 1.497154e-03
  [Batch 2400] Loss: 6.2280 | LR: 1.496902e-03
  [Batch 2500] Loss: 6.1919 | LR: 1.496638e-03
  [Batch 2600] Loss: 6.2167 | LR: 1.496364e-03
  [Batch 2700] Loss: 6.1969 | LR: 1.496079e-03
  [Batch 2800] Loss: 6.2349 | LR: 1.495784e-03
  [Batch 2900] Loss: 6.2214 | LR: 1.495478e-03
  [Batch 3000] Loss: 6.2072 | LR: 1.495161e-03
  [Batch 3100] Loss: 6.2261 | LR: 1.494833e-03
  [Batch 3200] Loss: 6.2074 | LR: 1.494495e-03
  [Batch 3300] Loss: 6.2143 | LR: 1.494146e-03
  [Batch 3400] Loss: 6.2338 | LR: 1.493786e-03
  [Batch 3500] Loss: 6.2252 | LR: 1.493416e-03
  [Batch 3600] Loss: 6.2179 | LR: 1.493035e-03
  [Batch 3700] Loss: 6.2071 | LR: 1.492643e-03
  [Batch 3800] Loss: 6.2351 | LR: 1.492241e-03
  [Batch 3900] Loss: 6.2214 | LR: 1.491828e-03
  [Batch 4000] Loss: 6.2052 | LR: 1.491404e-03
  [Batch 4100] Loss: 6.2098 | LR: 1.490970e-03
[Epoch 1] Train Loss: 6.2343 | Train PPL: 509.94 | Val PPL: 586.34
Iteracciones: 4146
  [Batch 100] Loss: 6.2495 | LR: 1.490316e-03
  [Batch 200] Loss: 6.2263 | LR: 1.489856e-03
  [Batch 300] Loss: 6.2344 | LR: 1.489385e-03
  [Batch 400] Loss: 6.2114 | LR: 1.488903e-03
  [Batch 500] Loss: 6.2047 | LR: 1.488411e-03
  [Batch 600] Loss: 6.2100 | LR: 1.487908e-03
  [Batch 700] Loss: 6.2455 | LR: 1.487395e-03
  [Batch 800] Loss: 6.2426 | LR: 1.486870e-03
  [Batch 900] Loss: 6.2231 | LR: 1.486336e-03
  [Batch 1000] Loss: 6.2103 | LR: 1.485791e-03
  [Batch 1100] Loss: 6.2151 | LR: 1.485235e-03
  [Batch 1200] Loss: 6.2201 | LR: 1.484668e-03
  [Batch 1300] Loss: 6.2332 | LR: 1.484092e-03
  [Batch 1400] Loss: 6.2416 | LR: 1.483504e-03
  [Batch 1500] Loss: 6.2139 | LR: 1.482906e-03
  [Batch 1600] Loss: 6.2381 | LR: 1.482298e-03
  [Batch 1700] Loss: 6.2339 | LR: 1.481679e-03
  [Batch 1800] Loss: 6.2259 | LR: 1.481049e-03
  [Batch 1900] Loss: 6.2405 | LR: 1.480409e-03
  [Batch 2000] Loss: 6.2351 | LR: 1.479759e-03
  [Batch 2100] Loss: 6.2061 | LR: 1.479098e-03
  [Batch 2200] Loss: 6.2545 | LR: 1.478427e-03
  [Batch 2300] Loss: 6.2078 | LR: 1.477745e-03
  [Batch 2400] Loss: 6.2284 | LR: 1.477052e-03
  [Batch 2500] Loss: 6.2155 | LR: 1.476350e-03
  [Batch 2600] Loss: 6.2546 | LR: 1.475637e-03
  [Batch 2700] Loss: 6.2626 | LR: 1.474913e-03
  [Batch 2800] Loss: 6.2224 | LR: 1.474179e-03
  [Batch 2900] Loss: 6.2374 | LR: 1.473435e-03
  [Batch 3000] Loss: 6.1935 | LR: 1.472680e-03
  [Batch 3100] Loss: 6.2550 | LR: 1.471915e-03
  [Batch 3200] Loss: 6.2127 | LR: 1.471139e-03
  [Batch 3300] Loss: 6.2396 | LR: 1.470353e-03
  [Batch 3400] Loss: 6.2167 | LR: 1.469557e-03
  [Batch 3500] Loss: 6.2333 | LR: 1.468751e-03
  [Batch 3600] Loss: 6.2291 | LR: 1.467934e-03
  [Batch 3700] Loss: 6.2269 | LR: 1.467107e-03
  [Batch 3800] Loss: 6.2255 | LR: 1.466269e-03
  [Batch 3900] Loss: 6.2200 | LR: 1.465421e-03
  [Batch 4000] Loss: 6.2001 | LR: 1.464563e-03
  [Batch 4100] Loss: 6.2527 | LR: 1.463695e-03
[Epoch 2] Train Loss: 6.2259 | Train PPL: 505.67 | Val PPL: 590.95
Iteracciones: 4146
  [Batch 100] Loss: 6.2078 | LR: 1.462409e-03
  [Batch 200] Loss: 6.2155 | LR: 1.461516e-03
  [Batch 300] Loss: 6.2298 | LR: 1.460612e-03
  [Batch 400] Loss: 6.2328 | LR: 1.459698e-03
  [Batch 500] Loss: 6.2511 | LR: 1.458774e-03
  [Batch 600] Loss: 6.2325 | LR: 1.457840e-03
  [Batch 700] Loss: 6.2306 | LR: 1.456896e-03
  [Batch 800] Loss: 6.2383 | LR: 1.455941e-03
  [Batch 900] Loss: 6.2364 | LR: 1.454977e-03
  [Batch 1000] Loss: 6.2289 | LR: 1.454002e-03
  [Batch 1100] Loss: 6.2293 | LR: 1.453017e-03
  [Batch 1200] Loss: 6.2201 | LR: 1.452022e-03
  [Batch 1300] Loss: 6.2049 | LR: 1.451017e-03
  [Batch 1400] Loss: 6.2217 | LR: 1.450002e-03
  [Batch 1500] Loss: 6.2590 | LR: 1.448977e-03
  [Batch 1600] Loss: 6.2093 | LR: 1.447942e-03
  [Batch 1700] Loss: 6.2149 | LR: 1.446896e-03
  [Batch 1800] Loss: 6.2238 | LR: 1.445841e-03
  [Batch 1900] Loss: 6.2283 | LR: 1.444776e-03
  [Batch 2000] Loss: 6.2237 | LR: 1.443701e-03
  [Batch 2100] Loss: 6.2449 | LR: 1.442616e-03
  [Batch 2200] Loss: 6.2085 | LR: 1.441521e-03
  [Batch 2300] Loss: 6.2100 | LR: 1.440416e-03
  [Batch 2400] Loss: 6.2175 | LR: 1.439301e-03
  [Batch 2500] Loss: 6.2073 | LR: 1.438176e-03
  [Batch 2600] Loss: 6.2344 | LR: 1.437041e-03
  [Batch 2700] Loss: 6.2449 | LR: 1.435897e-03
  [Batch 2800] Loss: 6.2303 | LR: 1.434742e-03
  [Batch 2900] Loss: 6.2168 | LR: 1.433578e-03
  [Batch 3000] Loss: 6.2272 | LR: 1.432404e-03
  [Batch 3100] Loss: 6.2467 | LR: 1.431220e-03
  [Batch 3200] Loss: 6.2155 | LR: 1.430027e-03
  [Batch 3300] Loss: 6.2444 | LR: 1.428823e-03
  [Batch 3400] Loss: 6.2342 | LR: 1.427610e-03
  [Batch 3500] Loss: 6.2126 | LR: 1.426387e-03
  [Batch 3600] Loss: 6.2129 | LR: 1.425155e-03
  [Batch 3700] Loss: 6.2218 | LR: 1.423913e-03
  [Batch 3800] Loss: 6.2376 | LR: 1.422661e-03
  [Batch 3900] Loss: 6.2162 | LR: 1.421399e-03
  [Batch 4000] Loss: 6.2218 | LR: 1.420128e-03
  [Batch 4100] Loss: 6.2312 | LR: 1.418847e-03
[Epoch 3] Train Loss: 6.2248 | Train PPL: 505.13 | Val PPL: 595.08
Iteracciones: 4146
  [Batch 100] Loss: 6.2142 | LR: 1.416960e-03
  [Batch 200] Loss: 6.2197 | LR: 1.415656e-03
  [Batch 300] Loss: 6.2231 | LR: 1.414342e-03
  [Batch 400] Loss: 6.2301 | LR: 1.413018e-03
  [Batch 500] Loss: 6.2142 | LR: 1.411685e-03
  [Batch 600] Loss: 6.2097 | LR: 1.410343e-03
  [Batch 700] Loss: 6.2222 | LR: 1.408991e-03
  [Batch 800] Loss: 6.1956 | LR: 1.407629e-03
  [Batch 900] Loss: 6.2480 | LR: 1.406258e-03
  [Batch 1000] Loss: 6.2346 | LR: 1.404878e-03
  [Batch 1100] Loss: 6.2246 | LR: 1.403488e-03
  [Batch 1200] Loss: 6.2158 | LR: 1.402089e-03
  [Batch 1300] Loss: 6.2372 | LR: 1.400681e-03
  [Batch 1400] Loss: 6.2297 | LR: 1.399263e-03
  [Batch 1500] Loss: 6.2116 | LR: 1.397836e-03
  [Batch 1600] Loss: 6.2321 | LR: 1.396400e-03
  [Batch 1700] Loss: 6.2621 | LR: 1.394954e-03
  [Batch 1800] Loss: 6.2021 | LR: 1.393499e-03
  [Batch 1900] Loss: 6.2373 | LR: 1.392035e-03
  [Batch 2000] Loss: 6.2345 | LR: 1.390562e-03
  [Batch 2100] Loss: 6.2186 | LR: 1.389079e-03
  [Batch 2200] Loss: 6.2329 | LR: 1.387587e-03
  [Batch 2300] Loss: 6.1994 | LR: 1.386086e-03
  [Batch 2400] Loss: 6.2068 | LR: 1.384576e-03
  [Batch 2500] Loss: 6.2252 | LR: 1.383057e-03
  [Batch 2600] Loss: 6.2300 | LR: 1.381529e-03
  [Batch 2700] Loss: 6.2172 | LR: 1.379992e-03
  [Batch 2800] Loss: 6.2384 | LR: 1.378445e-03
  [Batch 2900] Loss: 6.2325 | LR: 1.376890e-03
  [Batch 3000] Loss: 6.2282 | LR: 1.375325e-03
  [Batch 3100] Loss: 6.2340 | LR: 1.373752e-03
  [Batch 3200] Loss: 6.2350 | LR: 1.372170e-03
  [Batch 3300] Loss: 6.2383 | LR: 1.370579e-03
  [Batch 3400] Loss: 6.2113 | LR: 1.368979e-03
  [Batch 3500] Loss: 6.2191 | LR: 1.367369e-03
  [Batch 3600] Loss: 6.2329 | LR: 1.365752e-03
  [Batch 3700] Loss: 6.2099 | LR: 1.364125e-03
  [Batch 3800] Loss: 6.2463 | LR: 1.362489e-03
  [Batch 3900] Loss: 6.2067 | LR: 1.360845e-03
  [Batch 4000] Loss: 6.2564 | LR: 1.359192e-03
  [Batch 4100] Loss: 6.2281 | LR: 1.357530e-03
[Epoch 4] Train Loss: 6.2242 | Train PPL: 504.81 | Val PPL: 595.86
Iteracciones: 4146
  [Batch 100] Loss: 6.2346 | LR: 1.355088e-03
  [Batch 200] Loss: 6.2326 | LR: 1.353405e-03
  [Batch 300] Loss: 6.2328 | LR: 1.351713e-03
  [Batch 400] Loss: 6.2196 | LR: 1.350012e-03
  [Batch 500] Loss: 6.2096 | LR: 1.348303e-03
  [Batch 600] Loss: 6.2403 | LR: 1.346586e-03
  [Batch 700] Loss: 6.2446 | LR: 1.344859e-03
  [Batch 800] Loss: 6.2329 | LR: 1.343124e-03
  [Batch 900] Loss: 6.2333 | LR: 1.341381e-03
  [Batch 1000] Loss: 6.2174 | LR: 1.339629e-03
  [Batch 1100] Loss: 6.2238 | LR: 1.337869e-03
  [Batch 1200] Loss: 6.2416 | LR: 1.336100e-03
  [Batch 1300] Loss: 6.2317 | LR: 1.334323e-03
  [Batch 1400] Loss: 6.2250 | LR: 1.332537e-03
  [Batch 1500] Loss: 6.2499 | LR: 1.330744e-03
  [Batch 1600] Loss: 6.2272 | LR: 1.328941e-03
  [Batch 1700] Loss: 6.2379 | LR: 1.327131e-03
  [Batch 1800] Loss: 6.2489 | LR: 1.325312e-03
  [Batch 1900] Loss: 6.2034 | LR: 1.323485e-03
  [Batch 2000] Loss: 6.2012 | LR: 1.321649e-03
  [Batch 2100] Loss: 6.1972 | LR: 1.319806e-03
  [Batch 2200] Loss: 6.2461 | LR: 1.317954e-03
  [Batch 2300] Loss: 6.2488 | LR: 1.316094e-03
  [Batch 2400] Loss: 6.2261 | LR: 1.314226e-03
  [Batch 2500] Loss: 6.2501 | LR: 1.312350e-03
  [Batch 2600] Loss: 6.2196 | LR: 1.310466e-03
  [Batch 2700] Loss: 6.2193 | LR: 1.308574e-03
  [Batch 2800] Loss: 6.2287 | LR: 1.306674e-03
  [Batch 2900] Loss: 6.2122 | LR: 1.304765e-03
  [Batch 3000] Loss: 6.2354 | LR: 1.302849e-03
  [Batch 3100] Loss: 6.2341 | LR: 1.300925e-03
  [Batch 3200] Loss: 6.2332 | LR: 1.298993e-03
  [Batch 3300] Loss: 6.2327 | LR: 1.297053e-03
  [Batch 3400] Loss: 6.2203 | LR: 1.295105e-03
  [Batch 3500] Loss: 6.2038 | LR: 1.293150e-03
  [Batch 3600] Loss: 6.2224 | LR: 1.291186e-03
  [Batch 3700] Loss: 6.2649 | LR: 1.289215e-03
  [Batch 3800] Loss: 6.2355 | LR: 1.287236e-03
  [Batch 3900] Loss: 6.2109 | LR: 1.285250e-03
  [Batch 4000] Loss: 6.2257 | LR: 1.283255e-03
  [Batch 4100] Loss: 6.2308 | LR: 1.281254e-03
[Epoch 5] Train Loss: 6.2238 | Train PPL: 504.63 | Val PPL: 596.43
Iteracciones: 4146
  [Batch 100] Loss: 6.2489 | LR: 1.278317e-03
  [Batch 200] Loss: 6.2350 | LR: 1.276296e-03
  [Batch 300] Loss: 6.2082 | LR: 1.274268e-03
  [Batch 400] Loss: 6.2194 | LR: 1.272232e-03
  [Batch 500] Loss: 6.2242 | LR: 1.270189e-03
  [Batch 600] Loss: 6.2472 | LR: 1.268139e-03
  [Batch 700] Loss: 6.1936 | LR: 1.266080e-03
  [Batch 800] Loss: 6.2283 | LR: 1.264015e-03
  [Batch 900] Loss: 6.2323 | LR: 1.261942e-03
  [Batch 1000] Loss: 6.2271 | LR: 1.259862e-03
  [Batch 1100] Loss: 6.2306 | LR: 1.257774e-03
  [Batch 1200] Loss: 6.2347 | LR: 1.255679e-03
  [Batch 1300] Loss: 6.2287 | LR: 1.253577e-03
  [Batch 1400] Loss: 6.1844 | LR: 1.251468e-03
  [Batch 1500] Loss: 6.2050 | LR: 1.249351e-03
  [Batch 1600] Loss: 6.2266 | LR: 1.247227e-03
  [Batch 1700] Loss: 6.2327 | LR: 1.245097e-03
  [Batch 1800] Loss: 6.2365 | LR: 1.242959e-03
  [Batch 1900] Loss: 6.2317 | LR: 1.240814e-03
  [Batch 2000] Loss: 6.2284 | LR: 1.238661e-03
  [Batch 2100] Loss: 6.2254 | LR: 1.236502e-03
  [Batch 2200] Loss: 6.2163 | LR: 1.234336e-03
  [Batch 2300] Loss: 6.2341 | LR: 1.232163e-03
  [Batch 2400] Loss: 6.2280 | LR: 1.229983e-03
  [Batch 2500] Loss: 6.2182 | LR: 1.227796e-03
  [Batch 2600] Loss: 6.2368 | LR: 1.225603e-03
  [Batch 2700] Loss: 6.2447 | LR: 1.223402e-03
  [Batch 2800] Loss: 6.2138 | LR: 1.221195e-03
  [Batch 2900] Loss: 6.2265 | LR: 1.218981e-03
  [Batch 3000] Loss: 6.2505 | LR: 1.216760e-03
  [Batch 3100] Loss: 6.2389 | LR: 1.214532e-03
  [Batch 3200] Loss: 6.2254 | LR: 1.212298e-03
  [Batch 3300] Loss: 6.2099 | LR: 1.210057e-03
  [Batch 3400] Loss: 6.2090 | LR: 1.207810e-03
  [Batch 3500] Loss: 6.2239 | LR: 1.205556e-03
  [Batch 3600] Loss: 6.2242 | LR: 1.203295e-03
  [Batch 3700] Loss: 6.2121 | LR: 1.201028e-03
  [Batch 3800] Loss: 6.2084 | LR: 1.198755e-03
  [Batch 3900] Loss: 6.2366 | LR: 1.196475e-03
  [Batch 4000] Loss: 6.1771 | LR: 1.194188e-03
  [Batch 4100] Loss: 6.2167 | LR: 1.191896e-03
[Epoch 6] Train Loss: 6.2236 | Train PPL: 504.50 | Val PPL: 596.64
Iteracciones: 4146
  [Batch 100] Loss: 6.2323 | LR: 1.188537e-03
  [Batch 200] Loss: 6.2319 | LR: 1.186229e-03
  [Batch 300] Loss: 6.2269 | LR: 1.183914e-03
  [Batch 400] Loss: 6.2046 | LR: 1.181593e-03
  [Batch 500] Loss: 6.2081 | LR: 1.179266e-03
  [Batch 600] Loss: 6.2089 | LR: 1.176933e-03
  [Batch 700] Loss: 6.2302 | LR: 1.174594e-03
  [Batch 800] Loss: 6.2242 | LR: 1.172249e-03
  [Batch 900] Loss: 6.2248 | LR: 1.169897e-03
  [Batch 1000] Loss: 6.2159 | LR: 1.167540e-03
  [Batch 1100] Loss: 6.1897 | LR: 1.165176e-03
  [Batch 1200] Loss: 6.2086 | LR: 1.162807e-03
  [Batch 1300] Loss: 6.2244 | LR: 1.160431e-03
  [Batch 1400] Loss: 6.2502 | LR: 1.158050e-03
  [Batch 1500] Loss: 6.2014 | LR: 1.155663e-03
  [Batch 1600] Loss: 6.2092 | LR: 1.153270e-03
  [Batch 1700] Loss: 6.2080 | LR: 1.150872e-03
  [Batch 1800] Loss: 6.2299 | LR: 1.148467e-03
  [Batch 1900] Loss: 6.2125 | LR: 1.146057e-03
  [Batch 2000] Loss: 6.2067 | LR: 1.143641e-03
  [Batch 2100] Loss: 6.2184 | LR: 1.141220e-03
  [Batch 2200] Loss: 6.2266 | LR: 1.138792e-03
  [Batch 2300] Loss: 6.2102 | LR: 1.136360e-03
  [Batch 2400] Loss: 6.2114 | LR: 1.133921e-03
  [Batch 2500] Loss: 6.2171 | LR: 1.131478e-03
  [Batch 2600] Loss: 6.2282 | LR: 1.129029e-03
  [Batch 2700] Loss: 6.2335 | LR: 1.126574e-03
  [Batch 2800] Loss: 6.2337 | LR: 1.124114e-03
  [Batch 2900] Loss: 6.2270 | LR: 1.121648e-03
  [Batch 3000] Loss: 6.2285 | LR: 1.119178e-03
  [Batch 3100] Loss: 6.2216 | LR: 1.116701e-03
  [Batch 3200] Loss: 6.2260 | LR: 1.114220e-03
  [Batch 3300] Loss: 6.2227 | LR: 1.111734e-03
  [Batch 3400] Loss: 6.2174 | LR: 1.109242e-03
  [Batch 3500] Loss: 6.2610 | LR: 1.106745e-03
  [Batch 3600] Loss: 6.2165 | LR: 1.104243e-03
  [Batch 3700] Loss: 6.2018 | LR: 1.101736e-03
  [Batch 3800] Loss: 6.2510 | LR: 1.099223e-03
  [Batch 3900] Loss: 6.2281 | LR: 1.096706e-03
  [Batch 4000] Loss: 6.1873 | LR: 1.094184e-03
  [Batch 4100] Loss: 6.2250 | LR: 1.091657e-03
[Epoch 7] Train Loss: 6.2234 | Train PPL: 504.39 | Val PPL: 597.50
Iteracciones: 4146
  [Batch 100] Loss: 6.1961 | LR: 1.087959e-03
  [Batch 200] Loss: 6.2331 | LR: 1.085420e-03
  [Batch 300] Loss: 6.2442 | LR: 1.082876e-03
  [Batch 400] Loss: 6.2197 | LR: 1.080327e-03
  [Batch 500] Loss: 6.2574 | LR: 1.077773e-03
  [Batch 600] Loss: 6.2227 | LR: 1.075215e-03
  [Batch 700] Loss: 6.2261 | LR: 1.072652e-03
  [Batch 800] Loss: 6.2012 | LR: 1.070085e-03
  [Batch 900] Loss: 6.2142 | LR: 1.067513e-03
  [Batch 1000] Loss: 6.2170 | LR: 1.064936e-03
  [Batch 1100] Loss: 6.2321 | LR: 1.062355e-03
  [Batch 1200] Loss: 6.2477 | LR: 1.059770e-03
  [Batch 1300] Loss: 6.2366 | LR: 1.057180e-03
  [Batch 1400] Loss: 6.2144 | LR: 1.054585e-03
  [Batch 1500] Loss: 6.2444 | LR: 1.051986e-03
  [Batch 1600] Loss: 6.2221 | LR: 1.049383e-03
  [Batch 1700] Loss: 6.2257 | LR: 1.046776e-03
  [Batch 1800] Loss: 6.2186 | LR: 1.044164e-03
  [Batch 1900] Loss: 6.2407 | LR: 1.041548e-03
  [Batch 2000] Loss: 6.1961 | LR: 1.038928e-03
  [Batch 2100] Loss: 6.2036 | LR: 1.036304e-03
  [Batch 2200] Loss: 6.2328 | LR: 1.033675e-03
  [Batch 2300] Loss: 6.2088 | LR: 1.031043e-03
  [Batch 2400] Loss: 6.2297 | LR: 1.028406e-03
  [Batch 2500] Loss: 6.2285 | LR: 1.025766e-03
  [Batch 2600] Loss: 6.1986 | LR: 1.023121e-03
  [Batch 2700] Loss: 6.2149 | LR: 1.020473e-03
  [Batch 2800] Loss: 6.1946 | LR: 1.017821e-03
  [Batch 2900] Loss: 6.2293 | LR: 1.015165e-03
  [Batch 3000] Loss: 6.2260 | LR: 1.012505e-03
  [Batch 3100] Loss: 6.2353 | LR: 1.009841e-03
  [Batch 3200] Loss: 6.2193 | LR: 1.007174e-03
  [Batch 3300] Loss: 6.2388 | LR: 1.004503e-03
  [Batch 3400] Loss: 6.2236 | LR: 1.001828e-03
  [Batch 3500] Loss: 6.2260 | LR: 9.991494e-04
  [Batch 3600] Loss: 6.2597 | LR: 9.964675e-04
  [Batch 3700] Loss: 6.2159 | LR: 9.937820e-04
  [Batch 3800] Loss: 6.2192 | LR: 9.910931e-04
  [Batch 3900] Loss: 6.2198 | LR: 9.884006e-04
  [Batch 4000] Loss: 6.2191 | LR: 9.857048e-04
  [Batch 4100] Loss: 6.2416 | LR: 9.830055e-04
[Epoch 8] Train Loss: 6.2231 | Train PPL: 504.28 | Val PPL: 597.60
Iteracciones: 4146
  [Batch 100] Loss: 6.2428 | LR: 9.790586e-04
  [Batch 200] Loss: 6.2285 | LR: 9.763512e-04
  [Batch 300] Loss: 6.2193 | LR: 9.736406e-04
  [Batch 400] Loss: 6.2305 | LR: 9.709267e-04
  [Batch 500] Loss: 6.2273 | LR: 9.682097e-04
  [Batch 600] Loss: 6.2126 | LR: 9.654895e-04
  [Batch 700] Loss: 6.2226 | LR: 9.627663e-04
  [Batch 800] Loss: 6.2222 | LR: 9.600400e-04
  [Batch 900] Loss: 6.2600 | LR: 9.573107e-04
  [Batch 1000] Loss: 6.2178 | LR: 9.545784e-04
  [Batch 1100] Loss: 6.2294 | LR: 9.518431e-04
  [Batch 1200] Loss: 6.2330 | LR: 9.491050e-04
  [Batch 1300] Loss: 6.2264 | LR: 9.463640e-04
  [Batch 1400] Loss: 6.2027 | LR: 9.436202e-04
  [Batch 1500] Loss: 6.2137 | LR: 9.408736e-04
  [Batch 1600] Loss: 6.2292 | LR: 9.381243e-04
  [Batch 1700] Loss: 6.2003 | LR: 9.353722e-04
  [Batch 1800] Loss: 6.2012 | LR: 9.326176e-04
  [Batch 1900] Loss: 6.2260 | LR: 9.298602e-04
  [Batch 2000] Loss: 6.2400 | LR: 9.271004e-04
  [Batch 2100] Loss: 6.2016 | LR: 9.243379e-04
  [Batch 2200] Loss: 6.2381 | LR: 9.215730e-04
  [Batch 2300] Loss: 6.2484 | LR: 9.188056e-04
  [Batch 2400] Loss: 6.2650 | LR: 9.160358e-04
  [Batch 2500] Loss: 6.2296 | LR: 9.132635e-04
  [Batch 2600] Loss: 6.2187 | LR: 9.104890e-04
  [Batch 2700] Loss: 6.2253 | LR: 9.077121e-04
  [Batch 2800] Loss: 6.2158 | LR: 9.049330e-04
  [Batch 2900] Loss: 6.2276 | LR: 9.021517e-04
  [Batch 3000] Loss: 6.2235 | LR: 8.993682e-04
  [Batch 3100] Loss: 6.2247 | LR: 8.965825e-04
  [Batch 3200] Loss: 6.2139 | LR: 8.937947e-04
  [Batch 3300] Loss: 6.1885 | LR: 8.910049e-04
  [Batch 3400] Loss: 6.2180 | LR: 8.882130e-04
  [Batch 3500] Loss: 6.2384 | LR: 8.854192e-04
  [Batch 3600] Loss: 6.2502 | LR: 8.826234e-04
  [Batch 3700] Loss: 6.2210 | LR: 8.798257e-04
  [Batch 3800] Loss: 6.2118 | LR: 8.770261e-04
  [Batch 3900] Loss: 6.2341 | LR: 8.742247e-04
  [Batch 4000] Loss: 6.2211 | LR: 8.714216e-04
  [Batch 4100] Loss: 6.2154 | LR: 8.686167e-04
[Epoch 9] Train Loss: 6.2230 | Train PPL: 504.19 | Val PPL: 597.15
Iteracciones: 4146
  [Batch 100] Loss: 6.1731 | LR: 8.645185e-04
  [Batch 200] Loss: 6.2602 | LR: 8.617094e-04
  [Batch 300] Loss: 6.2000 | LR: 8.588988e-04
  [Batch 400] Loss: 6.2269 | LR: 8.560866e-04
  [Batch 500] Loss: 6.2424 | LR: 8.532729e-04
  [Batch 600] Loss: 6.2224 | LR: 8.504577e-04
  [Batch 700] Loss: 6.2291 | LR: 8.476411e-04
  [Batch 800] Loss: 6.2235 | LR: 8.448231e-04
  [Batch 900] Loss: 6.2185 | LR: 8.420037e-04
  [Batch 1000] Loss: 6.2133 | LR: 8.391829e-04
  [Batch 1100] Loss: 6.2222 | LR: 8.363609e-04
  [Batch 1200] Loss: 6.2244 | LR: 8.335377e-04
  [Batch 1300] Loss: 6.2224 | LR: 8.307133e-04
  [Batch 1400] Loss: 6.2435 | LR: 8.278877e-04
  [Batch 1500] Loss: 6.2196 | LR: 8.250609e-04
  [Batch 1600] Loss: 6.2096 | LR: 8.222331e-04
  [Batch 1700] Loss: 6.2322 | LR: 8.194043e-04
  [Batch 1800] Loss: 6.2166 | LR: 8.165745e-04
  [Batch 1900] Loss: 6.1920 | LR: 8.137437e-04
  [Batch 2000] Loss: 6.2275 | LR: 8.109120e-04
  [Batch 2100] Loss: 6.2345 | LR: 8.080794e-04
  [Batch 2200] Loss: 6.2262 | LR: 8.052460e-04
  [Batch 2300] Loss: 6.2126 | LR: 8.024118e-04
  [Batch 2400] Loss: 6.2270 | LR: 7.995769e-04
  [Batch 2500] Loss: 6.2313 | LR: 7.967412e-04
  [Batch 2600] Loss: 6.1992 | LR: 7.939049e-04
  [Batch 2700] Loss: 6.2280 | LR: 7.910679e-04
  [Batch 2800] Loss: 6.2215 | LR: 7.882304e-04
  [Batch 2900] Loss: 6.2123 | LR: 7.853923e-04
  [Batch 3000] Loss: 6.2352 | LR: 7.825537e-04
  [Batch 3100] Loss: 6.2203 | LR: 7.797146e-04
  [Batch 3200] Loss: 6.1931 | LR: 7.768751e-04
  [Batch 3300] Loss: 6.2208 | LR: 7.740352e-04
  [Batch 3400] Loss: 6.2179 | LR: 7.711950e-04
  [Batch 3500] Loss: 6.2200 | LR: 7.683544e-04
  [Batch 3600] Loss: 6.2381 | LR: 7.655136e-04
  [Batch 3700] Loss: 6.2343 | LR: 7.626726e-04
  [Batch 3800] Loss: 6.2072 | LR: 7.598314e-04
  [Batch 3900] Loss: 6.2333 | LR: 7.569901e-04
  [Batch 4000] Loss: 6.2251 | LR: 7.541486e-04
  [Batch 4100] Loss: 6.2323 | LR: 7.513071e-04
[Epoch 10] Train Loss: 6.2228 | Train PPL: 504.10 | Val PPL: 597.72
Iteracciones: 4146
  [Batch 100] Loss: 6.2345 | LR: 7.471585e-04
  [Batch 200] Loss: 6.2529 | LR: 7.443170e-04
  [Batch 300] Loss: 6.2362 | LR: 7.414756e-04
  [Batch 400] Loss: 6.2105 | LR: 7.386343e-04
  [Batch 500] Loss: 6.2345 | LR: 7.357932e-04
  [Batch 600] Loss: 6.2331 | LR: 7.329523e-04
  [Batch 700] Loss: 6.2297 | LR: 7.301116e-04
  [Batch 800] Loss: 6.1931 | LR: 7.272713e-04
  [Batch 900] Loss: 6.2539 | LR: 7.244312e-04
  [Batch 1000] Loss: 6.2175 | LR: 7.215915e-04
  [Batch 1100] Loss: 6.2044 | LR: 7.187522e-04
  [Batch 1200] Loss: 6.2118 | LR: 7.159134e-04
  [Batch 1300] Loss: 6.2374 | LR: 7.130751e-04
  [Batch 1400] Loss: 6.2350 | LR: 7.102373e-04
  [Batch 1500] Loss: 6.2225 | LR: 7.074000e-04
  [Batch 1600] Loss: 6.2355 | LR: 7.045634e-04
  [Batch 1700] Loss: 6.2321 | LR: 7.017274e-04
  [Batch 1800] Loss: 6.2164 | LR: 6.988921e-04
  [Batch 1900] Loss: 6.2284 | LR: 6.960576e-04
  [Batch 2000] Loss: 6.2175 | LR: 6.932238e-04
  [Batch 2100] Loss: 6.2264 | LR: 6.903909e-04
  [Batch 2200] Loss: 6.2136 | LR: 6.875588e-04
  [Batch 2300] Loss: 6.2032 | LR: 6.847276e-04
  [Batch 2400] Loss: 6.2342 | LR: 6.818973e-04
  [Batch 2500] Loss: 6.2228 | LR: 6.790680e-04
  [Batch 2600] Loss: 6.2060 | LR: 6.762397e-04
  [Batch 2700] Loss: 6.2481 | LR: 6.734125e-04
  [Batch 2800] Loss: 6.2248 | LR: 6.705864e-04
  [Batch 2900] Loss: 6.2138 | LR: 6.677614e-04
  [Batch 3000] Loss: 6.2224 | LR: 6.649376e-04
  [Batch 3100] Loss: 6.2140 | LR: 6.621150e-04
  [Batch 3200] Loss: 6.2105 | LR: 6.592937e-04
  [Batch 3300] Loss: 6.2365 | LR: 6.564737e-04
  [Batch 3400] Loss: 6.2327 | LR: 6.536550e-04
  [Batch 3500] Loss: 6.2253 | LR: 6.508378e-04
  [Batch 3600] Loss: 6.2343 | LR: 6.480219e-04
  [Batch 3700] Loss: 6.2203 | LR: 6.452075e-04
  [Batch 3800] Loss: 6.2468 | LR: 6.423946e-04
  [Batch 3900] Loss: 6.2190 | LR: 6.395832e-04
  [Batch 4000] Loss: 6.2224 | LR: 6.367735e-04
  [Batch 4100] Loss: 6.2234 | LR: 6.339653e-04
[Epoch 11] Train Loss: 6.2226 | Train PPL: 504.00 | Val PPL: 598.18
Early stopping en epoch: 11
Entrenamiento finalizado.
Best Train Loss: 6.2343 | Best Train PPL: 509.94
