{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLHTFer5jbDu",
        "outputId": "1b64d531-9f11-43b0-8476-6a7113d6f665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q matplotlib datasets ftfy sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VxlQX1m3ifIa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.profiler as profiler # Ver en qué capas se consume más memoria GPU o tiempo.\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import ftfy\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "from datasets import DatasetDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOetmwqWjs_P",
        "outputId": "19c8a86b-1767-410d-9dc7-6bb895da0e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n"
          ]
        }
      ],
      "source": [
        "print(\"GPU available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PTFyHbu-iiO-"
      },
      "outputs": [],
      "source": [
        "def evaluate_ppl(model, dataloader, loss_fn, device):\n",
        "    model.eval() # Le dice al modelo que se comporte como inferencia\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TbyUdJBNiomp"
      },
      "outputs": [],
      "source": [
        "TOK_MODEL_PATH = \"./resources/models/bpe_model_shakespeare.model\"\n",
        "DATASET_PATH = \"./resources/datasets/tinyshakespeare.txt\"\n",
        "train_path = Path(\"./resources/datasets/shakespeare_clean_train.txt\")\n",
        "test_path = Path(\"./resources/datasets/shakespeare_clean_test.txt\")\n",
        "valid_path = Path(\"./resources/datasets/shakespeare_clean_validation.txt\")\n",
        "\n",
        "def light_clean_fn(example):\n",
        "    t = example[\"text\"]\n",
        "    t = ftfy.fix_text(t)\n",
        "    t = unicodedata.normalize(\"NFKC\", t)\n",
        "    t = (t.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
        "        .replace(\"–\", \"-\").replace(\"—\", \"-\"))\n",
        "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "    t = re.sub(r\"\\s*\\n\\s*\", \"\\n\", t)\n",
        "    t = t.strip(\" \\n\")\n",
        "    return {\"text\": t}\n",
        "\n",
        "def load_data():\n",
        "    print(\"Carga datasets\")\n",
        "    print(\"\\tTiny Shakespeare\")\n",
        "    dataset = load_dataset(\"text\", data_files={\"raw\": DATASET_PATH})\n",
        "\n",
        "    # Dividir en train (90%) y test (10%)\n",
        "    train_test = dataset[\"raw\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    # Dividir train en train (90%) y validation (10%)\n",
        "    train_valid = train_test[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    # Reunir en un DatasetDict\n",
        "    tinishakespeare = {\n",
        "        \"train\": train_valid[\"train\"],\n",
        "        \"validation\": train_valid[\"test\"],\n",
        "        \"test\": train_test[\"test\"]\n",
        "    }\n",
        "\n",
        "    print(\"\\tWikitext-2\")\n",
        "    wikitext2 = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
        "\n",
        "    return tinishakespeare,wikitext2\n",
        "\n",
        "\n",
        "def pre_clean_dataset(dataset):\n",
        "    cleaned = DatasetDict({\n",
        "        'train': dataset['train'].map(light_clean_fn, num_proc=4),\n",
        "        'validation': dataset['validation'].map(light_clean_fn, num_proc=4),\n",
        "        'test': dataset['test'].map(light_clean_fn, num_proc=4),\n",
        "    })\n",
        "\n",
        "    with train_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for line in cleaned[\"train\"][\"text\"]:\n",
        "            if line.strip():\n",
        "                f.write(line.strip() + \"\\n\")\n",
        "\n",
        "    with valid_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for line in cleaned[\"validation\"][\"text\"]:\n",
        "            if line.strip():\n",
        "                f.write(line.strip() + \"\\n\")\n",
        "\n",
        "    with test_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for line in cleaned[\"test\"][\"text\"]:\n",
        "            if line.strip():\n",
        "                f.write(line.strip() + \"\\n\")\n",
        "\n",
        "def read_datasets(only_test = False):\n",
        "\n",
        "    if only_test:\n",
        "        with test_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            test_text = f.read()\n",
        "        return None,None, test_text\n",
        "\n",
        "    with train_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        train_text = f.read()\n",
        "\n",
        "    with valid_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        valid_text = f.read()\n",
        "\n",
        "    with test_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        test_text = f.read()\n",
        "\n",
        "    return train_text,valid_text,test_text\n",
        "\n",
        "\n",
        "def tokenizador(dataset):\n",
        "    if not os.path.exists(TOK_MODEL_PATH):\n",
        "        print(\"Entrenamiento modelo BPE\")\n",
        "        # Aprende el vocabulario y como dividir en subpalabras\n",
        "        spm.SentencePieceTrainer.Train(\n",
        "            input=train_path, # Corpus de train, se pueden pasar varios\n",
        "            model_prefix=\"./resources/models/bpe_model_shakespeare\",        # genera prefijo modelos generados:  bpe_model_shakespeare.model y bpe_model_shakespeare.vocab\n",
        "            vocab_size=10000,                 # 8k–32k para corpora pequeños, 32 k para wikitext2\n",
        "            model_type=\"bpe\",\n",
        "            character_coverage=1.0,           # inglés\n",
        "            byte_fallback=True,               # evita UNK en chars raros\n",
        "            normalization_rule_name=\"nfkc\",  # Normalización previa\n",
        "            remove_extra_whitespaces=True, # colapsa espacios extra\n",
        "            num_threads=os.cpu_count(),\n",
        "            pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
        "        )\n",
        "    sp = spm.SentencePieceProcessor(model_file=TOK_MODEL_PATH)\n",
        "    print(\"Tokenización del corpus de train, test y validation\")\n",
        "    tok_ids = sp.encode(dataset, out_type=int)\n",
        "    print(\"Número total de tokens en el corpus:\", len(tok_ids))\n",
        "    return tok_ids, sp.vocab_size()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yEMuM_nhi0Zc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads: int, d_model: int):\n",
        "\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model debe ser divisible por num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // num_heads  # dimensión por cabeza\n",
        "\n",
        "        # Proyecciones lineales para Q, K, V\n",
        "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Proyección final después de concatenar todas las cabezas\n",
        "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x): # Que pasa con los datos al llamar al módulo\n",
        "        \"\"\"\n",
        "        x: (B, T, d_model)\n",
        "        Devuelve: (B, T, d_model)\n",
        "        \"\"\"\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        # 1. Proyecciones lineales\n",
        "        Q = self.Wq(x)  # (B, T, d_model)\n",
        "        K = self.Wk(x)  # (B, T, d_model)\n",
        "        V = self.Wv(x)  # (B, T, d_model)\n",
        "\n",
        "        # 2. Reorganizar en múltiples cabezas\n",
        "        # (B, T, d_model) -> (B, num_heads, T, d_k)\n",
        "        Q = Q.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 3. Calcular scores de atención\n",
        "        # (B, num_heads, T, d_k) x (B, num_heads, d_k, T) -> (B, num_heads, T, T)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 4. Máscara causal (triangular superior a -inf)\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "        scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        # 5. Normalizar con softmax\n",
        "        #with torch.amp.autocast(device_type=\"cuda\",enabled=False):\n",
        "        scores = scores.float()\n",
        "        attn = torch.softmax(scores, dim=-1)  # (B, num_heads, T, T)\n",
        "\n",
        "        # 6. Aplicar atención a V\n",
        "        out = torch.matmul(attn, V)  # (B, num_heads, T, d_k)\n",
        "\n",
        "        # 7. Recombinar heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)  # (B, T, d_model)\n",
        "\n",
        "        # 8. Proyección final\n",
        "        out = self.Wo(out)  # (B, T, d_model)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# Add & norm\n",
        "\n",
        "\n",
        "class NormLayer(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X: (batch, seq_len, hidden_dim)\n",
        "        #with torch.amp.autocast(device_type=\"cuda\",enabled=False): # Desactivo mixed precision manualmente pues es recomendable en la LayerNorm y como la he implementado yo, quiza no lo detecta automaticamente (solo reconoce capas nativas de pytorch) [35]\n",
        "        X = X.float()\n",
        "        mean = X.mean(dim=-1, keepdim=True)       # media por posición\n",
        "        var = X.var(dim=-1, keepdim=True, unbiased=False)  # varianza\n",
        "        X_hat = (X - mean) / torch.sqrt(var + self.eps)    # normaliza\n",
        "        return self.gamma * X_hat + self.beta\n",
        "\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, norm_shape, dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout) # Para regularizar\n",
        "        self.ln = NormLayer(norm_shape) # nn.LayerNorm(norm_shape)\n",
        "\n",
        "    def forward(self, X, Y): # Y es la salida de la subcapa previa y X la entrada a la subcapa\n",
        "        return self.ln(self.dropout(Y) + X) # Aplica add y luego layernorm\n",
        "\n",
        "# FFNN\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,d_model, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        # Se usan 3 capas densas, con dropout y activación GELU\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_ff)\n",
        "        self.linear3 = nn.Linear(d_ff, d_model) # Ver si estas capas expanden y contraen\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "       x = self.linear1(x)\n",
        "       x = self.gelu(x)\n",
        "       x = self.dropout(x)\n",
        "       x = self.linear2(x)\n",
        "       x = self.gelu(x)\n",
        "       x = self.dropout(x)\n",
        "       x = self.linear3(x)\n",
        "       return x\n",
        "\n",
        "# Bloque 1 transformer decoder-only\n",
        "\n",
        "class TransformerDecoderOnlyBlock(nn.Module):\n",
        "    def __init__(self, num_heads, d_model, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "        self.addnorm1 = AddNorm(d_model, dropout)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "        self.addnorm2 = AddNorm(d_model, dropout)\n",
        "        self.apply(self._init_weights) # Recorre las capas aplicando la función\n",
        "\n",
        "    def _init_weights(self, m): # Inicialización de pesos: Xavier para MHA y FFNN y normal para embeddings\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias) # Para evitar desplazamiento inicial arbitrario\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self,x):\n",
        "        attention = self.mha(x)\n",
        "        x = self.addnorm1(x, attention)\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.addnorm2(x, ffn_out)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q5pZhgYOi5et"
      },
      "outputs": [],
      "source": [
        "\n",
        "class miniGPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, num_heads=8, d_ff=2048, num_layers=6, context_len=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(context_len, d_model)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerDecoderOnlyBlock(num_heads, d_model, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = NormLayer(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        pos = torch.arange(T, device=idx.device).unsqueeze(0).expand(B, T)\n",
        "\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHx8_YdUkcPN",
        "outputId": "568a3cc2-c3a5-4d1e-cc72-f4d0bd3ab947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Carpetas creadas correctamente.\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(\"./resources/models\", exist_ok=True)\n",
        "os.makedirs(\"./resources/datasets\", exist_ok=True)\n",
        "\n",
        "print(\"Carpetas creadas correctamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvY8deNbkeEW",
        "outputId": "cff37f57-d4e3-47a0-d944-f52a261ead7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset tinyshakespeare.txt descargado correctamente.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O ./resources/datasets/tinyshakespeare.txt\n",
        "print(\"Dataset tinyshakespeare.txt descargado correctamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z8sr-oGbkr_I"
      },
      "outputs": [],
      "source": [
        "class LMWindowDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokens, context_len):\n",
        "        self.toks = tokens\n",
        "        self.ctx = context_len\n",
        "    def __len__(self):\n",
        "        return len(self.toks) - self.ctx # number of windows\n",
        "    def __getitem__(self, i): # inputs:targets\n",
        "        x = torch.tensor(self.toks[i:i+self.ctx], dtype=torch.long)\n",
        "        y = torch.tensor(self.toks[i+1:i+self.ctx+1], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by21iBATlra-",
        "outputId": "255857f9-63ea-4788-e040-e8001e6f23c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "x2gxBFv2i_hx",
        "outputId": "c5dc7743-6fc1-4b4e-e2c6-e2a645a4e02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "Carga datasets\n",
            "\tTiny Shakespeare\n",
            "\tWikitext-2\n",
            "Longitud corpus train: 896135 caracteres\n",
            "Longitud corpus valid: 100448 caracteres\n",
            "Longitud corpus test: 111570 caracteres\n",
            "Tokenización del corpus de train, test y validation\n",
            "Número total de tokens en el corpus: 273905\n",
            "Tokenización del corpus de train, test y validation\n",
            "Número total de tokens en el corpus: 31059\n",
            "Preparando DataLoader\n",
            "Se invoca el modelo\n",
            "TRAIN\n",
            "Iteracciones: 4276\n",
            "  [Batch 100] Loss: 6.2542\n",
            "  [Batch 200] Loss: 6.2575\n",
            "  [Batch 300] Loss: 6.2547\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3970750975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m#scaler.update() # Se encarga de ajustar el factor de escalado de la pérdida para la próxima iteración.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "embedding_dim = 512              \n",
        "context_len = 256\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_PATH = \"./resources/models/gpt_model.pth\"\n",
        "BEST_MODEL_PATH = \"./resources/models/best_gpt_model.pth\"\n",
        "########################\n",
        "# Lectura de datasets\n",
        "########################\n",
        "\n",
        "tinishakespeare,wikitext2 = load_data()\n",
        "\n",
        "#########################\n",
        "# Limpieza y tokenización\n",
        "##########################\n",
        "pre_clean_dataset(tinishakespeare)\n",
        "\n",
        "train_text,valid_text,test_text = read_datasets()\n",
        "\n",
        "print(f\"Longitud corpus train: {len(train_text)} caracteres\")\n",
        "print(f\"Longitud corpus valid: {len(valid_text)} caracteres\")\n",
        "print(f\"Longitud corpus test: {len(test_text)} caracteres\")\n",
        "\n",
        "train_ids,vocab_size = tokenizador(train_text)\n",
        "val_ids,_ = tokenizador(valid_text)\n",
        "\n",
        "#########################\n",
        "# Preparación de batches <input, target>\n",
        "##########################\n",
        "\n",
        "# Se formatiza como <input, target>\n",
        " \n",
        "print(\"Preparando DataLoader\")\n",
        "dataset = LMWindowDataset(train_ids, context_len=256)  # o 256\n",
        "val_dataset = LMWindowDataset(val_ids, context_len=256)  # o 256\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True,num_workers=os.cpu_count(),pin_memory=torch.cuda.is_available(),prefetch_factor=2)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False,num_workers=os.cpu_count(),pin_memory=torch.cuda.is_available(),prefetch_factor=2)\n",
        "\n",
        "\n",
        "########################\n",
        "# Carga el modelo\n",
        "########################\n",
        "\n",
        "print(\"Se invoca el modelo\")\n",
        "model = miniGPT2(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=512,\n",
        "    num_heads=8,\n",
        "    d_ff=1024,\n",
        "    num_layers=6,\n",
        "    context_len=256,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(\"TRAIN\")\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "num_epochs = 20\n",
        "best_val_loss = float(\"inf\")\n",
        "best_loss = float(\"inf\")\n",
        "best_ppl = float(\"inf\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#optimizer_sgd = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "#optimizer_rmsprop = torch.optim.RMSprop(model.parameters(),lr=1e-3, weight_decay=1e-2,momentum=0.9)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1.5e-4, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*len(loader))\n",
        "\n",
        "\n",
        "\n",
        "patience_limit = 10 # Para early stopping\n",
        "patience_counter = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_ppls, val_ppls = [], []\n",
        "\n",
        "#scaler = torch.amp.GradScaler(\"cuda\") \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    '''\n",
        "    Solo para validar en uno o pocos batches\n",
        "    with profiler.profile(\n",
        "        activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
        "        profile_memory=True,\n",
        "        record_shapes=True\n",
        "    ) as prof:\n",
        "    '''\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    torch.cuda.empty_cache()            \n",
        "    print(f\"Iteracciones: {len(loader)}\")\n",
        "    for num_batch, (batch_x, batch_y) in enumerate(loader, start=1):\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device) # todevice mueve a GPU si está disponible, es necesario pues le modelo está en GPU también.\n",
        "        optimizer.zero_grad(set_to_none=True) # Limpia los gradientes previos. Pues en pytorch se acumulan por defecto y al llamar a backward se suman a los previos, es decir, se estarían combinando gradientes de varios batches. Lo pone a none para ahorrar memoria con set_to_none.\n",
        "\n",
        "        #with torch.amp.autocast(\"cuda\"): # Decide de forma segura que algunas operaciones se hagan en FP16 para ahorrar memoria y acelerar GPU\n",
        "        logits = model(batch_x) # Salida del modelo [batch_size, seq_len, vocab_size]. Cada posición de la secuencia tiene una distribución sobre el vocabulario.\n",
        "            # Devuelve, para cada token de entrada, un vector de tamaño vocab_size con valores reales. En PyTorch, nn.CrossEntropyLoss()  ya incluye internamente el softmax\n",
        "        loss = loss_fn(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            batch_y.view(-1) # Batch_y tiene: [batch_size, seq_len], esto lo aplana en [batch_size * seq_len] para cross entropy\n",
        "        )\n",
        "        loss.backward()\n",
        "        #scaler.scale(loss).backward() # Escala y calcula el gradiente de los pesos del modelo (en que dirección cambio el peso para reducir la pérdida). Scale multiplica la pérdida por un factor grande antes de hacer el backward. Los pesos aún no cambian. \n",
        "        \n",
        "        # Gradient Clipping para estabilidad [35]: limita el tamaño máximo que puede tener el conjunto de gradientes antes de actualizar los pesos.El clipping no cambia la dirección del gradiente (sigue apuntando hacia la misma mejora), solo reduce su magnitud para que no provoque saltos gigantes en los pesos.\n",
        "        #scaler.unscale_(optimizer) # En AMP, los gradientes están temporalmente amplificados (por GradScaler).Se desescalan para no recortar valores falsos.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        #scaler.step(optimizer) # usa los gradientes calculados para ajustar los pesos. Sin GradScaler sería: optimizer.step(). El scaler primero desescala los gradientes si aún no se ha hecho (los divide por el mismo factor que usó en scale) y luego llama a optimizer.step().\n",
        "        #scaler.update() # Se encarga de ajustar el factor de escalado de la pérdida para la próxima iteración.\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if num_batch % 100 == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f\"  [Batch {num_batch}] Loss: {loss.item():.4f} | LR: {current_lr:.6e}\")\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    train_ppl = math.exp(avg_loss) # Perplejidad: Si baja es que comprende mejor los datos. Es el exponente de la entropía\n",
        "    # Validación para implementar early stopping y guardar mejor modelo\n",
        "\n",
        "    val_loss, val_ppl = evaluate_ppl(model, val_loader, loss_fn, device)\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f} | Train PPL: {train_ppl:.2f} | Val PPL: {val_ppl:.2f}\")\n",
        "    train_losses.append(avg_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_ppls.append(train_ppl)\n",
        "    val_ppls.append(val_ppl)\n",
        "\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_loss = avg_loss\n",
        "        best_ppl = train_ppl\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience_limit:\n",
        "            print(f\"Early stopping en epoch: {epoch+1}\")\n",
        "            break\n",
        "\n",
        "\n",
        "print(\"Entrenamiento finalizado.\")\n",
        "torch.save(model, MODEL_PATH)\n",
        "\n",
        "print(f\"Best Train Loss: {best_loss:.4f} | Best Train PPL: {best_ppl:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(train_ppls, label=\"Train PPL\")\n",
        "plt.plot(val_ppls, label=\"Validation PPL\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Evolución de Perplexity durante entrenamiento\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('./resources/imagenes/resultado_entrenamiento_v1.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/transformer_colab/results\n",
        "!cp -r /content/resources /content/drive/MyDrive/transformer_colab/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
