%&pdflatex
%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode

\documentclass[11pt]{book}

%--------------------------------------------------------------------------- 
% Paquetes y fuentes
%---------------------------------------------------------------------------

% Definir un nuevo comando para la B caligráfica
\newcommand{\BD}{\text{$\mathcal{B}\mathcal{D}$}}

\usepackage{geometry}               % define las dimensiones de la página
\geometry{
 a4paper, % Tamaño del papel
 centering, % Zona útil centrada
 margin = 25mm, % Márgenes en blanco: 25mm
}

\usepackage[utf8]{inputenc}        % para usar el teclado normalmente
\usepackage[spanish,es-tabla]{babel}        % idioma español
\usepackage[T1]{fontenc}           % gestión de fuentes con acentos
\usepackage{csquotes}              % citas (necesario con biblatex)

% Interlineado
\usepackage{setspace}
\onehalfspacing

% Espaciado entre párrafos
\setlength{\parskip}{6pt}

% Matemáticas y teoremas
\usepackage{amssymb}               
\usepackage{amsmath}               
\usepackage{amsthm}                
\usepackage{newtxtext,newtxmath}             % fuente Times

% Gráficos y figuras
\usepackage{tikz}                  
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
%\usepackage[draft]{graphicx}       % draft = no carga imágenes al compilar

% Código y algoritmos
\usepackage{listings}
\usepackage[ruled,vlined,spanish,onelanguage]{algorithm2e}
\SetKw{Para}{Para}

% Color portada
\usepackage{xcolor}
\definecolor{miRojo}{HTML}{BA0C2F}

% Hipervínculos
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% Tipografía
\usepackage{microtype}

% Cabeceras y pies de página
\usepackage{titleps}

% Bibliografía
\usepackage[style=apa,backend=biber]{biblatex}
\DeclareLanguageMapping{spanish}{spanish-apa}
\addbibresource{refs.bib}

% Referencias cruzadas inteligentes
\usepackage{cleveref}

% Tabla 
\usepackage{booktabs} % Para líneas más elegantes en la tabla
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage[most]{tcolorbox}

\usepackage[font=small,labelfont=bf,labelsep=colon,justification=centering]{caption}



%--------------------------------------------------------------------------- 
% Definición de estilos de página
%---------------------------------------------------------------------------

\newpagestyle{miestilo}{
  \sethead{}{}{}
  \setfoot{}{}{\thepage}
}
\pagestyle{miestilo}



\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{empty}\cleardoublepage}}
\newcommand{\sectionmarkwithoutsections}[1]{\markright{#1}}

\makeindex


\renewcommand{\spanishoperators}{Re}

%--------------------------------------------------------------------------- 
% Inicio del documento
%---------------------------------------------------------------------------

\begin{document}
\frontmatter
\pagestyle{miestilo}


%\pagenumbering{roman} % Incluido en \frontmatter


%--- PORTADA DEL TFG
\begin{titlepage}
    \centering

    \begin{figure}
    \centering
    \includegraphics[width=0.25\linewidth]{img/logo_imf.png}
    \end{figure}

    \vspace*{1cm}
    {\Large \textbf{MÁSTER EN} \\[0.3cm]
    \textbf{Big Data e Inteligencia Artificial} \\[1cm]
    \textbf{ONLINE}}\\[3cm]
    
    {\LARGE \textcolor{miRojo}{\textbf{Implementación de una arquitectura Transformer educativa (NLP).}}} \\[2cm]
    
    {\large TFM elaborado por: \\[0.3cm]
    \textbf{Diego García Muro}} \\[1cm]
    
    {\large Tutor/a de TFM: \\[0.3cm]
    \textbf{Daniel Rubio Yagüe}} \\[4cm]
    
    {\large - Soria, 16 Septiembre del 2025 - }
    
    \vfill
\end{titlepage}
%--- FIN DE LA PORTADA

\clearemptydoublepage


%--- RESUMEN

\chapter{Resumen}

La arquitectura Transformer es un modelo que ha revolucionado el campo del procesamiento del lenguaje natural, impulsando avances significativos en tareas como la traducción automática, la generación de texto y la comprensión del lenguaje, siendo la base de los modelos Bert y GPT. A diferencia de los modelos pre-exsistentes como las redes neuronales recurrentes (RNN), basadas en secuencias, los Transformers se basan en mecanismos de atención que permiten procesar las palabras en paralelo, mejorando la eficiencia y la capacidad de capturar relaciones contextuales a largo plazo.

A lo largo de esta memoria se detalla el proceso de implementación y validación de una arquitectura Transformer Decoder Only orientada a la generación de texto en dos contextos concretos que son las obras de Shakespeare y artículos de Wikipedia. No se busca competir con los modelos de última generación, sino comprender los fundamentos teóricos y prácticos de los Transformers y explorar, de manera didáctica, su funcionamiento en tareas de procesamiento de lenguaje natural.

En un primer capítulo se explica que es el Procesamiento de Lenguaje Natural (NLP), abordando arquitecturas previas como RNN y LSTM, para posteriormente introducir la arquitectura Transformer y fijar el alcance y limitaciones de este proyecto.

Posteriormente, se describen los materiales y métodos empleados, comenzando con un análisis y descripción de los componentes que forman parte de la arquitectura, para comprender su funcionamiento a bajo nivel. Aquí se detalla como el texto es procesado, desde la tokenización y generación de embeddings, hasta los distintos bloques que componen el Transformer, como los mecanismos de atención, capas feed-forward y normalización. Se incluyen las librerías y el lenguaje de programación utilizado, hiperparámetros y características del sistema.

A continuación, en el capítulo de Resultados se detalla el proceso completo de desarrollo, desde la obtención, análisis y preparación de los datos hasta la implementación y entrenamiento del modelo, incluyendo resultados obtenidos, así como mejoras llevadas a cabo. Se incluye la implementación de un modelo LSTM y las distintas técnicas de validación cuantitativas y cualitativas llevadas a cabo. Finalmente, se presentan las conclusiones obtenidas y posibles líneas futuras de trabajo.

%--- FIN DEL RESUMEN

\chapter{Abstract}
The Transformer architecture is a model that has revolutionized the field of natural language processing, driving significant advances in tasks such as machine translation, text generation, and language understanding, and serving as the foundation for models such as BERT and GPT. Unlike pre-existing sequence-based models such as recurrent neural networks (RNNs), Transformers rely on attention mechanisms that enable words to be processed in parallel, improving efficiency and the ability to capture long-range contextual relationships.

Throughout this thesis, the implementation and validation process of a Transformer Decoder Only architecture focused on text generation is described, applied to two specific contexts: the works of Shakespeare and Wikipedia articles. The aim is not to compete with state of the art models but to understand the theoretical and practical foundations of Transformers and to explore, in a didactic manner, their functioning in natural language processing tasks.

The first chapter introduces the concept of Natural Language Processing (NLP), addressing earlier architectures such as RNNs and LSTMs, before introducing the Transformer architecture and defining the scope and limitations of this project.

Subsequently, the materials and methods used are described, beginning with an analysis and explanation of the components that make up the architecture to understand its low level functioning. This includes how the text is processed, from tokenization and embedding generation to the various blocks that compose the Transformer, such as attention mechanisms, feed-forward layers, and normalization. The programming language, libraries used, hyperparameters, and system specifications are also detailed.

Next, in the Results chapter, the complete development process is presented, from data acquisition, analysis, and preparation to model implementation and training, including the results obtained and the improvements made. The implementation of an LSTM model and the various quantitative and qualitative validation techniques carried out are also included. Finally, the conclusions and possible future lines of work are presented.

\clearemptydoublepage

\tableofcontents

\clearemptydoublepage

%-------------
\mainmatter             % Parte principal del texto
\pagestyle{miestilo}    % Estilo de cabeceras para la parte principal del texto

%--- CAPÍTULO 1 DEL TFM
\chapter{Introducción y antecedentes}

En los últimos años, el auge de la inteligencia artificial ha estado impulsado por los llamados Large Language Models (LLMs), que han supuesto una revolución, transformando sectores enteros. Estos modelos están presentes tanto como herramientas que poco a poco reemplazan a los buscadores tradicionales, usadas por el público en general, como herramientas open source o de pago por uso, accesibles localmente o a través de la nube, que han dado un giro al negocio en general, originando nuevas oportunidades, automatizando tareas y ofreciendo soluciones a problemas que, o no existían o no se podían abordar con tanta efectividad.

Este tipo de modelos se construyen haciendo uso de enormes recursos computacionales y pueden llegar a tener billones de parámetros, lo que dificulta enormemente su implementación. Es por ello, que en este anteproyecto se propone el desarrollo de un modelo reducido basado en la arquitectura Transformer, con un número reducido de parámetros capaz de entrenarse usando una GPU de propósito general en unas horas. El objetivo principal no es competir con modelos de última generación, sino comprender los fundamentos teóricos y prácticos de los Transformers y explorar, de manera didáctica, su funcionamiento en tareas de procesamiento de lenguaje natural.

\section{Procesamiento del lenguaje natural}

El procesamiento de lenguaje natural (NLP) es una tecnología que permite a los cordenadores interpretar, manipular y comprender el lenguaje humano Surgió en la década de 1950, derivado de los primeros experimentos sobre traducción automática, ganando popularidad en las décadas siguientes con aplicaciones como el filtrado de correo no deseado, la clasificación de documentos y los chatbots básicos. En 2020 se dió el punto de inflexión, con el auge de los modelos de aprendizaje profundo, que utilizaban arquitecturas de redes neuronales para analizar secuencias de datos, lo que permitió analizar bloques de texto más grandes.

El NLP está presente en la mayoría de las aplicaciones que usamos a diario, desde los asistentes de voz como Siri o Alexa, hasta los sistemas de traducción automática como Google Translate. En el ámbito empresarial los casos de uso aumentan cada día, entre los más comunes se encuentran: clasificación de documentos, análisis de sentimientos, chatbots y asistentes virtuales, motores de recomendación y análisis de tendencias en redes sociales.

\section{Modelos previos: RNN y LSTM}

Las redes neuronales recurrentes (RNN) y redes de memoria a largo plazo (LSTM) son arquitecturas de redes neuronales capaces de procesar datos secuenciales donde el orden cronológico es importante. Una neurona recurrente, en cada instante de tiempo, recibe como entrada tanto la entrada actual como la salida del instante de tiempo anterior, lo que le permite mantener una memoria, a corto plazo, de estados previos. Por otro lado, LSTM es capaz de recordar dependencias entre los términos a lo largo de tiempos más largos, gracias a su estructura de celdas de memoria y puertas que regulan el flujo de información. En sí, manteniene información en memoria que cada neurona es capaz de borrar, escribir o leer. 

\section{De modelos secuenciales a mecanismos de atención}
Antes de la aparición de esta arquitectura, los modelos existentes procesaban las palabras de forma secuencial con el fin de entender el lenguaje. Esto implica una complejidad [$O(n^2)$] de forma que para procesar 3 palabras se tenían que ejecutar 6 operaciones secuenciales (para la tercera palabra necesita recordar la primera y segunda y procesar la tercera, para la segunda tendría que recordar la primera y procesar la segunda y para la primera se debía procesar esa palabra), esto, para $10$ palabras aumenta a $45$ operaciones, y para $100$, $4950$. Computacionalmente es muy ineficiente. 

Gracias al paper \textit{"Attention is ALl you Need"} publicado en 2017 \parencite{vaswani2017attention}, donde se presentan los \texttt{mecanismos de atención} y se logra optimizar el proceso a una complejidad [$O(n)$]. Esto es porque plantean la posibilidad de que cada palabra mire simultáneamente a las demás, es decir, ya no hay un procesamiento secuencial, sino paralelo. 

Estos mecanismos constan de 8 operaciones. En primer lugar trabajan con \textit{embeddings} \parencite{neuraforge2023guide}, esto es, vectores densos que recogen, tras el entrenamiento, información sobre cada token en relación con el resto, ya sea información semántica, relaciones sintácticas, etc. Estas representaciones, se proyectan a 3 matrices, también entrenables, que son la base de los mecanismos de atención: Q (¿Qué busco?), K (¿Qué aporto?), V (La información real aportada) \parencite{epichka2023qkv}. A partir de ellas, se calcula una métrica de atención, que indicaría para cada token o palabra, como de relevante es. Esta métrica se convierte a una distribución de probabilidad a través de la función \textit{softmax}, de forma que simula una conciencia humana, pues los seres humanos nos enfocamos en aquella información relevante mientras que el resto queda en la periferia. Por último, se integra la información, dando a la información real (V) un peso, dado por esos pesos de atención calculados previamente.

Todo este proceso no se ejecuta una única vez, sino que se apila en capas, de forma que cada capa o cabeza de atención se encarga de una tarea distinta. Así, las capas más superficiales capturan relaciones sintácticas del tipo sujeto-verbo-objeto, mientras que las capas más profundas abordan el razonamiento abstracto y el procesamiento meta-cognitivo, de hecho, se cree que en las capas 70 a 80 de los modelos GPT la representación se asemeja al comportamiento humano \parencite{plainenglish2021gpt}. En este punto, un elemento clave es el \textit{flujo residual}. Gracias a él, toda la red comparte y acumula información y cada capa comparte sus aportaciones al resto. 

Todo lo explicado hasta ahora no es más que operaciones matemáticas que permiten al modelo encontrar patrones, pero por si solo, un transformer no es inteligente, para que exista creatividad, razonamiento y abstracción debe cumplir con las llamadas \textit{Scaling Laws} \parencite{wolfe_llm_scaling}. Estas sostienen que el rendimiento del modelo mejora de forma predecible al escalar estas 3 dimensiones: número de parámetros (más de 100B ya conllevan al pensamiento abstracto), datos diversos y cómputo masivo. Es por ello, que no se espera como resultado de este proyecto un modelo capaz de razonar como lo hacen los modelos SOTA del mercado.

Pese a los grandes avances y la repercusión que están teniendo los modelos LLM hoy en día, basados en este tipo de arquitectura, hay que decir que presentan varios problemas. Por un lado la atención es [O(n²)], pues para $10.000$ palabras hay $10.000 x 10.000$ relaciones posibles, por otro lado, las infraestructuras son fijas y, además, todos los parámetros se activan siempre, lo que es ineficiente. Es por ello que están surgiendo soluciones como la atención dispersa (el modelo no atiende a todas las posiciones), arquitecturas capaces de modificarse a sí mismas de forma autónoma y técnicas como \textit{mixture of experts} que permiten usar una fracción de los parámetros en cada paso \parencite{plainenglish2021gpt}.

%--- FIN DEL CAPÍTULO 1

\clearemptydoublepage

%--- CAPÍTULO 2 DEL TFM
\chapter{Objetivos del proyecto}

El proyecto tiene como objetivo implementar y analizar una arquitectura Transformer funcional que no busque competir con los modelo SOTA, sino con el propósito de comprender en profundidad su funcionamiento a bajo nivel y evaluar su desempeño en un corpus de texto reducido, siendo capaz de generar texto con cierta coherencia en contextos limitados, como las obras de Shakespeare o fragmentos de artículos de Wikipedia.

\section{Objetivos específicos}

\begin{itemize}
    \item Analizar los principios teóricos que sustentan la arquitectura \textit{Transformer} y sus diferencias con modelos secuenciales como RNN y LSTM.
    \item Implementar desde cero una versión reducida de un \textit{Transformer Decoder Only} utilizando librerías de Python.
    \item Diseñar y aplicar un proceso de tokenización y generación de \textit{embeddings} adecuado al corpus utilizado.
    \item Entrenar y evaluar el modelo con un conjunto de datos reducido, analizando su capacidad para generar texto coherente con técnicas cualitativas y cuantitativas.
    \item Comparación de los resultados obtenidos frente a los producidos por un modelo LSTM, también implementado en este proyecto.
    \item Realizar un análisis exploratorio de los resultados obtenidos y proponer posibles mejoras o líneas futuras de trabajo.
\end{itemize}


%--- FIN DEL CAPÍTULO 2

\clearemptydoublepage

%--- CAPÍTULO 3 DEL TFM
\chapter{Material y métodos}
\section{PyTorch y Tensorflow}
Ambos son frameworks de aprendizaje profundo y de código abierto para gestionar e implementar los procesos de aprendizaje automático. Sin embargo, TensorFlow se basa en un gráfico computacional estático, esto es, que antes de la ejecución se definen todas las operaciones y la estructura del modelo, mientras que PyTorch utiliza un gráfico computacional dinámico, lo que permite modificar el modelo sobre la marcha durante la ejecución.
En este proyecto se emplea mayormente PyTorch para implementar la arquitectura Transformer, quedando Tensorflow reservado para el desarrollo del modelo LSTM utilizado para comparar los resultados obtenidos.

\section{Flask}
Flask es un framework web escrito en Python que facilita la creación y despliegue de aplicaciones web. En este proyecto se utiliza para desplegar una API capaz de recibir un texto inicial junto con una serie de hiperparámetros que invoca al método de generación de texto en base al modelo entrenado.

\section{Librerías y entorno utilizado}
El proyecto se implementa íntegramente en Python 3.12, montado en un entorno virtual donde se instalarán las librerías necesarias a través de pip3. 
El ordenador utilizado cuenta con una GPU NVIDIA GeForce RTX 4090 con $15943$ MB de memoria total. 
La versión de CUDA es la 12.9 y las librerías utilizadas se recogen en la siguiente tabla:

\begin{table}[h]
\centering
\caption{Librerías principales utilizadas en el proyecto}
\label{tab:librerias_principales}
\begin{tabular}{l c}
\toprule
\textbf{Librería} & \textbf{Versión} \\
\midrule
Flask & 3.1.2 \\
Torch (PyTorch) & 2.9.0 \\
Datasets (HuggingFace) & 4.1.0 \\
TensorFlow & 2.20.0 \\
NumPy & 2.3.3 \\
Pandas & 2.3.2 \\
Matplotlib & 3.10.7 \\
Scikit-learn & 1.5.2 \\
NLTK & 3.9.1 \\
wordcloud & 1.9.4\\
mauve-text & 0.4.0 \\
\bottomrule
\end{tabular}
\end{table}

En el anexo \ref{sec:entorno} se incluyen los pasos de preparación del entorno.

\section{Tokenización Byte Pair Encoding}
Para que el modelo sea capaz de identificar patrones y llegar a "comprender" un texto, es necesario su conversión a valores numéricos. Para ello, el texto, ya limpio, se descompone en unidades mínimas denominadas tokens. Un \texttt{token} se puede entender como un fragmento de un texto, por ejemplo, un carácter, una palabra o una subpalabra dentro de un vocabulario. Estos tokens se transforman a un valor numérico, que corresponde con el índice de ese fragmento en el vocabulario obtenido. Sin embargo, esto no es sufiente para que el modelo comprenda el texto, para ello cada token se representa como un vector que captura el contexto de las palabras y otra información importante, son los llamados \texttt{embeddings}, que se explicarán más adelante.

Existen diferentes técnicas de tokenización, pudiento clasificarse como tokenización en caracteres, en palabras y en subpalabras. La primera tiene la ventaja de disminuir la complejidad, sin embargo, el modelo está más limitado a la hora de aprender representaciones significativas. Puede ser útil para idiomas como el chino, donde la morfología es compleja y no existen los espacios. La segunda es la tokenización por espacio y puntuación, es decir, la frase \textit{"Mi perro come mucho."} no se descompone en los tokens: \textit{"Mi", "perro", "come", "mucho."}, que es a lo que equivaldría hacer un split por espacios, sino que se tokenizaría como: \textit{"Mi", "perro", "come", "mucho", "."}. El problema de este método es la complejidad en memoria y tiempo, pues genera un vocabulario muy grande y con ellos una matriz de embeddings también demasiado grande. A esto hay que sumar que aparece el problema de palabras no vistas o \textit{UNK}. 

La tokenización de subpalabras resuelve estos problemas. Se basa en la idea de que las palabras que se usan con frecuencia no deberían dividirse, mientras que las más raras sí, manteniendo así palabras más frecuentes. Por ejemplo, en un texto en inglés, la palabra \textit{"annoyingly"} no es una palabra común, mientras que las subpalabras \textit{"annoying"} y \textit{"ly"} sí. Permite, con ello, ver más vocabulario, sin perder significado de las palabras menos frecuentes \parencite{lmpo2020bpe}

En este proyecto, dado que son textos en inglés y las limitaciones técnicas que existen, se ha decidido utilizar un tokenizador de subpalabras. Entre estos, destacan dos: \texttt{Byte Pair Encoding (BPE)} y \texttt{WordPiece} \parencite{huggingface_tokenizer_summary}. 

Ambos funcionan de forma similar, la principal diferencia radica en cómo realizan las "uniones" a partir del vocabulario base. BPE lo que hace es primeramente una pre-tokenización de los datos de entrenamiento por espacios o basada en reglas. Con ello, obtiene un conjunto de palabras únicas y su frecuencia. Tras ello, crea un vocabulario base con los distintos símbolos que aparecen en las palabras únicas. La idea ahora es aprender reglas de unión para crear nuevos símbolos a partir de otros dos en base a la frecuencia. El proceso se repite hasta alcanzar el tamaño deseado, que se configura como hiperparámetro. En GPT-3 el tamaño es de $50000$ tokens aproximadamente \parencite{paul2025vocabsize}. Para evitar un vocabulario base muy extenso si se quiere tener todos los caracteres con el fin de evitar palabras o tokens no vistos, se pueden usar bytes, así se tiene un tamaño fijo de $256$, utilizando reglas para los caracteres de puntuación que no se puedan representar.

WordPiece, en lugar de escoger simplemente los pares de símbolos más frecuentes, utiliza probabilidades. Para cada par de símbolos, calcula la probabilidad combinada entre la individual de cada uno. El par con el valor de $\text{score}(A,B)$ más alto es el que se añade al vocabulario, ya que indica que ambos símbolos aparecen juntos con más probabilidad de la esperada si fueran independientes.

\[
\text{puntuación}(A,B) = \frac{P(AB)}{P(A) \cdot P(B)}
\]

Donde:
\begin{itemize}
    \item $P(AB)$ es la probabilidad estimada de que aparezca el token combinado $AB$
    \item $P(A)$ y $P(B)$ son las probabilidades de cada símbolo por separado.
\end{itemize}

Dado que los \textit{datasets} que utilizo no son muy grandes, voy a emplear BPE con SentencePiece \parencite{google_sentencepiece}. Además, este método ha sido utilizado en modelos como GPT \parencite{reddit2021tokenizers}. 

SentencePiece es un framework de tokenización y detokenización desarrollado por Google que facilita la implementación de BPE. Entre sus características principales destaca el hecho de que no depende del idioma, ya que trabaja directamente con caracteres crudos, permite entrenar sin preprocesamiento previo y sin precisar librerías externas como NLTK o SpaCy, y trata los espacios en blanco como caracteres independientes, lo que resuelve problemas en lenguas sin segmentación explícita como el chino. El resultado es un modelo portable y eficiente. 

Asimismo, SentencePiece incluye mecanismos avanzados que mejoran la robustez de los modelos: 
\begin{itemize}
    \item \textbf{Byte fallback}, hace que si aparece cualquier caracter desconocido no genere un \textit{"UNK"}, sino que lo descompone en bytes.
    \item \textbf{BPE-dropout}, que introduce aleatoriedad en el proceso de segmentación durante el entrenamiento para mejorar la generalización, en sí lo que hace es omitir uniones aleatoriamente durante la tokenización.
    \item \textbf{Regularización por subpalabras}, que permite generar múltiples segmentaciones posibles de una misma secuencia, actuando como técnica de \textit{data augmentation}.
\end{itemize}

\section{Embeddings}
La tokenización permite convertir palabras o fragmentos de texto en números enteros, pero estos números no capturan el significado o contexto de las palabras. Hay técnicas como  \textit{One-Hot Encoding} que representan cada palabra como un vector binario en el que solo una posición es 1 y el resto son ceros. Esto presenta varias desventajas: genera vectores de alta dimensionalidad, dispersos y no capturan ningún tipo de información sobre el significado, el contexto o la similitud entre palabras.

Es por ello que se utilizan ampliamente los \textit{embeddings}. Se trata de vectores densos, de números reales, capaces de capturar el significado de las palabras, sus relaciones semánticas y su contexto dentro del lenguaje. Los embeddings se obtienen mediante entrenamiento, de manera que palabras con contextos similares tienden a tener vectores cercanos entre sí en el espacio vectorial \parencite{geeksforgeeks_embedding}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/ejemplo_embedding.png}
    \caption{Ejemplo similitud vectorial embeddings}
    \label{fig:placeholder6}
\end{figure}

Internamente, un embedding se representa como una matriz de pesos $E \in \mathbb{R}^{V \times d}$, donde $V$ es el tamaño del vocabulario y $d$ la dimensión del embedding. Cada fila de $E$ contiene el vector asociado a un token. Durante el entrenamiento, solo se actualizan los vectores correspondientes a los tokens presentes en cada lote, lo que permite un aprendizaje eficiente.

Otra ventaja de estos vectores multidimensionales es la posibilidad de operar con ellos, así un ejemplo clásico a la hora de hablar de embeddings, demuestra cómo capturan el contexto: 
\[
\text{vec}(\text{``rey''}) - \text{vec}(\text{``hombre''}) + \text{vec}(\text{``mujer''}) \approx \text{vec}(\text{``reina''})
\]

En este tipo de arquitecturas, los embeddings se combinan con codificaciones posicionales para incorporar la información del orden de las palabras en la secuencia. Con ello se consigue que el modelo tenga acceso tanto al significado individual de las palabras como a su posición relativa dentro de la frase. Así puede capturar diferencias según dónde aparezca la palabra, no es lo mismo "un banco de peces" que "dinero en el banco" \parencite{perez_embeddings}.

\section{Arquitectura Transformer} \label{Arquitectura Transformer}

En este proyecto, como ya se ha explicado, se va a implementar una arquitectura Transformer para la generación de texto en un contexto reducido, por lo que se ha decidido desarrollar una arquitectura del tipo Decoder Only, ya que no es necesario un proceso de codificación previo, como sería el caso de un traductor, el cual requeriría de un encoder.

A continuación, se describen los distintos bloques que componen esta arquitectura.

\subsection{Positional encoding}
Un transformer, a diferencia de otros modelos, procesa los datos en paralelo, lo que hace que desconozcan en que orden aparecen los distintos tokens. Por este motivo se realiza una codificación posicional, que asigna una posición relativa a cada token o palabra en la secuencia \parencite{phillips2019positional}. 

Este aspecto es fundamental, ya que en una frase el significado de cada palabra depende del contexto y la posición en la que aparece. Por ejemplo, "Me senté en el banco" es distinto a "El banco de peces". En el artículo original "Attention is All You Need", se propone la utilización de funciones seno y coseno para dar a cada posición una representación única, ya que cada palabra se representa con un vector numérico. Esto permite que la salida esté normalizada, pues estos valores se encuentran en el rango [-1 ,1] y no requiere de entrenamiento adicional ya que los valores posicionales son determinados de forma fija y única. 

    \[
    PE_{(pos, 2i)} = \sin\!\left(\frac{pos}{10000^{\tfrac{2i}{d}}}\right), 
    \qquad
    PE_{(pos, 2i+1)} = \cos\!\left(\frac{pos}{10000^{\tfrac{2i}{d}}}\right)
    \]
En este proyecto, se ha optado por utilizar una codificación posicional aprendida, donde el modelo aprende durante el entrenamiento una representación vectorial para cada posición dentro del contexto definido. Este método ofrece mayor flexibilidad y capacidad de adaptación a los datos, aunque limita la generalización a secuencias de longitud distinta a la utilizada durante el entrenamiento.

\subsection{Masked Multi-head Attention}\label{subsec:multihead}

El componente principal de esta arquitectura son los llamados \texttt{mecanismos de atención}. Estos sopesan la importancia de distintos tokens en la secuencia de entrada. En sí, se puede decir que cada palabra mira al resto para ver cuáles tienen mayor importancia a la hora de entender el contexto. Por ejemplo, "El perro ladra en el prado", la palabra "ladra" tendrá un peso mayor en "perro" que "prado". 

En la práctica, se emplean múltiples mecanismos de atención en paralelo, es lo que se conoce como \texttt{multi-head attention}. Cada una de estas cabezas aprende una proyección distinta del espacio de representaciones, lo que permite capturar diferentes tipos de dependencias(sintácticas, semánticas,etc.) entre las palabras. Finalmente, las salidas de todas las cabezas se concatenan y combinan, proporcionando una representación más rica y completa del contexto.

El proceso de calcular esos valores de importancia requiere de 3 elementos: los vectores Q,K y V. Donde cada W asociada a esos elementos son una matriz de pesos entrenables \parencite{analytics2020qkv}.

\subsubsection{Query (Q)}
Refieren a los embeddings de los tokens de la secuencia de entrada y puede entenderse como lo que se está buscando. Siguiendo con el ejemplo de la frase anterior, para el vector correspondiente con el token "El", este valor se calcula: \[W_{El} * W_{Q}\]

\subsubsection{Key (K)}
Se entiende como lo que ofrece cada token. Por ejemplo, el token "perro" puede ofrecer: "sustantivo, sujeto, animal,etc.". Siguiendo con el ejemplo de la frase anterior, para el vector correspondiente con el token "El", este valor se calcula: \[W_{El} * W_{K}\]

\subsubsection{Value (V)}
La información real que se transmite, si es relevante. Siguiendo con el ejemplo de la frase anterior, para el vector correspondiente con el token "El", este valor se calcula: \[W_{El} * W_{V}\]

El proceso de atención se puede comparar con la búsqueda de un video en YouTube. Esta plataforma almacena sus videos en un diccionario clave-valor, siendo la clave el nombre. Cuando se realiza una búsqueda (Query) se calcula la similitud con esas claves (Key) para devolver el resultado.

Como punto de partida se usa el vector de embeddings calculado previamente, y en base a este se calcularían los vectores anteriores, que se pueden entender como 3 versiones de dicho embedding. Para su cálculo, se aplican transformaciones lineales que encuentran la mejor combinación de pesos y tras esto se calcula la atención en base a la siguiente expresión:
\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
\]

\[
QK^{T}
\]
\noindent\makebox[\linewidth][c]{\parbox{.9\linewidth}{\small
Matriz de similitudes. Con \emph{dot product}, el valor es grande si los vectores son similares y pequeño si apuntan en distinta dirección.
}}

\[
\mathrm{softmax}\!\left(\frac{QK^{T}}{\sqrt{d_k}}\right)V
\]
\noindent\makebox[\linewidth][c]{\parbox{.9\linewidth}{\small
Como el \emph{dot product} puede dar valores grandes, que hagan que softmax se concentre en ellos, se normaliza con $\sqrt{d_k}$ para estabilizar. La función \textit{softmax} produce una distribución de probabilidades y, al multiplicar por $V$, se obtiene una combinación ponderada que añade contexto a los embeddings.
}}

En estos mecanismos de atención se incluye habitualmente el término \textit{masked}, ya que se emplean máscaras con el fin de restringir la información a la que tiene acceso el modelo durante el cálculo de la atención. Estas máscaras permiten, por ejemplo, impedir que el modelo considere tokens futuros que aún no deberían ser visibles en una tarea de generación autoregresiva, o bien ignorar tokens que no contienen información semántica relevante. 

Entre los tipos de máscaras más comunes se encuentra la \textit{Padding Mask}, que se aplica para ignorar los tokens de relleno, que no contienen información semántica, añadidos a las secuencias con el fin de que todas tengan la misma longitud dentro de un lote de entrenamiento. También se utiliza la \textit{Sequence Mask}, que sirve para ocultar determinadas partes de la secuencia de acuerdo con un criterio específico. Por último, la \textit{Look-ahead Mask} (o \textit{Causal mask}) se emplea en modelos autoregresivos para evitar que la predicción de un token en la posición $t$ dependa de información futura, garantizando así que las predicciones en una posición concreta solo tengan en cuenta los tokens anteriores o la misma posición \parencite{swarms_masking_pytorch}.


\subsection{Add \& Norm \parencite{sharma2024addnorm}} \label{subsec:addnorm}

En la arquitectura Transformer aparece reiteradamente una capa denominada \textit{Add \& Norm}. La principal finalidad de esta capa es estabilizar y mejorar el entrenamiento y se divide en los dos procesos siguientes:

\subsubsection{Conexión Residual (\textit{Add})}
En redes profundas con arquitecturas complejas los pesos pueden tomar valores muy pequeños, hasta el punto de desaparer (\textit{vanishing gradients}) o demasiado grandes (\textit{exploding gradients}) al retropropagar hacia atrás, lo que conlleva que la red deje de aprender o sea muy inestable. Para solucionar estos problemas, en 2015 nace ResNet una arquitectura en la que la entrada de una capa, pasa a su salida directamente, saltándose las operciones intermedias. Es decir, a nivel simplificado se tendría: 

\[
y = F(x) + x
\]

Donde $x$ es la entrada de la capa anterior, $y$ la salida de la capa actual y $F(x)$ la función de activación correspondiente a la capa actual.

Con ello, la capa ya no tiene que aprender la función completa $F(x)$, sino solo la diferencia respecto a la identidad. Esto asegura que siempre exista al menos una señal que retroceda hasta las capas iniciales durante el entrenamiento.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/ResNet.png}
    \caption{Ejemplo gráfico conexión residual}
    \label{fig:placeholder13}
\end{figure}

Se entiende mejor al calcular las derivadas parciales para actualizar los pesos en \textit{backpropagation}. Como se observa en la siguiente expresión, el gradiente nunca se anula completamente gracias al término de identidad: 

\[
\frac{\partial \mathcal{E}}{\partial x}
= \frac{\partial \mathcal{E}}{\partial y}
    \frac{\partial y}{\partial x}
= \frac{\partial \mathcal{E}}{\partial y}
    \left( \frac{dF(x)}{dx} + 1 \right).
\]

De esta manera, incluso cuando $\frac{dF(x)}{dx}$ tiende a cero, el gradiente conserva la contribución de la identidad ($+1$). Así, la red es capaz de mantener un flujo estable de información hacia atrás, evitando que los gradientes mueran. Además, puede aprender a ignorar aquellas transformaciones que no sean beneficiosas en el entrenamiento, llevando sus pesos hacia cero y dejando pasar directamente la señal original $x$ \parencite{apxml_addnorm}. 

\subsubsection{Normalización (\textit{Layer Norm})}

Tras la conexión residual, se aplica una capa de normalización. A diferencia de otras capas de normalización, como la normalización por lotes (\textit{Batch Norm}), que normaliza los datos a lo largo del lote de entrenamiento, la normalización por capas (\textit{Layer Norm}) lo hace para cada muestra individual en sus características internas. Esto es especialmente importante en NLP, donde las secuencias de entrada pueden variar en longitud y contenido y no interesa que cada una se vea afectada por estadísticas de otra. Además, si se usase la normalización por lotes, se haría \textit{padding}, y los valores de relleno afectarían a las estadísticas de la normalización, especialmente en frases cortas. A continuación, se muestra un ejemplo que plasma claramente la diferencia entre ambas técnicas:

Dadas las siguiente secuencias: 

\[
\text{secuencia}_1 = [2,3], \quad \text{secuencia}_2 = [4,5]
\]

La normalización por lotes se aplica a lo largo del batch:
\[
\mu_1 = \frac{2 + 4}{2} = 3, 
\quad 
\sigma_1 = \sqrt{\frac{(2-3)^2 + (4-3)^2}{2}} = 1
\]

Mientras que la normalización por capa se aplica dentro de cada secuencia considerando todas sus características, así la posición 1:
\[
\mu_{\text{seq1}} = \frac{2 + 3}{2} = 2.5, 
\quad 
\sigma_{\text{seq1}} = \sqrt{\frac{(2-2.5)^2 + (3-2.5)^2}{2}} = 0.5
\]

\[
\mu_{\text{seq2}} = \frac{4 + 5}{2} = 4.5, 
\quad 
\sigma_{\text{seq2}} = \sqrt{\frac{(4-4.5)^2 + (5-4.5)^2}{2}} = 0.5
\]

Con ello, en base a estas dos operaciones, de media y varianza, se realzia la normalización de los valores de entrada en cada capa aplicando la siguiente expresión: 

\begin{equation}
    \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
    \label{eq:normalization_sigma}
\end{equation}

En dicha expresión se observan dos parámetros entrenables, $\gamma$ y $\beta$, que permiten al modelo aprender la escala y el desplazamiento óptimos para cada característica. Sin estos parámetros, la normalización es demasiado restrictiva, pues los resultados se aplanan siempre a un mismo rango. El término $\epsilon$ es un valor pequeño añadido para evitar divisiones por cero y garantizar la estabilidad numérica.

Otra de las ventajas de normalizar es que mitiga el \textit{internal covariate shift} \parencite{sharma2024addnorm}. Durante el entrenamiento las distribuciones de las activaciones cambian, dado que los pesos cambian, lo que ralentiza la convergencia \parencite{apxml_addnorm}, haciendo el entrenamiento más inestable y el optimizador pierde tiempo corrigiendo desajustes. Gracias a la normalización, las variables se mantienen en una escala controlada.


\subsection{Feed Forward} \label{subsec:feedforward}
En términos generales se trata del proceso por el que la información pasa a través de una red, únicamente en sentido hacia delante, desde su entrada hasta su salida, pasando por todas las capas ocultas que hubiese. 

Se define como una capa o bloque que toma la salida de la capa de atención y la procesa a través de una red neuronal densa, esto es, totalmente conectada. Esta red consta de dos capas lineales con una función de activación no lineal entre ellas, que en el paper original es ReLU. La primera capa expande la dimensionalidad del embedding, mientras que la segunda la reduce de nuevo a su tamaño original. Esto permite al modelo aprender representaciones más complejas y captar interacciones no lineales, lo cual es un factor importante pues permite, por ejemplo, que el modelo entienda el contexto y significado de una palabra en función de donde aparece en la frase \parencite{kyeg_feedforward_demystified}.

Pese a que originalmente se usaban 2 capas lineales con ReLU, en la actualidad se ha demostrado una mejora significativa al aumentar el número de capas, obteniendo un entrenamiento más estable, pues se vió que el aprendizaje al inicio era mayor, y reduciendo el problema por la desaparición del gradiente. Además, se ha demostrado ampliamente que es un bloque fundamental, pues en varios experimentos, al eliminar esta capa, el rendimiento del modelo disminuye drásticamente \parencite{gerber2025ffn}.

\subsection{Linear y softmax}
Las últimas capas del modelo son una capa lineal y softmax. A través de la primera se obtienen los denominados \textit{logits}, que son puntuaciones sin normalizar sobre que tan probable es cada token para el modelo. La función de esta capa es transformar los embeddings de alta dimensión en una representación que pueda ser interpretada como una distribución de probabilidad sobre el vocabulario.

A través de \textit{softmax}, se transforman estas puntuaciones en una distribución de probabilidad, donde cada valor representa la probabilidad de que un token específico sea el siguiente en la secuencia generada. 

%--- FIN DEL CAPÍTULO 3

\clearemptydoublepage

%--- CAPÍTULO 4 DEL TFM
\chapter{Resultados}

% FUENTES DE DATOS
\section{Adquisión y análisis de las fuentes de datos}
En este proyecto se han seleccionado dos datasets distintos: Tiny Shakespeare y WikiText2.

\subsection{Tiny Shakespeare}
Se trata de un dataset creado por Andrej Karpathy compuesto por $40.000$ líneas con las obras de Shakespeare. El dataset está disponible en HuggingFace \parencite{huggingface_tinyshakespeare} y puede obtenerse con el comando \textit{wget} de la siguiente forma:

\texttt{wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tinyshakespeare.txt}
A continuación se adjunta una porción del dataset original: 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/tiny_shakespeare.png}
    \caption{Fragmento del dataset Tiny Shakespeare}
    \label{fig:placeholder1}
\end{figure}

\subsubsection{Análisis exploratorio de datos (EDA)}
En este análisis se extrae información estadística del número de caracteres, palabras, oraciones y líneas, así como longitud de las mismas. Además, se realiza un análisis sobre como se distribuyen las palabras en el corpus y se comprueba si se cumple la ley Zipf \parencite{zhou2024zipf_nlp} que indicaría si tiene una estructura típica del lenguaje natural. Los resultados obtenidos se indican a continuación.

El texto original consta de $40000$ líneas, $12449$ oraciones, $200837$ palabras y $1115394$ caracteres. La longitug media de palabras de es $4.13$ caracteres y de las oraciones de $20.46$. Del total de palabras del corpus se ha identificado un $48.42$\% de \textit{stop words} y un $71.86$\% de las palabras se repiten menos de 5 veces. No se ha identificado ningún caracter raro que no perteneciese a caracteres alfanuméricos o signos de puntuación. A continuación se presentan dos histogramas de como se distribuyen las longitudes de las palabras y las oraciones. Como se puede observar, la mayoría de los términos constan entre tres y seis caracteres, un comportamiento que concuerda con los textos en lenguaje natural y en concreto, con la naturaleza de este texto, pues son obras de Shakespeare escritas en un inglés de finales del siglo XVI y comienzos del XVI. Por otro lado, en las oraciones predominan estructuras breves, lo que se corresponde con la naturaleza de diálogos cortos que componen las obras de Shakespeare.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/hist_long_w_shake.png}
    \caption{Distribución de frecuencias longitud de palabras}
    \label{fig:placeholder34}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/hist_long_s_shake.png}
    \caption{Distribución de frecuencias longitud de oraciones}
    \label{fig:placeholder35}
\end{figure}

Se ha utilizado \textit{wordcloud} para visualizar las palabras más frecuentes. Tal y como muestra la siguiente imagen predominan palabras cortas como \textit{Thy, thou o thee} que son pronombres antiguos del inglés, hoy en día reemplazados por \textit{you o your}. También aparecen palabras típicas de la literatura de William Shakespeare, como \textit{love, king o death} y personajes principales de las obras: \textit{king richard o angelo}. Por último, varios verbos aparecen con frecuencia: \textit{hath (hash), make o say}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/word_count_shake.png}
    \caption{Wordcounter}
    \label{fig:placeholder36}
\end{figure}

Por último, la siguiente gráfica muestra que se cumple la ley Zipf. En ella se muestra la relación entre la frecuencia de las palabras y su posición en el ranking de uso. Se observa como la frecuencia de las palabras disminuye de manera inversamente proporcional a su rango, lo cual cumple con la Ley de Zipf y por tanto, con el comportamiento estadístico del lenguaje natural, donde las palabras más comunes aparecen con mucha más frecuencia que las menos comunes. Como las palabras que se repiten con alta frecuencia no son numerosas y se observa un rápido decaimiento en la gráfica se justifica el uso de la tokenización Byte Pair Encoding (BPE) para tratar el problema de los tokens desconocidos.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/zipf_shake.png}
    \caption{Ley Zipf}
    \label{fig:placeholder37}
\end{figure}

\subsection{WikiText2}
Este dataset contiene una colección de textos seleccionados de Wikipedia \parencite{huggingface_wikitext} \parencite{autonlp2020linkedwikitext}. Se puede descargar de Salesforce en Huggingface y consta de más de 2 millones de tokens de datos, divididos en $4358$ ejemplos para test, $36718$ para entrenamiento y $3760$ para validación. 

\subsubsection{Análisis exploratorio de datos (EDA)}

Se ha seguido el mismo patrón de análisis que para el dataset anterior. En este caso, la información es completamente diferente, tanto en contenido como estructura. Ya no se organiza en versos con una estructura de conversación y en un inglés antiguo, sino que se trata de párrafos agrupados en secciones de artículos indicadas con el siguiente formato: \textit{= = = Título = = =}, donde cada "=" representa un nivel jerárquico o sección para el tema principal. En el siguiente ejemplo se entiende mejor este detalle. Se tiene un tema principal que es el videojuego "Valkyria Chronicles III" y a continuación comienza la sección "Gameplay" que explica que tipo de videojuego es. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/wikitext2.png}
    \caption{Sección extraída de Wikitext-2}
    \label{fig:placeholder43}
\end{figure}

El dataset consta de $58238$ líneas, $96046$ oraciones, $2034076$ palabras y $13349882$ caracteres. La longitud promedio de palabras y oraciones es similar al dataset Tiny Shakespeare, siendo un poco mayor estas últimas con un valor de $4.94$ caracteres para las palabras y $26.71$ para las oraciones. Del total de palabras del corpus se ha identificado un $41.74$\% de \textit{stop words} y un $67.58$\% de las palabras se repiten menos de 5 veces. Se han identificado caracteres denominados raros, esto es, que no se trata de caracteres alfanuméricos o signos de puntuación en el inglés. Estos suponen un $0.1248$\% del total. Algunos de ellos se incluyen en la siguiente imagen:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/caracteres_raros.png}
    \caption{Caracteres detectados no alfanuméricos en el idioma inglés}
    \label{fig:placeholder38}
\end{figure}

A continuación se presentan dos histogramas de como se distribuyen las longitudes de las palabras y las oraciones. Se observa un sesgo claro hacia la izquierda con una disminución progresiva a medida que la longitud de las palabras se incrementa, identificandose palabras de más de $15$ caracteres. Las palabras de entre $3$ y $6$ caracteres son las más comunes, alcanzando frecuencias superiores a $400000$. En el caso de las oraciones también existe un sesgo hacia la izquierda, concentrándose la mayoría entre $1$ y $30$ caracteres.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/hist_long_w_wiki.png}
    \caption{Distribución de frecuencias longitud de palabras}
    \label{fig:placeholder40}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/hist_long_s_wiki.png}
    \caption{Distribución de frecuencias longitud de oraciones}
    \label{fig:placeholder41}
\end{figure}

Como palabras más frecuentes, tal y como muestra la siguiente imagen predominan palabras cortas como \textit{two, one o new}. Estas palabras son muy generales, sin reflejar un tema específico, lo que representa la naturaleza de este tipo de datasets. Además, confirman lo visto hasta ahora, pues las palabras más frecuentes contienen pocos caracteres, lo cual es común en el lenguaje natural.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/word_count_wiki.png}
    \caption{Wordcounter}
    \label{fig:placeholder39}
\end{figure}

Se ha realizado también un análisis de la leu Zipz. Como se comprueba en la imagen \ref{fig:placeholder42}, sigue un patrón típico del lenguaje natural, con una cola larga que indica un elevado número de palabras poco frecuentes.
\begin{figure}[h]

    \centering
    \includegraphics[width=0.5\linewidth]{img/zipz_wiki.png}
    \caption{Wordcounter}
    \label{fig:placeholder42}
\end{figure}

% LIMPIEZA Y TOKENIZACIÓN

\section{Limpieza y tokenización}

En lo referido a la limpieza del texto existen diversas técnicas, como limpiar etiquetas HTML, caracteres especiales, stop words, duplicados, convertir a minúsculas, solucionar problemas de encoding, lematización o corrección de palabras (librerías de Python como SpellChecker permiten realizar esta tarea) \parencite{shabbir2021cleaning}. En este proyecto, dado que los datasets empleados ya están correctamente procesados y se va a utilizar un tokenizador Byte-Pair encoding (BPE) a través de SentencePiece, no es necesario un pre-procesamiento demasiado exhaustivo, se limita a corrección de posibles problemas de codificación utilizando ftfy, espacios en blanco y normalización de comillas y guiones usando unicode.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/preclean.png}
    \caption{Limpieza antes de tokenizar}
    \label{fig:placeholder2}
\end{figure}

\subsection{Implementación del tokenizador BPE}
Una vez el texto está preparado se inicia la implementación del proceso de tokenización. El primer paso es entrenar el tokenizador, de forma que aprenda el vocabulario y a dividir en subpalabras. Se establece el tipo de modelo, "bpe" en este caso, el tamaño del vocabulario final, la porción de caracteres del corpus que debe cubrir el modelo (1 para cubrir todos) y el número de hilos de CPU a usar durante el entrenamiento. Además, se utiliza el parámetro \textit{byte\_fallback} para controlar los caracteres desconocidos, tal y como se ha explicado previamente. Se aplica una normalización NFKC (descompone los caracteres en formas básicas y los recompone siguiendo una forma estándar) antes de entrenar donde se estandarizan caracteres y eliminan espacios innecesarios o múltiples. Esto es algo que conviene usar con lenguajes como el japonés que utilizan caracteres desnormalizados o textos sucios y desnormalizados, sin embargo, en este caso, los datasets empleados son en inglés y ya están listos para usarse y se podría evitar. Por último, se asignan los IDs especiales:

 \begin{itemize}
     \item pad: token de padding para rellenar secuencias de distinta longitud.
     \item unk: Tokens desconocidos, pese a estar controlados con el parámetro mencionado.
     \item bos: Token de inicio de secuencia, para dar consistencia.
     \item eos: Token de fin de secuencia. Permite al modelo a aprender a parar.
 \end{itemize}
 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/sentence_trainer.png}
    \caption{Enter Caption}
    \label{fig:placeholder3}
\end{figure}
 

El entrenamiento genera dos archivos que contienen el modelo de segmentación y el vocabulario aprendido. Posteriormente, se inicializa un objeto \texttt{SentencePieceProcessor} a partir del modelo entrenado, y se procede a tokenizar los conjuntos de datos (entrenamiento, validación y prueba) mediante el método \texttt{sp.encode}, que devuelve una lista de identificadores numéricos correspondientes a las subpalabras detectadas en el texto. 

Finalmente, la función imprime el número total de tokens generados y devuelve tanto la secuencia de identificadores (\texttt{tok\_ids}) como el objeto \texttt{SentencePieceProcessor}, que permitirá revertir la tokenización o aplicarla en futuros conjuntos de datos.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/limpieza_tokeniz_train.png}
    \caption{Enter Caption}
    \label{fig:placeholder30}
\end{figure}

% EMBEDDINGS 

\section{Embeddings y Positional Encodding}
Cualquier modelo basado en redes neuronales requiere que sus entradas sean valores numéricos, no arbitrarios, para poder trabajar con el lenguaje humano. Es cierto que los tokens del texto ya se han convertido a índices enteros usando técnicas de tokenización, no obstante, estos valores no son suficientes para que el modelo sea capaz de inferir relaciones semánticas entre las palabras y llegar a entender su significado, es por ello que cada token se representa mediante un vector denso de características, conocido como \textit{embedding}.

Para mapear los índices resultantes de la tokenización a una matriz de \textit{embeddings}, se implementa una capa de embedding utilizando \texttt{PyTorch}. Esta capa actúa como una tabla de búsqueda que asocia cada índice con un vector de características. Internamente, se trata de una matriz de pesos que se optimiza durante el proceso de entrenamiento del modelo.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/embeddings1.png}
    \caption{Embeddings}
    \label{fig:placeholder17}
\end{figure}

Cabe destacar que inicialmente la matriz de embeddings es aleatoria. A medida que se entrena el modelo, estos vectores se ajustan mediante retropropagación, lo que reduce la función de pérdida y permite que los embeddings capturen mejor la semántica, el contexto y otras relaciones relevantes entre los tokens. El resultado final es un tensor en el que cada fila representa un vector denso asociado a un token del vocabulario, adaptado a la tarea específica \parencite{bao2022embedding}. Se ha decidido inicializar los pesos siguiendo una inicialización Normal (ver sección \ref{sec:normal} del Anexo), lo que permite que inicialmente sean valores cercanos a 0, evitando que algunos tokens tengan valores demasiado grandes. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/normal_init.png}
    \caption{Inicialización Normal}
    \label{fig:placeholder18}
\end{figure}

% ATENCION

\section{Mecanismos de atención}
La base de la arquitectura Transformer es la atención. A través de la atención, cada palabra mira simultáneamente al resto y calcula una métrica de relevancia a través de la cual, se determina que fracción de la información real es relavante para cada token y cpor tanto, para el modelo. 

Como se ha mencionado previamente, se basa en 3 matrices entrenables, inicializadas originalmente con valores aleatorios, y una operación, softmax, que calcula una distribución de probabilidad para cada token (ver \cref{subsec:multihead}).

Usando \texttt{PyTorch} es posible trabajar de forma sencilla con tensores y replicar toda la lógica operacional. En primer lugar, se implementa un mecanismo de atención aplicando una máscara causal, a continuación se desarrolla la lógica de múltiples cabezas para capturar distintas relaciones entre los tokens.

El primer punto es crear las matrices de pesos: 
\[W_Q, W_K, W_V\]

Aquí, PyTorch ofrece el módulo \texttt{nn.Linear}, que lo que hace es aplicar una transformación lineal a los datos de entrada \parencite{kanaries_nnlinear}, incializando aleatoriamente una matriz de pesos y un vector de sesgos, el cual, en este caso, no se va a inicializar pues no interesa que los vectores se desplacen en el espacio. Este módulo, toma dos parámetros: \textit{in\_features} y \textit{out\_features}, y la matriz resultante tiene un tamaño: $\textit{in\_features} * \textit{out\_features}$.

De este modo, a partir de la misma entrada $x \in \mathbb{R}^{B \times T \times d_{model}}$ (secuencia de embeddings), se obtienen tres vistas distintas que serán usadas posteriormente en el cálculo de la atención.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/Q_K_V.png}
    \caption{Proyección matrices Q, K, V}
    \label{fig:Proyeccion8}
\end{figure}

Los siguientes pasos son crear la métrica de atención, normalizar con softmax y aplicar los \textit{scores} a la información real. Con PyTorch es muy sencillo de realizar a través de las funciones \textit{matmul}, que permite multiplicar tensores, y \textbf{softmax}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/atencion.png}
    \caption{Implementación Self Attention}
    \label{fig:placeholder9}
\end{figure}

Por último, queda aplicar la máscara de atención. Dado que el modelo implementado es de carácter autorregresivo y emplea ventanas de tamaño fijo sin añadir \textit{padding}, únicamente es necesaria la máscara causal. Su implementación resulta sencilla: se construye una matriz triangular superior de unos, cuyos valores se reemplazan por $-\infty$. Esta operación se aplica antes de la normalización con la función \textit{softmax}, de manera que las posiciones enmascaradas reciben una probabilidad prácticamente nula y, en la práctica, el modelo las ignora.  

Por ejemplo, para una secuencia de longitud $T=4$, la matriz de \textit{metrica\_atencion} $S$ se transforma en:  

\[
metrica\_atencion' =
\begin{bmatrix}
s_{11} & -\infty & -\infty & -\infty \\
s_{21} & s_{22} & -\infty & -\infty \\
s_{31} & s_{32} & s_{33} & -\infty \\
s_{41} & s_{42} & s_{43} & s_{44}
\end{bmatrix}
\]

De este modo, cada token únicamente puede atenderse a sí mismo y a los anteriores, lo que garantiza la naturaleza autorregresiva del modelo.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/masked.png}
    \caption{Implementación máscara causal}
    \label{fig:placeholder10}
\end{figure}

\subsection{Implementación de las múltiples cabezas de atención}

Para poder aplicar la atención múltiple las matrices originales Q, K, V se reorganizan en múltiples cabezas. Para ello, se emplea la función \texttt{view} de \texttt{PyTorch}, que permite reorganizar el tensor sin modificar sus datos en memoria. En este caso, se pasa de una estructura de tamaño $(B, T, d_{model})$ a otra de tamaño $(B, \textit{num\_heads}, T, d_k)$, donde $d_k = d_{model} / \textit{num\_heads}$ es la dimensión de cada cabeza. Posteriormente, se aplica una transposición para situar la dimensión de las cabezas en la segunda posición:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/heads.png}
    \caption{Implementación Multi-Head Attention}
    \label{fig:placeholder11}
\end{figure}

De este modo, cada cabeza de atención opera de manera independiente sobre un subespacio de dimensión $d_k$, calculando sus propios valores de similitud ($QK^T$) y generando una salida parcial. Finalmente, las salidas de todas las cabezas se concatenan de nuevo y se proyectan mediante una capa lineal adicional, como se puede ver en la imagen \ref{fig:placeholder12}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/heads2.png}
    \caption{Salida Multi-Head Attention}
    \label{fig:placeholder12}
\end{figure}

Por último, mencionar que la inicialización de los pesos se realiza mediante el método \textit{Xavier Uniform} (ver sección \ref{sec:xavier} del Anexo), que estabiliza el entrenamiento en capas profundas. 

% Add & Norm
\section{Normalización \texttt{(Add \& Norm)}}
Como se ha explicado en \ref{subsec:addnorm}, esta capa consta de dos elementos. Primero, una conexión residual que suma la entrada original a la salida del bloque de atención múltiple. En segundo lugar, una capa de normalización por capas que estabiliza y acelera el entrenamiento. En la imagen \ref{fig:placeholder14} se visualiza mejor el flujo.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/esquema1.png}
    \caption{Flujo capa Add \& Norm}
    \label{fig:placeholder14}
\end{figure}

Como se puede ver en la imagen \ref{fig:placeholder26}, la conexión residual se realiza mediante una simple suma de tensores. La capa de normalización se ha implementado de cero, aunque \texttt{PyTorch} ofrece el módulo \texttt{nn.LayerNorm}, que realiza la misma función. En la implementación se calcula la media y la varianza por posición y devuelve el resultado de la expresión \eqref{eq:normalization_sigma}. 

Hay que señalar aquí el uso de \texttt{nn.Parameter} para los parámetros entrenables $\gamma$ y $\beta$. Se tratan como tensores especiales y se registran como parámetros entrenables del modelo, en concreto, se asocian a este módulo. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/add_norm.png}
    \caption{Implementación Add \& Norm}
    \label{fig:placeholder26}
\end{figure}

\section{Feed Forward} 
Tal y como se ha explicado en la sección \ref{subsec:feedforward}, el rendimiento del transformer es significativamente mejor al ampliar el número de capas lineales. Siguiendo las recomendaciones del paper \parencite{gerber2025ffn}, se ha decidido implementar 3 capas utilizando como función de activación \textit{GELU} (ver sección \ref{sec:gelu} del Anexo), que ha demostrado ser más efectiva que ReLU en este tipo de arquitecturas. La inicialización de los pesos se realiza mediante el método \textit{Xavier Uniform} al igual que en las capas de atención, para mantener la estabilidad del entrenamiento.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/xavier_init.png}
    \caption{Inicialización Xavier Uniform}
    \label{fig:placeholder15}
\end{figure}

La implementación es relativamente sencilla, se haría como si se tratara de un perceptrón multicapa \parencite{kyeg_feedforward_demystified}, donde se incluyen 3 capas lineales y dos funciones de activación GELU entre ellas. Además, se introduce \textit{dropout} para evitar sobreajuste, apagando aleatoriamente algunas neuronas durante el entrenamiento.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/ffnn.png}
    \caption{Capa FeedForward}
    \label{fig:placeholder16}
\end{figure}



\section{Entrenamiento}

Tras añadir la capa lineal de salida al final del bloque Transformer, se completan los componentes fundamentales de la arquitectura. El siguiente paso consiste en definir los parámetros y establecer la infraestructura necesaria para el entrenamiento del modelo.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/gpt2.png}
    \caption{Modelo final}
    \label{fig:placeholder19}
\end{figure}

\subsection{Configuración de parámetros de entrenamiento}

En lo referido a la configuración de parámetros se han seguido inicialmente las recomendaciones dadas en el \textit{paper: Attention Is Not All You Need} \parencite{gerber2025ffn}, teniendo en cuenta las características de este proyecto, tanto a nivel de \textit{hardware} como de la arquitectura implementada. No obstante, como se explicará en la sección \ref{subsec:fine-tuning}, se han realizado ciertos ajustes para mejorar el entrenamiento.

Para el prototipo inicial, entrenado con el dataset \textit{Tiny Shakespeare}, se ha definido un vocabulario compuesto por $16000$ tokens, el tamaño de los embeddings es de $512$ tokens y la ventana de contexto de $256$. Se han utilizado $8$ cabezas de atención, para capturar bien distintos patrones y el número de capas del \textit{transformer} es de $6$, un número no muy elevado por las limitaciones técnicas del proyecto. En cuanto al bloque \textit{FeedForward}, se define la dimensión \textit{d\_ff} con un valor de $2048$, expandiendo cada vector de $512$ a $2048$ dimensiones. Para no saturar la memoria de la GPU el número de batches se ha mantenido en $64$, pese a incluir ciertos elementos regulatorios que se explicarán a continuación. Por último, se ha entrenado para $20$ épocas.


\subsection{Elementos regulatorios}

Para el entrenamiento del modelo se ha planteado inicialmente la técnica \texttt{Mixed Precision Training}. Esto es porque se ha visto que reduce el uso de memoria y acelera el entrenamiento sin comprometer la precisión del modelo, lo que permite aumentar el tamaño de los batches y no desbordar la memoria de la GPU. En esencia, se utilizan tipos de datos de menor precisión (como \texttt{float16}) para las operaciones que no requieren alta precisión, mientras que las operaciones críticas se mantienen en \texttt{float32}. 

Para una primera implementación se utilizan dos elementos clave. Por un lado \texttt{GradScaler}, que escala los valores de la función de pérdida dinámicamente para prevenir el desbordamiento de los gradientes cuando se usan tipos de datos de menor precisión. Lo que hace es multiplicar la pérdida por un factor grande antes de calcular los gradientes. En cada iteración se decide si ese factor fue bueno o malo, en base a si ha habido desbordamiento o no. Si no hubo desbordamiento, se aumenta ligeramente el factor para incrementar la precisión. En caso contrario, se reduce el factor para la próxima iteración \parencite{amit2024mixedprecision}. Por otro lado, \texttt{autocast}, que permite especificar bloques de código donde se desea utilizar precisión mixta \parencite{stack_overflow_gradscaler}. Dentro de estos bloques, las operaciones se ejecutan automáticamente en la precisión más adecuada según el hardware y la operación específica. No obstante, hay casos en los que se debe indicar a \texttt{autocast} de forma manual que no modifique la precisión de los datos, por ejemplo, en la capa de normalización, donde al no ser nativa de \texttt{PyTorch}, pues se ha implementado desde 0, quizá no lo detecta automáticamente \parencite{amit2024mixedprecision}. También se ha desactivado en la capa de \texttt{Softmax} pues son capas donde escalar puede introducir inestabilidad al modelo.

En combinación con estas técnicas, se ha aplicado \texttt{Gradient Clipping} para evitar que los gradientes se vuelvan demasiado grandes y provoquen actualizaciones inestables de los pesos. Lo que hace es reducir su magnitud, sin alterar la dirección. 


No obstante, al entrenar el modelo, se han obtenido valores nulos en la función de pérdida, por lo que se ha decidido prescindir de \texttt{GradScaler} y \texttt{autocast}, haciendo ajustes en los hiperparámetros del optimizador e implementando \texttt{CosineAnnealingLR} \parencite{pytorch_cosineannealinglr}, con ello lo que se logra es reducir el ratio de aprendizaje por época y lote de entrenamiento. Pese a ello, no se ha observado una mejora significativa en los datos de validación, por lo que se ha decidido implementar un proceso de \textit{warm-up} empleando la clase de PyTorch \texttt{LambdaLR}. Todo este proceso de ajuste de hiperparámetros se presenta mejor en la sección \ref{subsec:fine-tuning}.



\subsection{Función de pérdida}

En este tipo de modelos, las métricas tradicionales no capturan adecuadamente los matices que determinan el rendimiento del modelo. Por ello, se ha optado por utilizar la \textit{Cross Entropy Loss} como función de pérdida. Esta métrica mide la diferencia entre la distribución de probabilidad predicha por el modelo y la distribución real de los datos. 
Matemáticamente, se define como:

\[
\mathcal{L}_{CE} = - \sum_{w \in \mathcal{V}} Y_t[w] \, \log \hat{Y}_t[w],
\]

donde $\mathcal{V}$ representa el vocabulario del modelo, $Y_t[w]$ es la distribución real del token correcto en el paso temporal $t$ y $\hat{Y}_t[w]$ es la probabilidad predicha para dicho token tras aplicar la función \textit{softmax}.

Durante la implementación, se emplea la función \texttt{nn.CrossEntropyLoss()} de \texttt{PyTorch}, que combina internamente la operación \textit{softmax} y el cálculo de la entropía (ver sección \ref{sec:normal} del Anexo) en un solo paso:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/cross_entropy.png}
    \caption{Función de pérdida del modelo \parencite{geeksforgeeks_crossentropy}}
    \label{fig:placeholder20}
\end{figure}

Dado que la función \texttt{CrossEntropyLoss} recibe como entrada dos tensores, uno con las predicciones del modelo (\textit{logits}) y otro con las etiquetas reales, sus dimensiones deben ajustarse al formato esperado. El primer tensor debe tener forma \texttt{[N, C]}, donde \texttt{N} representa el número total de tokens que el modelo intenta predecir y \texttt{C} corresponde al número de posibles tokens del vocabulario. El segundo tensor debe tener forma \texttt{[N]}, conteniendo para cada posición el índice del token correcto. Por ello, los tensores se aplanan mediante \texttt{.view(-1, logits.size(-1))} para los \textit{logits} y \texttt{.view(-1)} para las etiquetas reales.

A través de esta función de pérdida, se calcula la perplejidad \parencite{keerthanams2025evaluating}, que mide la incertidumbre al predecir la siguiente palabra. Por ejemplo, una perplejidad de 10 indica el modelo, en promedio, es tan incierto como si tuviera que elegir uniformemente entre 10 opciones. Matemáticamente, se define como:
\[
\text{PPL} = e^{\mathcal{L}_{CE}}
\]

Una perplejidad baja indica que el modelo es bueno prediciendo la siguiente palabra, mientras que una alta sugiere que el modelo está confundido. Durante el entrenamiento, se monitoriza la perplejidad en los conjuntos de validación para evaluar el progreso del modelo y ajustar hiperparámetros si es necesario.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/perplexity.png}
    \caption{Calculo de perplejidad en PyTorch \parencite{stackoverflow_perplexity_pytorch}}
    \label{fig:placeholder21}
\end{figure}

El problema de la perplejidad \parencite{singh2024perplexitymatters} es que depende altamante del vocabulario, con lo que puede llegar a darse que un modelo bueno parezca malo debido a palabras poco comunes en vocabularios extensos. En este caso, el vocabulario no es muy extenso y al usar \texttt{BPE} se reduce el número de palabras poco comunes o desconocidas, con lo que se ha considerado una métrica adecuada. Por otro lado, no valida la corrección semántica o contextual, por lo que a la hora de validar el modelo se han planteado otras técnicas como \texttt{MAUVE} \parencite{krishnap25_mauve_github} (ver sección \ref{sec:mauve} del Anexo) y \texttt{Distinct-n} (ver sección \ref{sec:distinct-n} del Anexo), además de la validación humana configurando la temperatura, como se explicará en la sección \ref{subsec:validacion}.

Por último, se ha implementado un mencanismo del tipo \textit{Early Stopping} por el cual si la perplejidad en los datos de validación no mejora durante un periodo consecutivo se finaliza el entrenamiento. Con esto se pretende evitar sobreajuste y que el modelo se ajuste demasiado bien a los datos de entrenamiento respecto a los de validación.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/early_stop.png}
    \caption{Calculo de perplejidad en PyTorch \parencite{stackoverflow_perplexity_pytorch}}
    \label{fig:placeholder23}
\end{figure}

\subsection{Optimizadores}

Se han probado 3 optimizadores distintos, pero a la vez altamente relacionados. En primer lugar el optimizador Stochastic Gradient Descent (SGD), quien introdujo un cambio respecto al tradicional descenso del gradiente, y es que el gradiente no se calcula sobre todo el conjunto de datos original, sino sobre una muestra aleatoria del conjunto de entrenamiento \parencite{geeksforgeeks_sgd}. Ciertas investigaciones indican que pese a utilizar un ratio de aprendizaje constante, lo que puede ocasionar una convergencia lenta, generaliza mejor que Adam y es buento evitando el sobreajuste \parencite{ultralytics_adam_glossary}. 

Otro de los optimizadores es Root Mean Square Propagation (RMSProp). Este optimizador mejora su rendimiento para funciones de pérdida no convexas, ajustando el ratio de aprendizaje, dado que un solo valor no funciona igual de bien para cada parámetro, haciendo que algunos pesos apenas cambien y otros cambien demasiado. Da estabilidad \parencite{kashyap2024rmsprop}.

Por último, se ha utilizado AdamW. Es un optimizador utilizado en GPT por su alta capacidad de generalización . Se diferencia de Adam en que desacopla el decaimiento del peso \textit{weight decay}, una regularización que castiga pesos grandes para evitar sobreajustes, del proceso de actualización del gradiente \parencite{datacamp_adamw_pytorch}.  Al igual que Adam, se basa en dos operaciones \parencite{yassin2024adamvsadamw}, por un lado el impulso o \textit{momentum}, y por otro lado el uso de un \textit{learning rate} adaptativo. El primer factor implica que no solo se considere el punto actual en cada paso durante el descenso por gradiente, si no que considera los pasados, así, se logran movimientos más suaves y un descenso más rápido y estable. El segundo factor hace que los ratios de aprendizaje se ajusten en base a los gradientes de los parámetros (RMSProp).

La implementación es muy simple y la dificultad reside más en ajustar los hiperparámetros. En el caso de AdamW se han seguido las recomendaciones dadas en \parencite{datacamp_adamw_pytorch}, que indica partir con un \textit{learning rate} de $10^-3"$ y un \textit{weight decay} de $10^-2"$ a $10^-4"$, siendo mayor para modelos más complejos. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/optimizers.png}
    \caption{Optimizadores \parencite{stackoverflow_sgd_optimizer}}
    \label{fig:placeholder22}
\end{figure}


\subsection{Resumen del modelo para Tiny Shakespeare}

En primer lugar se tokeniza el dataset de entrenamiento, que contiene $896135$ caracteres. Para ello se utiliza un tokenizador \textit{BPE} a través de \textit{SentencePiece}, que es entrenado con estos datos y aprende una reglas de segmentación y un vocabulario. A través de este tokenizador ya entrenado, se tokenizan los datos de entrenamiento, validación y test. Como el tamaño definido para el vocabulario es de $16000$ elementos, se obtiene un conjunto de números enteros que representan valores (índices) dentro de ese vocabulario con valores comprendidos entre $0$ y $15999$. En total se generan $265566$ índices para el dataset de entrenamiento. 

La función del modelo es dado cada token, predecir el siguiente. Es por ello que un paso previo a transformar este vector a embeddings se crea un dataset de \texttt{PyTorch} que, junto con \textit{DataLoader} genera ejemplos de secuencias de entrada y objetivos (se desplaza una posición la de entrada) y se agrupan en batches, que irán alimentando las capas de Embedding del modelo. Aquí cabe resaltar que, como el tamaño del batch es de $64$, el modelo recibe como entrada tensores con tamaño $(64,256)$. Para verlo mejor, es como si el modelo recibe: \textit{[El, perro, ladra, en, el]} y busca predecir \textit{[perro, ladra, en, el,prado]}, para cada token.


Cada uno de estos batch alimentan el modelo durante $20$ épocas. En primer lugar entran a las capas de embedding. Aquí los vectores de tamaño $(64,256)$ se convierten a vectores multidimensionales (embeddings) de tamaño $(64,256, 512)$, es decir, cada token, se convierte en un vector de $512$ elementos, y son sumados al \textit{Positional Embedding} para que el modelo sea consciente de la posición de cada token en la secuencia. 

Tras esto, los embeddings avanzan a los bloques principales de la arquitectura, que se replican $6$ veces. Estos, son los bloques de atención con máscara causal y 8 cabezas, las capas de normalización y adicción y Feed Forward, quien expande la entrada de tamaño $(64,256, 512)$ para añadir no linealidad y la vuelve a comprimir a su tamaño inicial. Estos bloques se apilan a través de \texttt{nn.ModuleList}. 

Finalmente, tras salir del bloque principal, la salida pasa de nuevo por una capa de normalización y finalmente se obtienen los \textit{logits} gracias a una capa lineal. 

El proceso se repite el número de épocas mencionado y se valida con datos de validación y \textit{CrossEntropy}, utilizando la perplejidad para determinar si el modelo aprende o no.


\section{LSTM}

Se ha implementado un modelo LSTM sencillo utilizando \texttt{Tensorflow} y entrenado con los mismos datos que el Transformer para contratar los resultados. 

Se ha utilizado \textit{sparse categorical crossentropy}, pues los datos de salida son enteros que representan las clases (tokens) y no vectores one-hot. Por otro lado, no es necesario introducir ningún tipo de \textit{padding} o \textit{bucketing} ya que se crean ventanas de longitud fija, por lo que todas las secuencias de entrada tienen la misma longitud. El LSTM recibe como entrada secuencias de tamaño $(batch\_size, context\_len)$, donde \textit{context\_len} es el tamaño de la ventana de contexto definida.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/window_lstm.png}
    \caption{Ventanas de contexto LSTM}
    \label{fig:placeholder24}
\end{figure}

El modelo implementado consta de dos capas LSTM de 512 y 256 neuronas respectivamente, seguidas de una capa densa con activación GELU (ver anexo \ref{sec:gelu}) y una capa de salida densa con activación \textit{softmax} para obtener las probabilidades de cada token en el vocabulario. Se ha combinado con una capa de convolución para capturar patrones locales en las secuencias de texto antes de que entren en las capas LSTM. Para evitar sobreajuste y mantener un entrenamiento estable se ha incluido \textit{dropout} entre las capas y normalización por lotes. Se ha empleado el mismo optimizador AdamW con un ratio de aprendizaje de $1e-4$ utilizado para el Transformer y un \textit{Early Stopping} igual a 5. El modelo ha sido entrenado durante 21 épocas con un tamaño de batch de $32$ y una ventana de contexto de $128$ tokens. Se ha observado un descenso de la perplejidad desde $195.79$ hasta $108.22$, valores peores que los obtenidos con el Transformer. Las curvas de pérdida se muestran en la imagen \ref{fig:placeholder33}:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/perplexity_lstm.png}
    \caption{Perplejidad LSTM durante el entrenamiento}
    \label{fig:placeholder24}
\end{figure}

Como se puede observar, no se aprecia sobreajuste y el entrenamiento se detuvo antes de que la perplejidad en los datos de validación empezase a aumentar respecto a los de entrenamiento. 

%--- FIN DEL CAPÍTULO 4

\clearemptydoublepage

%--- CAPÍTULO 5 DEL TFM
\chapter{Conclusiones}

\section{Primera aproximación con Tiny Shakespeare} \label{subsec:fine-tuning}

Se ha realizado un primer entrenamiento con $20$ épocas y un \textit{Early Stopping} con paciencia igual a 10. El optimizador utilizado es \texttt{AdamW} con un ratio de aprendizaje de $1.5*10^-3$ y \textit{weight decay} igual a $0.01$. Además se ha aplicado \texttt{CosineAnnealingLR} para ir ajustando durante el entrenamiento el ratio de aprendizaje, no solo a nivel de época, sino de batch. El resto de parámetros del modelo son: 

\begin{itemize}
    \item \textbf{Tamaño vocabulario}: $10000$
    \item \textbf{Tamaño embedding}: $512$
    \item \textbf{Tamaño contexto}: $256$
    \item \textbf{Tamaño de batches}: $64$
    \item \textbf{Tamaño FFNN}: $1024$
    \item \textbf{Número de batches por época}: $4146$
    \item \textbf{Número de cabezas de atención}: $8$
    \item \textbf{Número de capas}: $6$
    \item \textbf{Dropout}: $0.1$
\end{itemize}

Con ello, se han entrenado $11$ épocas antes del \textit{Early Stopping}, comenzando en la primera época con una pérdida de $6.23$ y una perplejidad de  $509.94$ para datos de entrenamiento y de $586.34$, para datos de validación y finalizando con una périda de $6.2226$ y perplejidad de $504.00$ y $598.18$ respectivamente. Se ve un descenso en los datos de entrenamiento, frente a un incremento en validación lo que suscita un sobreajuste, además no se aprecia un cambio significativo de los valores a lo largo de las épocas lo que hace intuir que el modelo no está aprendiendo nada significativo.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/resultado_entrenamiento_v1.png}
    \caption{Resultado primer entrenamiento}
    \label{fig:placeholder24}
\end{figure}

Para corregirlo se ha reducido el tamaño del vocabulario y el modelo, pues al ser un dataset pequeño, sin gran dispersión, se ha visto que esto da mejores resultados. Además, se ha ajustado el optimizador incluyendo como coeficientes de decaimiento exponencial los valores $0.9$ y $0.98$ \parencite{reddit_transformer_not_learning}. Para evitar divisiones por cero en el cálculo de los momentos, se ha añadido un valor $\epsilon$ igual a $10^-9$. A esto se ha añadido un \textit{scheduler} basado en la clase LambdaLR de PyTorch \parencite{pytorch_adjust_lr}, el cual permite modificar dinámicamente la tasa de aprendizaje en función del número de pasos de entrenamiento. En este caso, se ha definido una función que implementa una fase de warm-up de $1000$ iteraciones, durante la cual la tasa de aprendizaje aumenta linealmente desde 0 hasta su valor máximo inicial. Este procedimiento ayuda a estabilizar el entrenamiento en las primeras etapas, evitando actualizaciones abruptas de los parámetros cuando el modelo todavía no ha alcanzado una representación estable de los gradientes. Por último, otro cambio que se ha llevado a cabo ha sido cambiar el modo de normalización. Se estaba haciendo lo que se conoce como \texttt{Post-normalización}, es decir, la normalización se aplica después de cada subcapa (atención y feed-forward). Sin embargo, ciertos estudios sugieren que la \texttt{Pre-normalización}, donde la normalización se aplica antes de cada subcapa, puede mejorar la estabilidad del entrenamiento \parencite{doherty2024_hownot}.

Los nuevos parámetros del modelo son:
\begin{itemize}
    \item \textbf{Tamaño vocabulario}: $8000$
    \item \textbf{Tamaño embedding}: $256$
    \item \textbf{Tamaño contexto}: $128$
    \item \textbf{Tamaño de batches}: $64$
    \item \textbf{Tamaño FFNN}: $1024$
    \item \textbf{Número de batches por época}: $4146$
    \item \textbf{Número de cabezas de atención}: $8$
    \item \textbf{Número de capas}: $3$
    \item \textbf{Dropout}: $0.3$
\end{itemize}
Con ello, manteniendo el \textit{Early Stopping} se ha logrado un decaimiento progresivo del modelo en $40$ épocas, sin aparecer sobreajuste, obteniendo una pérdida de $3.8240$ y una perplejidad de $91.41$ en datos de validación y $45.79$ en entrenamiento. En la imagen \ref{fig:placeholder25} se ven mejor los resultados.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/resultado_entrenamiento_v2.png}
    \caption{Resultado primer entrenamiento}
    \label{fig:placeholder25}
\end{figure}

\section{Validación}
Para validar se utiliza el conjunto de datos reservados para tal fin ($35327$ tokens), los cuales no han sido vistos por el modelo durante el entrenamiento. Se obtiene una pérdida de $4.5039$ y una perplejidad de $90.37$, de media para un total de 550 lotes. Es buena señal pues indica que el modelo ha aprendido patrones generales y no se ha sobreajustado a los datos de entrenamiento.

Además de una validación con el conjunto de test, se ha implementado un método que consta de distintos hiperparámetros ajustables para generar texto a partir de unos tokens dados, como por ejemplo: "King Charles". Estos hiperparámetros son los siguientes \parencite{vyas2025transformer_decoder_part3}: 

\begin{itemize}
    \item Temperatura.
La temperatura ajusta la aleatoriedad en las respuestas del modelo al generar texto. Una temperatura baja hace que el modelo sea más determinista, eligiendo las palabras con mayor probabilidad, pues al dividir por un número menor a $1$ se amplifican las diferencias entre los valores, lo que puede resultar en respuestas repetitivas o predecibles. Por otro lado, una temperatura alta introduce más aleatoriedad, permitiendo al modelo explorar opciones menos probables y generando respuestas más variadas y creativas. En sí lo que hace es escalar los logits antes de calcular la distribución de probabilidades con la función \textit{softmax} del siguiente token. Matemáticamente, se expresa como:
    \[
        P(z_i) = \frac{e^{(z_i / T)}}{\sum_{j} e^{(w_j/ T)}}
    \]
Donde cada $z_i$ es un logit. Aquí a la hora de implementarlo se ha decidido que si la temperatura es $0.0$, no se aplica ninguna transformación, pues investigando he visto que se trataría como \textit{argmax}, es decir, siempre se elegiría la palabra con mayor probabilidad \parencite{vishnuraj2025temperature}.

    \item Top-k.
En lugar de escoger siempre el token con mayor probabilidad, lo que introduce respuestas repetitivas y poco naturales, con este método se seleccionan los \textit{k} tokens más probables y se normalizan sus probabilidades. Luego, se elige aleatoriamente entre estos \textit{k} tokens, lo que permite al modelo generar respuestas más variadas y creativas. La implementación se hace a través del método de \texttt{PyTorch} \textit{topk}, que devuelve los \textit{k} valores más altos de un tensor junto con sus índices. En este caso, se utiliza para filtrar los logits, manteniendo solo los \textit{k} más altos y estableciendo el resto a $-\infty$, lo que hace que su probabilidad sea prácticamente nula tras aplicar la función \textit{softmax}.

    \item Top-p (Núcleo de muestreo).
Se trata de otro método similar a Top-k. En este caso se ordenan los tokens por probabilidad hasta que la suma acumulada sea mayor o igual a un umbral \textit{p}. Luego, se normalizan las probabilidades de estos tokens seleccionados y se elige aleatoriamente entre ellos. Para la implementación, al igual que ocurre con el método anterior se ha seguido el ejemplo dado en \parencite{bsantraigi_topk_topp_batched}. En primer lugar se ordenan los logits de mayor a menor, a continuación, se calcula la suma acumulada de las probabilidades tras aplicar \textit{softmax} mediante la función \textit{cumsum}. Finalmente, se filtran los logits, estableciendo a $-\infty$ aquellos cuya suma acumulada supere el umbral \textit{p}, lo que hace que su probabilidad sea prácticamente nula tras aplicar la función \textit{softmax}.

    \item Número máximo de tokens a generar. Se establece un máximo número de tokens a generar para evitar que crezca indefinidamente la respuesta.
    
    \item Presence y frequency penalty \parencite{vyas2025transformer_decoder_part3}.
Se trata de dos métodos para reduucir o evitar la repetición de frases o tokens previos. El primero penaliza en base a si ha aparecido o no un token previamente y el segundo se basa en el número de veces que ha aparecido. Matemáticamente se expresa como: 
    \[
        z_i = z_i(w) - \text{presence\_penalty} \cdot \alpha - \text{frequency\_penalty} \cdot \beta
    \]

    donde $\alpha$ y $\beta$ son los coeficientes de penalización para presencia y frecuencia respectivamente.
A la hora de implementarlo se calcula esa expresión con tensores, $\alpha$ es un tensor binario que indica si el token ha aparecido o no y $\beta$ es un tensor con el número de veces que ha aparecido cada token obtenido a través del método \textit{bincount} que cuenta el número de ocurrencias de cada valor en un tensor de enteros no negativo.
\end{itemize}
\subsection{Resultados}

Se ha probado a generar texto con distintas combinaciones \ref{tab-combinaciones-tiny}, partiendo de unos mismos tokens iniciales: \textit{<<KING RICHARD:>>} y un número máximo de tokens a generar igual a $100$. En la tabla \ref{tab:resultados_validación_shakespeare} de la sección del anexo \ref{sec:res-tiny} se muestran los resultados.

\begin{table}[h]
    \centering
    \caption{Combinaciones de hiperparámetros para la generación de texto con Tiny Shakespeare}
    \label{tab:combinaciones-tiny}
    \begin{tabular}{l c c c c}
    \toprule
    \textbf{Temp} & \textbf{Top-k} & \textbf{Top-p} & \textbf{Presence Penalty} & \textbf{Frequency Penalty} \\
    \midrule
    0.5 & None & None & 0.0 & 0.0 \\
    1.0 & None & None & 0.0 & 0.0 \\
    1.5 & None & None & 0.0 & 0.0 \\
    1.0 & 10 & None & 0.0 & 0.0 \\
    1.0 & 90 & None & 0.0 & 0.0 \\
    1.0 & 200 & None & 0.0 & 0.0 \\
    1.0 & None & 0.05 & 0.0 & 0.0 \\
    1.0 & None & 0.30 & 0.0 & 0.0 \\
    1.0 & None & 0.90 & 0.0 & 0.0 \\
    1.0 & None & None & 0.2 & 0.0 \\
    1.0 & None & None & 0.5 & 0.0 \\
    1.0 & None & None & 1.0 & 0.0 \\
    1.0 & None & None & 0.0 & 0.2 \\
    1.0 & None & None & 0.0 & 0.5 \\
    1.0 & None & None & 0.0 & 1.0 \\
    0.9 & 100 & 0.95 & 0.3 & 0.2 \\
    0.7 & 80 & 0.9 & 0.2 & 0.3 \\
    \bottomrule
    \end{tabular}
\end{table}


En primer lugar se han probado distintas temperaturas. Se observa que a medida que la temperatura aumenta, la coherencia se pierde, aunque introduce más variedad. Por ejemplo, para una temperatura de $1.5$ introduce al personaje \textit{LOUCESTER}, cuando en realidad es \textit{GLOUCESTER}. Además, aparecen errores de sintáxis y palabras no existentes, como \textit{Ofigeia} o \textit{Theirrah} (parece una composición de \textit{sirrah} u \textit{sirrah}), que se pueden derivar de la tokenización BPE empleada. Con una temperatura baja, se enfatizan los logits más altos, haciendo que el modelo sea más conservador y emplee patrones comunes en el corpus de entrenamiento. 

Tras esto se han validado los parámetros \textit{top-k} y \textit{top-p}. Con un valor bajo de K, el texto se mantiene coherente, introduciendo nombres reales de personajes, pero aparecen repeticiones: \textit{<<to do not a man, I am not a man>>}. Al aumentar el valor de dicho parámetro se obtiene más variedad, y a diferencia de la temperatura, se mantiene una coherencia aceptable, sin neologismos, aunque las incongruencias y errores de sintáxis se hacen más notorios, lo que también puede atribuirse a un incremento de la verbosidad. Es cierto que aparecen, por ejemplo, casos como \textit{<<PETRUCHIO:SICINIUS:>>}, pero esto también ocurre en el texto de entrenamiento: \textit{<<DUKE VINCENTIO:KING RICHARD III:LEONTES:DUKE VINCENTIO:>>}. En el caso del parámetro P, la diferencia entre valores bajos y altos llama más la atención. Para un valor inferior al $1\%$ el texto resultante es un bucle de repeticiones de frases carentes de significado como tal: \textit{<<And I have been>>}. Al aumentar el valor aparecen frases con más sentido y variedad, pese a que empiezan a aparecer errores sintácticos como \textit{world’d}. Para un valor del $90\%$ introduce dialogos y diversidad, no obstante empieza a hacerse menos legible.

El parámetro de penalización de presencia reduce la repetición de palabras y al aumentar su valor hasta un $1.0$ se observa que introduce nuevos nombres de personajes, como \textit{ANGELO}, que no había aparecido hasta ahora en el resto de pruebas e introduce signos de exclamación e interrogación, haciendo el texto más natural, aunque los neologismos se mantienen en los tres fragmentos: \textit{goodinate}, \textit{nursetyaish}, \textit{awench} y la coherencia decae.

Finalmente, el parámetro de penalización de frecuencia muestra como al aumentar su valor se pierde el formato del texto (en versos) apareciendo frases mucho más largas, y a diferencia del caso anterior, se aprecia una mejora de coherencia para valores intermedios, aunque para el valor $1.0$ se ve como el uso de artículos es penalizado y se eliminan. 

Tras comprobar el efecto de cada parámetro se han probado tres combinaciones diferentes. 
En la primera de ellas prevalecen valores altos de top-k y top-p buscando variedad y creatividad, mientras que la temperatura se mantiene cercana a $1$. Las penalizaciones son bajas para no restringir demasiado la generación. Se obtiene un texto con buen cambio de ritmo, expresivo, mezclando personajes y manteniendo un estilo propio de Shakespeare como el texto de entrenamiento, sin neologismos. 

En la segunda evaluación se utiliza un valor más bajo de temperatura y disminuye el valor de P y K, lo que lo hace más determinista. Al reducir la penalización de presencia el modelo será más reacio a introducir nuevas palabras y al aumentar la penalización de frecuencia el modelo evitará repetir palabras. El resultado es un texto más coherente, aunque menos variado, sin neologismos o errores sintácticos importantes.

\subsubsection{Validación con MAUVE y Distinct-n}
Para validar la calidad y diversidad global del texto generado se han empleado las herramientas MAUVE y Distinct-n, de las que se recoge más información en el anexo. La primera, mide la coherencia, variedad y naturalidad del texto en base a una serie de ejemplos reales que son fragmentos del corpus de test. Se han hecho las pruebas inicialmente con cuatro fragmentos extraidos de manera aletoria compuestos por más de 14 frase cada uno. Para las prubas realizadas con Distinct-n se han obtenido los resultados para los valores $1$, $2$ y $3$ de \textit{n}, estos miden la diversidad de los textos, a mayor valor, mayor diversidad. Los resultados obtenidos se recogen en la siguiente tabla.
\begin{table}[h]
    \centering
    \caption{Combinaciones de hiperparámetros para la generación de texto con Tiny Shakespeare}
    \label{tab:res_mauve}
    \begin{tabular}{l c c c c c c c c c}
    \toprule
    \textbf{Fila} & \textbf{Temp} & \textbf{Top-k} & \textbf{Top-p} & \textbf{Presence Penalty} & \textbf{Frequency Penalty} & \textbf{Distinct-1} & \textbf{Distinct-2} & \textbf{Distinct-3} & \textbf{MAUVE}\\
    \midrule
    1 & 0.5 & None & None & 0.0 & 0.0 & 0.7541 & 0.9667 & 1.0000 & 0.974 \\
    2 & 1.0 & None & None & 0.0 & 0.0 & 0.9464 & 1.0000 & 1.0000 & 0.962\\
    3 & 1.5 & None & None & 0.0 & 0.0 & 0.9516 & 1.0000 & 1.0000 & 0.948\\
    4 & 1.0 & 10 & None & 0.0 & 0.0 & 0.6923 & 0.9531 & 0.9841 & 0.962 \\
    5 & 1.0 & 90 & None & 0.0 & 0.0 & 0.8889 & 1.0000 & 1.0000 & 0.948\\
    6 & 1.0 & 200 & None & 0.0 & 0.0 & 0.8548 & 1.0000 & 1.0000 & 0.980\\
    7 & 1.0 & None & 0.05 & 0.0 & 0.0 & 0.2090 & 0.3182 & 0.4154 & 0.392\\
    8 & 1.0 & None & 0.30 & 0.0 & 0.0 & 0.6949 & 0.9483 & 0.9825 & 0.958\\
    9 & 1.0 & None & 0.90 & 0.0 & 0.0 & 0.9153 & 1.0000 & 1.0000 & 0.975\\
    10 & 1.0 & None & None & 0.2 & 0.0 & 0.9538 & 1.0000 & 1.0000 & 0.971\\
    11 & 1.0 & None & None & 0.5 & 0.0 & 0.9219 & 1.0000 & 1.0000 & 0.975\\
    12 & 1.0 & None & None & 1.0 & 0.0 & 0.9324 & 1.0000 & 1.0000 & 0.971\\
    13 & 1.0 & None & None & 0.0 & 0.2 & 0.9672 & 1.0000 & 1.0000 & 0.973\\
    14 & 1.0 & None & None & 0.0 & 0.5 & 0.9167 & 1.0000 & 1.0000 & 0.962\\
    15 & 1.0 & None & None & 0.0 & 1.0 & 0.9710 & 1.0000 & 1.0000 & 0.977\\
    16 & 0.9 & 100 & 0.95 & 0.3 & 0.2 & 0.9412 & 1.0000 & 1.0000 & 0.977\\
    17 & 0.7 & 80 & 0.9 & 0.2 & 0.3 & 0.8438 & 0.9841 & 1.0000 & 0.979\\
    \bottomrule
    \end{tabular}
\end{table}

Por lo general la mayoría de los textos tienen un valor de MAUVE superior al $0.95$, lo que indica una alta similitud con los textos reales. Destaca el caso $7$, donde se aplica un valor de \textit{p} de $0.05$, con un valor de $0.392$. Se puede comprobar en la tabla \ref{tab:resultados_validacion_shakespeare} del anexo que es debido a que cada frase es una repetición casi idéntica de la anterior, careciendo de ningú ntipo de coherencia. Esto también se refleja en los valores de los n-gramas, que no superan el $0.5$. 

Los mejores resultados se han obtenido para las pruebas con un valor de \textit{k} de $200$, donde apenas se observa repetición y la similitud con los textos reales es alta, y para los últimos casos: temperatura constante y penalización de frecuencia igual a $1.0$ y los dos últimos casos donde se combinan hiperparámetros. El primero de esto tres últimos destaca, no solo por una alta similitud con los originales, sino que es el texto más fluido en base a las pruebas realizadas con Distinct-n. 

De estos resultados, se ha seleccionado los tres mejores, que se corresponderían con las filas 6, 15 y 17 de la tabla y se ha vuelto a calcular la métrica MAUVE incrementando el número de ejemplos reales a $10$, aumentando también la diversidad de los mismos. Se observa un ligero descenso del valor obtenido para el primer caso y último caso ($0.979$ y $0.974$ respectivamente), mientras que el segundo texto generado el valor aumenta a $0.978$. Los textos generados se pueden observar en la sección \ref{sec:res-tiny} del anexo, siendo el de mayor puntuación obtenida: 

\textit{KING RICHARD:}
\textit{But now must be the law,}
\textit{Thou hast you:}
\textit{Nay not be he were not?}
\textit{Or I's mine enemy in good lady't not in one and here.}
\textit{Whose repetition:}
\textit{Unations, sir, no}
\textit{So you: I will have no harm is my father's departure,}
\textit{As, well; and they joy}
\textit{YORK:}
\textit{I know to his fortunes of the time so I pray}


\subsubsection{Comparación con LSTM}

Para una primera validación se ha utilizado el mismo conjunto de test empleado en la arquitectura Transformer de Tiny Shakespeare. Los resultados obtenidos son peores que los que se obtuvieron para el modelo mencionado, con una périda de $4.62$ y un valor de $102.07$ de perplejidad, frente a los $4.5$ y $90.37$ obtenidos para el Transformer respectivamente.

Tras esto se ha generado texto ha partir del mismo prompt utilizado anteriormente: "KING RICHARD:", para un máximo de $100$ tokens. El resultado muestra una repetición de palabras continua, lo que se puede asociar a que se está escogiendo el mayor valor de los predichos como siguiente token: 

\textit{KING RICHARD:}
\textit{And I am not to be a man, and I have}
\textit{And, as I am, and I have not so much}
\textit{And, as I am, and I have not so much}
\textit{And, as I am, and I have not so much}
\textit{And, as I am, and I have not so much}
\textit{And, as I am, and I have not so much}
\textit{And, as I am, and I have not so much}

Se ha probado también a utilizar \texttt{Top-k} y ajustar la temperatura pero el resultado no ha mejorado.

\subsubsection{Validación a través de API}

Para agilizar la validación del modelo se ha implementado una API RESTful utilizando el framework \texttt{FastAPI} de Python. Esta API permite enviar solicitudes HTTP para generar texto a partir de un prompt inicial y los hiperparámetros definidos anteriormente. Las pruebas se realizan a través de \texttt{Postman}, y facilita ajustar los hiperparámetros dinámicamente y lanzar múltiples pruebas. Se ha expuesto el endpoint \textit{http://localhost:8011/LLM/api/v1/generar\_texto} localmente a través del puerto $8011$. En las imagenes \ref{fig:placeholder27} y \ref{fig:placeholder28} se muestra el \textit{curl} generado por \texttt{Postman} y una respuesta de ejemplo respectivamente.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/api_curl.png}
    \caption{Curl llamada API}
    \label{fig:placeholder27}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/api_res.png}
    \caption{Resultado llamada API}
    \label{fig:placeholder28}
\end{figure}
%-------------------------------
%--- FIN DEL CAPÍTULO 5

\clearemptydoublepage

\printbibliography[title={Referencias Bibliográficas}]

\appendix 
\chapter{Anexos}

\section{Preparación del entorno} \label{sec:entorno}

El primer paso es crear el entorno virtual para trabajar con las librerías de Python:
\begin{lstlisting}[language=bash]
python3 -m venv .venv
source .venv/bin/activate
\end{lstlisting}

El segundo punto consiste en instalar \texttt{CUDA}, \texttt{Pytorch} y \texttt{Tensorflow} y verificar que la GPU es detectada
\begin{lstlisting}[language=bash]
# Verificar CUDA, en mi caso tengo la version 12.9.
nvidia-smi

#Instalar torch para esa version de CUDA
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu129

#Instalar tensorflow
pip3 install "tensorflow[and-cuda]"

\end{lstlisting}

\begin{lstlisting}[language=Python]
import torch
print(torch.cuda.is_available())
print(torch.version.cuda)
print(torch.cuda.device_count())
\end{lstlisting}

Finalmente, se instalan el resto de librerías necesarias con el comando pip3.


\section{Gaussian Error Linear Unit (GELU)} \label{sec:gelu}
GELU es una función de activación introducida en 2016 que ha ganado popularidad por su capacidad para aumentar la capacidad de aprendizaje de las redes neuronales profundas. A diferencia de funciones tradicionales como ReLU o Sigmoid, GELU combina propiedades de ambas, permitiendo una activación suave y no lineal. 

En la imagen \ref{fig:placeholder32} se puede observar la comparación entre GELU y ReLU. Como se aprecia, la segunda tiene un corte abrupto en 0 y produce un resultado lineal para números negativos. A su vez, GELU, es no monótica, es decir, no siempre aumenta o disminuye, lo que puede ayudar a capturar relaciones complejas en los datos.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{img/gelu.png}
    \caption{GELU vs RELU \parencite{carr2025gelu}}
    \label{fig:placeholder32}
\end{figure}


\section{Xavier Uniform} \label{sec:xavier}

La inicialización Xvier o Glorot tiene como objetivo mantener la varianza de las activaciones y los gradientes a lo largo de las capas de una red neuronal, evitando problemas como el desvanecimiento o explosión del gradiente. Considera el número de neuronas en las capas de entrada y salida para determinar el rango adecuado para inicializar los pesos. Existen dos variantes de esta incialización, por un lado la uniforme y por otro lado la normal. En este caso se ha empleado la uniforme, que inicializa los pesos con valores muestreados de una distribución uniforme, frente a la normal que lo hace con una distribución Gaussiana, lo que conlleva más diversidad y puede ser menos estable. Mátematicamente lo que hace es:

\[
\text{x} = \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}
\]

Donde $\text{fan\_in}$ es el número de neuronas en la capa de entrada y $\text{fan\_out}$ es el número de neuronas en la capa de salida \parencite{codesignal_xavier_uniform_pytorch}. El valor constante $6$ se obtiene al igualar la varianza de una distribución uniforme, $\frac{a^2}{3}$, con la varianza objetivo de la inicialización Xavier, $\frac{2}{\text{fan\_in} + \text{fan\_out}}$, de modo que:

\[
\frac{x^2}{3} = \frac{2}{\text{fan\_in} + \text{fan\_out}} \quad \Rightarrow \quad x^2 = \frac{6}{\text{fan\_in} + \text{fan\_out}}
\]

Por lo tanto, los pesos se inicializan como $W \sim U(-x, x)$ con $x = \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}$ \parencite{365datascience_xavier_init}.


\section{Inicialización Normal} \label{sec:normal}

Este tipo de iniciallización inicializa los pesos con valores muestreados de una distribución Gaussiana (normal) con media cero y una desviación estándar específica. Esto significa que los valores de los pesos son aleatorios centrados en $0$ y la desviación controla que tan lejos peuden estar de dicho valor. Se ha empleado un valor de desviación estándar de $0.02$ pues es comunmente empleado en arquitecturas Transformer y permite mantener las activaciones estables en redes profundas.

\section{MAUVE} \label{sec:mauve}

Consiste en una librería de Python y Hugging Face que permite comparar la similitud o diferencia entre un texto generado por el LLM y el humano, capturando la calidad global y diversidad del texto generado. Lo que hace es representar los textos en un espacio de embeddings, los cuales se agrupan y se estiman sus distribuciones de probabilidad. Por último, utiliza divergencias de Kullback-Leibler para medir la similitud entre ambas distribuciones. Se tiene un valor de MAUVE alto si los textos generados son similares a los humanos en términos de coherencia, naturalidad y diversidad \parencite{pillutla2021mauve_software}.
\section{Distinct-n} \label{sec:distinct-n}

Se trata de una métrica que empleada para medir la diversidad de una frase. Se centra en el número de n-gramas únicos en relación con el número total de n-gramas en una secuencia, penalizando aquellas frases que contienen muchas palabras repetidas. Matemáticamente, se define como:
\[
\text{Distinct-n} = \frac{\text{Número de n-gramas únicos}}{\text{Número total de n-gramas}}
\]

Así, por ejemplo, en la frase "Mi perro come come mucho", para $n=2$, los bigramas son: "Mi perro", "perro come", "come come", "come mucho". Aquí, hay 4 bigramas en total, de los cuales 3 son únicos ("Mi perro", "perro come", "come mucho"). Por lo tanto, la métrica sería $\frac{3}{4} = 0.75$.


\section{Resultados obtenidos para Tiny Shakespeare} \label{sec:res-tiny}

A continuación se recogen los resultados obtenidos al validar el modelo entrenado con Tiny Shakespeare, partiendo del mismo prompt inicial: \textit{<<KING RICHARD:>>} y un máximo de $100$ tokens a generar.

\begin{longtable}{ccccc p{12cm}}
    \caption{Hiperparámetros y resultados de generación con Tiny Shakespeare}
    \label{tab:resultados_validacion_shakespeare} \\
    \toprule
    \textbf{Temp} & \textbf{Top-k} & \textbf{Top-p} &
    \textbf{Presence Penalty} & \textbf{Frequency Penalty} & \textbf{Resultado} \\
    \midrule
    \endfirsthead
    
    \multicolumn{6}{c}{{\tablename\ \thetable{} -- continuación}} \\
    \toprule
    \textbf{Temp} & \textbf{Top-k} & \textbf{Top-p} &
    \textbf{Presence Penalty} & \textbf{Frequency Penalty} & \textbf{Resultado} \\
    \midrule
    \endhead
    
    \bottomrule
    \multicolumn{6}{r}{} \\
    \endfoot
    
    \bottomrule
    \endlastfoot
    
    0.5 & None & None & 0.0 & 0.0 &
    KING RICHARD:\newline
    PETRUCHIO:\newline
    My brother, a thousand crowns,\newline
    And by the sun of my lord,\newline
    Iney’s the king,\newline
    And make the king’s wife,\newline
    And from the king,\newline
    You are sent for your brother.\newline
    In the day to be not, I am not say ’tis blood,\newline
    With all the right, and to be,\newline
    And in the wind of his,\newline
    Purder to the.\\
    
    1.0 & None & None & 0.0 & 0.0 &
    KING RICHARD:\newline
    A issue richer?\newline
    BRUTUS:\newline
    ired, poor high a\newline
    Thou art thou that a better man, girl?\newline
    'tis nothing but a squorn's wife, we'll thunder, and,\newline
    Come news?\newline
    Theirrah!\newline
    Less'd this outrage, Kate, sweet sir, the fished hath for the third our marks kines in estimation,\newline
    Ofigeia\newline
    There is no other tribute yet\newline
    What Eished\\
    
    1.5 & None & None & 0.0 & 0.0 &
    KING RICHARD:\newline
    Never charged thee other mistress repealll prove fair stateiness, slay?\newline
    LOUCESTER:\newline
    For your own thirst, father conclusion dead my sons;\newline
    And, shout: like from his mortal a piece Peter,\newline
    For night and leave report;\newline
    Pompey counsels amid treaty in myself lovely and favour runs within!\newline
    A below me confines nor doth depow tell thee less?\newline
    Imagine pates encothing\\
    
    1.0 & 10 & None & 0.0 & 0.0 &
    KING RICHARD:\newline
    Is to be\newline
    Madam with her:\newline
    To see your honour,\newline
    And I know not be a word.\newline
    That you\newline
    As my lord.\newline
    On if we must be gone, to do not a man, I am not a man,\newline
    MENENIUS:\newline
    That I will I am I\newline
    On this land's name,\newline
    I have I\newline
    When thou dost thou art a man of you, my father'\\
    
    1.0 & 90 & None & 0.0 & 0.0 &
    KING RICHARD:\newline
    Having the cause on'd the state:\newline
    KING RICHARD II:\newline
    'em of our enemies?\newline
    Their, do not to the same\newline
    Yea, then 'such;\newline
    ISABELLA:\newline
    With bag,\newline
    PETRUCHIO:\newline
    SICINIUS:\newline
    Could be your crown:\newline
    CORIOLANUS:\newline
    Fetch thy death hath been accused, my state and mine?\newline
    I's children should the world to live and good nurse\\
    
    1.0 & 200 & None & 0.0 & 0.0 &
    KING RICHARD:\newline
    But now must be the law,\newline
    Thou hast you:\newline
    Nay not be he were not?\newline
    Or I's mine enemy in good lady't not in one and here.\newline
    Whose repetition:\newline
    Unations, sir, no\newline
    So you: I will have no harm is my father's departure,\newline
    As, well; and they joy\newline
    YORK:\newline
    I know to his fortunes of the time so I pray\\
    
    1.0 & None & 0.05 & 0.0 & 0.0 &
    KING RICHARD:\newline
    And, and I have\newline
    And I have been\newline
    And I have been\newline
    And I have been\newline
    And I have been\newline
    And I have been\newline
    And, I have been\newline
    And I have been\newline
    And I have been\newline
    And I have been\newline
    And I'd, I have been a man, I have been\newline
    And I am a man, I have been\newline
    And I have you, I have done,\\
    
    1.0 & None & 0.30 & 0.0 & 0.0 &
    KING RICHARD:\newline
    If I have you to the way.\newline
    Which I am I am the poor soul,\newline
    And all the Capitol, and,\newline
    Within, sir,\newline
    And then I will not?\newline
    Why, the loss of the world'd,\newline
    To hear me.\newline
    Which he'd in the best of this,\newline
    I'd with the field\newline
    And all the king'd, and so far off.\newline
    If you\\
    
    0.9 & 100 & 0.95 & 0.3 & 0.2 &
    KING RICHARD:\newline
    And watch's his lands,\newline
    Within so?\newline
    With heigh!\newline
    First Citizen:\newline
    Against the word, I send you have met to take order.\newline
    But it down.\newline
    What if I would not so bright too, and Romeo should my holy love his wife,\newline
    To more proud that I'd for ever near with\newline
    The day! We must be king, here comes too long borne up in the ground!\newline
    I thank\\
    
    0.7 & 80 & 0.9 & 0.2 & 0.3 &
    KING RICHARD:\newline
    And make me, sir.\newline
    So I am I not to be hanged.\newline
    If what, for our mistress'd at the queen,\newline
    Nurse:\newline
    CAMILLO:\newline
    With all my lord; for his son:\newline
    Or do you all my daughter is the king's death?\newline
    And yet thou dost thou art not so; or two, and not well, when he hath but in the rest,\newline
    FRIAR LAURENCE:\newline
    H\\
    
    \end{longtable}
    

%--- FIN DE ANEXOS

\clearemptydoublepage

\end{document}





https://discuss.pytorch.org/t/how-to-use-collate-fn/27181
https://plainenglish.io/blog/understanding-collate-fn-in-pytorch-f9d1742647d3
https://blog.lukesalamone.com/posts/custom-pytorch-collate/

https://www.kaggle.com/code/shreydan/bpe-tokenizer-lstm-embeddings -> Implementación
\begin{lstlisting}[language=Python]

for batch_x, batch_y in loader:
    B, T = shape(batch_x)

    # Embeddings
    tok = TokenEmbedding(batch_x)                # [B, T, D]
    pos_ids = arange(T).expand(B, T)             # [B, T]
    pos = PosEmbedding(pos_ids)                  # [B, T, D]
    x = tok + pos                                # [B, T, D]

    # Repite N veces (capas del decoder)
    for layer in 1..N:
        # (Pre-Norm) Self-Attention enmascarada
        a = SelfAttention( LayerNorm(x), mask=causal(T) )   # [B, T, D]
        x = x + a                                           # residual

        # (Pre-Norm) Feed-Forward
        f = FeedForward( LayerNorm(x) )                     # [B, T, D]
        x = x + f                                           # residual

    # Proyección a vocabulario
    logits = Linear(x)                           # [B, T, vocab_size]

    # Pérdida (teacher forcing: batch_y es x desplazado +1)
    loss = CrossEntropy( logits.view(-1, vocab_size),
                         batch_y.view(-1) )

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()


for batch_x, batch_y in loader:
    tok_emb = embedding_layer(batch_x)                      # [B,T,D]
    pos_ids = arange(T).expand(B,T)
    pos_emb = pos_embedding_layer(pos_ids)                  # [B,T,D]
    x = tok_emb + pos_emb

    for _ in range(N):
        x = x + masked_multihead_self_attention( layer_norm(x) )
        x = x + feed_forward( layer_norm(x) )

    logits = linear(x)                                      # [B,T,V]
    loss = cross_entropy( logits.view(-1,V), batch_y.view(-1) )
    optimizer.zero_grad(); loss.backward(); optimizer.step()


    
\end{lstlisting}