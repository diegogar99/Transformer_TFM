% [1]
@online{phillips2019positional,
  author       = {Phillips, Hunter J.},
  title        = {Positional encoding},
  year         = {2019},
  month        = aug,
  url          = {https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6},
  organization = {Medium}
}

% [2]
@online{analytics2020qkv,
  author       = {{Analytics Vidhya}},
  title        = {Understanding Q, K, V in Transformer Self-Attention},
  year         = {2020},
  url          = {https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960},
  organization = {Medium}
}

% [3]
@online{epichka2023qkv,
  author       = {{Epichka}},
  title        = {QKV in Transformers},
  year         = {2023},
  url          = {https://epichka.com/blog/2023/qkv-transformer/},
  organization = {Epichka Blog}
}

% [4]
@online{soni2020masked,
  author       = {Soni, Sachin},
  title        = {Masked Multi-Head Attention in Transformer},
  year         = {2020},
  url          = {https://medium.com/@sachinsoni600517/masked-multi-head-attention-in-transformer-f3e096d56961},
  organization = {Medium}
}

% [5]
@online{huggingface_tinyshakespeare,
  author       = {{HuggingFace}},
  title        = {Tiny Shakespeare Dataset},
  year         = {2019},
  url          = {https://huggingface.co/datasets/karpathy/tiny_shakespeare},
  organization = {HuggingFace}
}

% [6]
@online{huggingface_wikitext,
  author       = {{HuggingFace}},
  title        = {WikiText Dataset},
  year         = {2019},
  url          = {https://huggingface.co/datasets/Salesforce/wikitext},
  organization = {HuggingFace}
}

% [7]
@online{autonlp2020linkedwikitext,
  author       = {{AutoNLP}},
  title        = {Linked Wikitext-2},
  year         = {2020},
  url          = {https://autonlp.ai/datasets/linked-wikitext-2},
  organization = {AutoNLP}
}

% [8]
@online{spotintelligence2023cleaning,
  author       = {{Spotintelligence}},
  title        = {Top 20 Essential Text Cleaning Techniques: Practical How-to Guide in Python},
  year         = {2023},
  month        = sep,
  day          = {18},
  url          = {https://spotintelligence.com/2023/09/18/top-20-essential-text-cleaning-techniques-practical-how-to-guide-in-python/},
  organization = {Spotintelligence}
}

% [9]
@online{ebimsv2021day2,
  author       = {Ebimsv},
  title        = {Day 2: Text preprocessing and tokenization},
  year         = {2021},
  url          = {https://medium.com/@ebimsv/day-2-text-preprocessing-and-tokenization-26ea25d3db0f},
  organization = {Medium}
}

% [10]
@online{shabbir2021cleaning,
  author       = {Shabbir, Sheeza},
  title        = {Text Cleaning in NLP: Libraries, Techniques and Getting Started},
  year         = {2021},
  url          = {https://medium.com/@datascientist_SheezaShabbir/text-cleaning-in-nlp-libraries-techniques-and-how-to-get-started-8c7c7e8ba7cf},
  organization = {Medium}
}

% [11]
@online{huggingface_tokenizer_summary,
  author       = {{HuggingFace}},
  title        = {Tokenizer Summary},
  year         = {2021},
  url          = {https://huggingface.co/docs/transformers/es/tokenizer_summary},
  organization = {HuggingFace Docs}
}

% [12]
@online{reddit2021tokenizers,
  author       = {{Reddit}},
  title        = {SentencePiece, WordPiece, BPE: which tokenizer is best?},
  year         = {2021},
  url          = {https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/?tl=es-es},
  organization = {Reddit r/MachineLearning}
}

% [13]
@online{lmpo2020bpe,
  author       = {LMPO},
  title        = {From text to tokens: Understanding BPE, WordPiece and SentencePiece in NLP},
  year         = {2020},
  url          = {https://medium.com/@lmpo/from-text-to-tokens-understanding-bpe-wordpiece-and-sentencepiece-in-nlp-1367d9d610af},
  organization = {Medium}
}

% [14]
@online{geeksforgeeks_bpe,
  author       = {{GeeksforGeeks}},
  title        = {Byte Pair Encoding (BPE) in NLP},
  year         = {2020},
  url          = {https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/},
  organization = {GeeksforGeeks}
}

% [15]
@online{google_sentencepiece,
  author       = {{Google}},
  title        = {SentencePiece},
  year         = {2018},
  url          = {https://github.com/google/sentencepiece},
  organization = {GitHub}
}

% [16]
@online{katiyar2021embedding,
  author       = {Katiyar, Smrati},
  title        = {Explaining Embedding Layer in PyTorch},
  year         = {2021},
  url          = {https://medium.com/@smrati.katiyar/explaining-embedding-layer-in-pytorch-1f22b88c1a69},
  organization = {Medium}
}

% [17]
@online{geeksforgeeks_embedding,
  author       = {{GeeksforGeeks}},
  title        = {Word Embedding in PyTorch},
  year         = {2021},
  url          = {https://www.geeksforgeeks.org/deep-learning/word-embedding-in-pytorch/},
  organization = {GeeksforGeeks}
}

% [18]
@online{bao2022embedding,
  author       = {Bao, Gary},
  title        = {How to use PyTorch’s nn.Embedding: A comprehensive guide with examples},
  year         = {2022},
  url          = {https://medium.com/@garybao/how-to-use-pytorchs-nn-embedding-a-comprehensive-guide-with-examples-da00ea42e952},
  organization = {Medium}
}

% [19]
@online{perez_embeddings,
  author       = {Pérez, J. A.},
  title        = {Embeddings en Transformers},
  year         = {2020},
  url          = {https://www.dlsi.ua.es/~japerez/materials/transformers/embeddings/},
  organization = {Universidad de Alicante}
}

% [20]
@online{neuraforge2023guide,
  author       = {{Neuraforge}},
  title        = {The Ultimate Guide to Preparing Text Data (Embeddings)},
  year         = {2023},
  url          = {https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text},
  organization = {Substack}
}

% [21]
@online{plainenglish2021gpt,
  author       = {{Plain English AI}},
  title        = {The 8 mathematical operations in GPT that accidentally created consciousness},
  year         = {2021},
  url          = {https://ai.plainenglish.io/the-8-mathematical-operations-in-gpt-that-accidentally-created-consciousness-ccaee58b241e},
  organization = {Medium}
}
% [22]
@inproceedings{vaswani2017attention,
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  title        = {Attention Is All You Need},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2017},
  pages        = {5998--6008},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher    = {Curran Associates, Inc.}
}

% [23]
@online{wolfe_llm_scaling,
  author       = {Wolfe, Cameron R.},
  title        = {Scaling Laws for LLMs: From GPT-3 to o3},
  year         = {2025},
  url          = {https://cameronrwolfe.substack.com/p/llm-scaling-laws},
  organization = {Substack}
}

% [24]
@online{swarms_masking_pytorch,
  author       = {{Swarms}},
  title        = {Understanding Masking in PyTorch for Attention Mechanisms},
  year         = {2021},
  url          = {https://medium.com/@swarms/understanding-masking-in-pytorch-for-attention-mechanisms-e725059fd49f},
  organization = {Medium}
}

% [25]
@online{paneru2025multihead,
  author       = {Paneru, Agrim},
  title        = {Implementing Multihead Attention from Scratch in PyTorch},
  year         = {2025},
  url          = {https://agrimpaneru.com.np/blog/multi-head-attention-pytorch/},
  organization = {Agrim Paneru Blog}
}

% [26]
@online{paneru2025selfattention,
  author       = {Paneru, Agrim},
  title        = {Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch},
  year         = {2025},
  url          = {https://agrimpaneru.com.np/blog/self-attention-pytorch/},
  organization = {Agrim Paneru Blog}
}

% [26]
@online{doshi2021transformers_visual_part2,
  author       = {Doshi, Ketan},
  title        = {Transformers Explained Visually (Part 2): How it works, step-by-step},
  year         = {2021},
  url          = {https://medium.com/data-science/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34},
  organization = {Medium}
}

% [27]
@online{kanaries_nnlinear,
  author       = {Matt Popovic},
  title        = {nn.Linear in PyTorch: Clearly Explained},
  year         = {2023},
  url          = {https://docs.kanaries.net/topics/Python/nn-linear},
  organization = {Kanaries Docs}
}

% [28]
@online{sharma2024addnorm,
  author       = {Sharma, Mohit},
  title        = {What is Add \& Norm, as quick as possible?},
  year         = {2024},
  url          = {https://molgorithm.medium.com/what-is-add-norm-as-soon-as-possible-178fc0836381},
  organization = {Medium}
}

% [29]
@online{apxml_addnorm,
  author       = {{ApXML}},
  title        = {Add \& Norm Layers (Residual Connections)},
  year         = {2025},
  url          = {https://apxml.com/courses/introduction-to-transformer-models/chapter-3-transformer-encoder-decoder-architecture/add-norm-layers},
  organization = {ApXML}
}

% [30] Implementación
@online{zhang2023d2l_transformer,
  author       = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  title        = {Transformer (en *Dive into Deep Learning*)},
  year         = {2023},
  url          = {https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html},
  organization = {Dive into Deep Learning}
}

% [31] REVISARRR
@online{aisagescribe_prepostnorm,
  author       = {{AI SageScribe}},
  title        = {Pre-Normalization vs. Post-Normalization in Transformers},
  year         = {2025},
  url          = {https://medium.com/@aisagescribe/pre-normalization-vs-post-normalization-in-transformers-e84872e0a3cd},
  organization = {Medium}
}

% [32] FFN is important to model performance.
@article{gerber2025ffn,
  author       = {Gerber, Isaac},
  title        = {Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models},
  journal      = {arXiv preprint arXiv:2505.06633v1},
  year         = {2025},
  url          = {https://arxiv.org/html/2505.06633v1},
  note         = {Submitted 10 May 2025}
}

% [33]
@online{kyeg_feedforward_demystified,
  author       = {Kye Gomez},
  title        = {The Feedforward Demystified: A Core Operation of Transformers},
  year         = {2023},
  url          = {https://medium.com/@kyeg/the-feedforward-demystified-a-core-operation-of-transformers-afcd3a136c4c},
  organization = {Medium}
}
