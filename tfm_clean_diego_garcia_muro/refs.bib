% [1]
@online{phillips2019positional,
  author       = {Phillips, Hunter J.},
  title        = {Positional encoding},
  year         = {2019},
  month        = aug,
  url          = {https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6},
  organization = {Medium}
}

% [2]
@online{analytics2020qkv,
  author       = {{Analytics Vidhya}},
  title        = {Understanding Q, K, V in Transformer Self-Attention},
  year         = {2020},
  url          = {https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960},
  organization = {Medium}
}

% [3]
@online{epichka2023qkv,
  author       = {{Epichka}},
  title        = {QKV in Transformers},
  year         = {2023},
  url          = {https://epichka.com/blog/2023/qkv-transformer/},
  organization = {Epichka Blog}
}

% [4]
@online{soni2020masked,
  author       = {Soni, Sachin},
  title        = {Masked Multi-Head Attention in Transformer},
  year         = {2020},
  url          = {https://medium.com/@sachinsoni600517/masked-multi-head-attention-in-transformer-f3e096d56961},
  organization = {Medium}
}

% [5]
@online{huggingface_tinyshakespeare,
  author       = {{HuggingFace}},
  title        = {Tiny Shakespeare Dataset},
  year         = {2019},
  url          = {https://huggingface.co/datasets/karpathy/tiny_shakespeare},
  organization = {HuggingFace}
}

% [6]
@online{huggingface_wikitext,
  author       = {{HuggingFace}},
  title        = {WikiText Dataset},
  year         = {2019},
  url          = {https://huggingface.co/datasets/Salesforce/wikitext},
  organization = {HuggingFace}
}

% [7]
@online{autonlp2020linkedwikitext,
  author       = {{AutoNLP}},
  title        = {Linked Wikitext-2},
  year         = {2020},
  url          = {https://autonlp.ai/datasets/linked-wikitext-2},
  organization = {AutoNLP}
}

% [8]
@online{spotintelligence2023cleaning,
  author       = {{Spotintelligence}},
  title        = {Top 20 Essential Text Cleaning Techniques: Practical How-to Guide in Python},
  year         = {2023},
  month        = sep,
  day          = {18},
  url          = {https://spotintelligence.com/2023/09/18/top-20-essential-text-cleaning-techniques-practical-how-to-guide-in-python/},
  organization = {Spotintelligence}
}

% [9]
@online{ebimsv2021day2,
  author       = {Ebimsv},
  title        = {Day 2: Text preprocessing and tokenization},
  year         = {2021},
  url          = {https://medium.com/@ebimsv/day-2-text-preprocessing-and-tokenization-26ea25d3db0f},
  organization = {Medium}
}

% [10]
@online{shabbir2021cleaning,
  author       = {Shabbir, Sheeza},
  title        = {Text Cleaning in NLP: Libraries, Techniques and Getting Started},
  year         = {2021},
  url          = {https://medium.com/@datascientist_SheezaShabbir/text-cleaning-in-nlp-libraries-techniques-and-how-to-get-started-8c7c7e8ba7cf},
  organization = {Medium}
}

% [11]
@online{huggingface_tokenizer_summary,
  author       = {{HuggingFace}},
  title        = {Tokenizer Summary},
  year         = {2021},
  url          = {https://huggingface.co/docs/transformers/es/tokenizer_summary},
  organization = {HuggingFace Docs}
}

% [12]
@online{reddit2021tokenizers,
  author       = {{Reddit}},
  title        = {SentencePiece, WordPiece, BPE: which tokenizer is best?},
  year         = {2021},
  url          = {https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/?tl=es-es},
  organization = {Reddit r/MachineLearning}
}

% [13]
@online{lmpo2020bpe,
  author       = {LMPO},
  title        = {From text to tokens: Understanding BPE, WordPiece and SentencePiece in NLP},
  year         = {2020},
  url          = {https://medium.com/@lmpo/from-text-to-tokens-understanding-bpe-wordpiece-and-sentencepiece-in-nlp-1367d9d610af},
  organization = {Medium}
}

% [14]
@online{geeksforgeeks_bpe,
  author       = {{GeeksforGeeks}},
  title        = {Byte Pair Encoding (BPE) in NLP},
  year         = {2020},
  url          = {https://www.geeksforgeeks.org/nlp/byte-pair-encoding-bpe-in-nlp/},
  organization = {GeeksforGeeks}
}

% [15]
@online{google_sentencepiece,
  author       = {{Google}},
  title        = {SentencePiece},
  year         = {2018},
  url          = {https://github.com/google/sentencepiece},
  organization = {GitHub}
}

% [16]
@online{katiyar2021embedding,
  author       = {Katiyar, Smrati},
  title        = {Explaining Embedding Layer in PyTorch},
  year         = {2021},
  url          = {https://medium.com/@smrati.katiyar/explaining-embedding-layer-in-pytorch-1f22b88c1a69},
  organization = {Medium}
}

% [17]
@online{geeksforgeeks_embedding,
  author       = {{GeeksforGeeks}},
  title        = {Word Embedding in PyTorch},
  year         = {2021},
  url          = {https://www.geeksforgeeks.org/deep-learning/word-embedding-in-pytorch/},
  organization = {GeeksforGeeks}
}

% [18]
@online{bao2022embedding,
  author       = {Bao, Gary},
  title        = {How to use PyTorch’s nn.Embedding: A comprehensive guide with examples},
  year         = {2022},
  url          = {https://medium.com/@garybao/how-to-use-pytorchs-nn-embedding-a-comprehensive-guide-with-examples-da00ea42e952},
  organization = {Medium}
}

% [19]
@online{perez_embeddings,
  author       = {Pérez, J. A.},
  title        = {Embeddings en Transformers},
  year         = {2020},
  url          = {https://www.dlsi.ua.es/~japerez/materials/transformers/embeddings/},
  organization = {Universidad de Alicante}
}

% [20]
@online{neuraforge2023guide,
  author       = {{Neuraforge}},
  title        = {The Ultimate Guide to Preparing Text Data (Embeddings)},
  year         = {2023},
  url          = {https://neuraforge.substack.com/p/the-ultimate-guide-to-preparing-text},
  organization = {Substack}
}

% [21]
@online{plainenglish2021gpt,
  author       = {{Plain English AI}},
  title        = {The 8 mathematical operations in GPT that accidentally created consciousness},
  year         = {2021},
  url          = {https://ai.plainenglish.io/the-8-mathematical-operations-in-gpt-that-accidentally-created-consciousness-ccaee58b241e},
  organization = {Medium}
}
% [22]
@inproceedings{vaswani2017attention,
  author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  title        = {Attention Is All You Need},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2017},
  pages        = {5998--6008},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher    = {Curran Associates, Inc.}
}

% [23]
@online{wolfe_llm_scaling,
  author       = {Wolfe, Cameron R.},
  title        = {Scaling Laws for LLMs: From GPT-3 to o3},
  year         = {2025},
  url          = {https://cameronrwolfe.substack.com/p/llm-scaling-laws},
  organization = {Substack}
}

% [24]
@online{swarms_masking_pytorch,
  author       = {{Swarms}},
  title        = {Understanding Masking in PyTorch for Attention Mechanisms},
  year         = {2021},
  url          = {https://medium.com/@swarms/understanding-masking-in-pytorch-for-attention-mechanisms-e725059fd49f},
  organization = {Medium}
}

% [25]
@online{paneru2025multihead,
  author       = {Paneru, Agrim},
  title        = {Implementing Multihead Attention from Scratch in PyTorch},
  year         = {2025},
  url          = {https://agrimpaneru.com.np/blog/multi-head-attention-pytorch/},
  organization = {Agrim Paneru Blog}
}

% [26]
@online{paneru2025selfattention,
  author       = {Paneru, Agrim},
  title        = {Self-Attention Explained: Implementing Scaled Dot-Product Attention with PyTorch},
  year         = {2025},
  url          = {https://agrimpaneru.com.np/blog/self-attention-pytorch/},
  organization = {Agrim Paneru Blog}
}

% [262]
@online{doshi2021transformers_visual_part2,
  author       = {Doshi, Ketan},
  title        = {Transformers Explained Visually (Part 2): How it works, step-by-step},
  year         = {2021},
  url          = {https://medium.com/data-science/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34},
  organization = {Medium}
}

% [27]
@online{kanaries_nnlinear,
  author       = {Matt Popovic},
  title        = {nn.Linear in PyTorch: Clearly Explained},
  year         = {2023},
  url          = {https://docs.kanaries.net/topics/Python/nn-linear},
  organization = {Kanaries Docs}
}

% [28]
@online{sharma2024addnorm,
  author       = {Sharma, Mohit},
  title        = {What is Add \& Norm, as quick as possible?},
  year         = {2024},
  url          = {https://molgorithm.medium.com/what-is-add-norm-as-soon-as-possible-178fc0836381},
  organization = {Medium}
}

% [29]
@online{apxml_addnorm,
  author       = {{ApXML}},
  title        = {Add \& Norm Layers (Residual Connections)},
  year         = {2025},
  url          = {https://apxml.com/courses/introduction-to-transformer-models/chapter-3-transformer-encoder-decoder-architecture/add-norm-layers},
  organization = {ApXML}
}

% [30] Implementación
@online{zhang2023d2l_transformer,
  author       = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  title        = {Transformer (en *Dive into Deep Learning*)},
  year         = {2023},
  url          = {https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html},
  organization = {Dive into Deep Learning}
}

% [31] REVISARRR
@online{aisagescribe_prepostnorm,
  author       = {{AI SageScribe}},
  title        = {Pre-Normalization vs. Post-Normalization in Transformers},
  year         = {2025},
  url          = {https://medium.com/@aisagescribe/pre-normalization-vs-post-normalization-in-transformers-e84872e0a3cd},
  organization = {Medium}
}

% [32] FFN is important to model performance.
@article{gerber2025ffn,
  author       = {Gerber, Isaac},
  title        = {Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models},
  journal      = {arXiv preprint arXiv:2505.06633v1},
  year         = {2025},
  url          = {https://arxiv.org/html/2505.06633v1},
  note         = {Submitted 10 May 2025}
}

% [33]
@online{kyeg_feedforward_demystified,
  author       = {Kye Gomez},
  title        = {The Feedforward Demystified: A Core Operation of Transformers},
  year         = {2023},
  url          = {https://medium.com/@kyeg/the-feedforward-demystified-a-core-operation-of-transformers-afcd3a136c4c},
  organization = {Medium}
}


% [34] Lo de los bucles para train con autocast y gradScaler
@online{stack_overflow_gradscaler,
  author       = {{StackOverflow users}},
  title        = {Is GradScaler necessary with Mixed Precision training with PyTorch?},
  year         = {2022},
  url          = {https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch},
  organization = {StackOverflow}
}


% [35] Mixed Precision Training no reduce la precisión del modelo. autocast y gradscaler. Tambien la estructura del for y eso. Torch.no_grad
@online{amit2024mixedprecision, 
  author       = {Amit, Hey},
  title        = {What Every User Should Know About Mixed Precision Training in PyTorch},
  year         = {2024},
  month        = nov,
  url          = {https://medium.com/data-scientists-diary/what-every-user-should-know-about-mixed-precision-training-in-pytorch-63c6544e5a05},
  organization = {Data Scientist’s Diary}
}

% [36] Cross-Entropy Loss
@online{fuster2024crossentropy,
  author       = {Fuster, Marina},
  title        = {Cross-Entropy Loss for Next Token Prediction in Transformers},
  year         = {2024, August 6},
  url          = {https://marinafuster.medium.com/cross-entropy-loss-for-next-token-prediction-83c684fa26d5},
  organization = {Medium}
}

% [37] Cross-Entropy Loss Function
@online{geeksforgeeks_crossentropy,
  author       = {{GeeksforGeeks}},
  title        = {What Is Cross-Entropy Loss Function?},
  year         = {2025},
  url          = {https://www.geeksforgeeks.org/machine-learning/what-is-cross-entropy-loss-function/},
  organization = {GeeksforGeeks}
}


% [38] Preplejidad, problemas y otras metricas
@online{singh2024perplexitymatters,
  author       = {Singh, Deepankar},
  title        = {Why Perplexity Matters: A Deep Dive into NLP’s Fluency Metric},
  year         = {2024},
  month        = nov,
  url          = {https://medium.com/ai-enthusiast/why-perplexity-matters-a-deep-dive-into-nlps-fluency-metric-c3872811d598},
  organization = {Medium}
}


% [39] Entropía, perplejidad, BPB, BPC
@online{keerthanams2025evaluating,
  author       = {Keerthanams},
  title        = {Evaluating AI Models — Understanding Entropy, Perplexity, BPB, and BPC},
  year         = {2025, May 8},
  url          = {https://medium.com/@keerthanams1208/evaluating-ai-models-understanding-entropy-perplexity-bpb-and-bpc-df816062f21a},
  organization = {Medium}
}

% [40]  MAUVE metric git 
@misc{krishnap25_mauve_github,
  author       = {Pillutla, Krishna and colaboradores},
  title        = {mauve: Package to compute MAUVE, a similarity score between neural text and human text},
  howpublished = {\url{https://github.com/krishnap25/mauve}},
  year         = {2025},
  note         = {GitHub repository}
}

% [41] Hugging Face mauve
@online{huggingface_mauve_space,
  author       = {{HuggingFace}},
  title        = {MAUVE — Hugging Face Space (evaluate-metric)},
  year         = {2025},
  url          = {https://huggingface.co/spaces/evaluate-metric/mauve},
  organization = {HuggingFace Spaces}
}

% [41] MAUVE implementation and explanation, creo que es como el de git
@online{pillutla_mauve_docs,
  author       = {Pillutla, Krishna and colaboradores},
  title        = {MAUVE: Measuring the Gap Between Neural Text and Human Text},
  year         = {2021},
  url          = {https://krishnap25.github.io/mauve/},
  organization = {MAUVE Project}
}

% [42] Distinct-N metric git
@misc{distinctn_github,
  author       = {{neural-dialogue-metrics}},
  title        = {Distinct-N: Compute the Distinct-N metric proposed by Jiwei Li et al.},
  howpublished = {\url{https://github.com/neural-dialogue-metrics/Distinct-N}},
  year         = {2025},
  note         = {GitHub repository}
}

% [43] Implementar perplexity
@online{stackoverflow_perplexity_pytorch,
  author       = {{StackOverflow users}},
  title        = {Calculate perplexity in PyTorch},
  year         = {2019},
  url          = {https://stackoverflow.com/questions/59209086/calculate-perplexity-in-pytorch},
  organization = {StackOverflow}
}


% [44] Adamw, uso en gpt, generaliza mejor, impl y ajuste hiperparams
@online{datacamp_adamw_pytorch,
  author       = {{DataCamp}},
  title        = {AdamW Optimizer in PyTorch Tutorial},
  year         = {2024},
  url          = {https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch},
  organization = {DataCamp}
}


% [45] Impulso y lr adaptativo
@online{yassin2024adamvsadamw,
  author       = {Yassin, Ahmed},
  title        = {Adam vs. AdamW: Understanding Weight Decay and Its Impact on Model Performance},
  year         = {2024, Nov 8},
  url          = {https://yassin01.medium.com/adam-vs-adamw-understanding-weight-decay-and-its-impact-on-model-performance-b7414f0af8a1},
  organization = {Medium}
}


% [46] SGD
@online{geeksforgeeks_sgd,
  author       = {{GeeksforGeeks}},
  title        = {ML – Stochastic Gradient Descent (SGD)},
  year         = {2025},
  url          = {https://www.geeksforgeeks.org/machine-learning/ml-stochastic-gradient-descent-sgd/},
  organization = {GeeksforGeeks}
}

% [47] SGD problemas/ventajas
@online{ultralytics_adam_glossary,
  author       = {{Ultralytics}},
  title        = {Optimizador Adam ‒ Glosario Ultralytics},
  year         = {2025},
  url          = {https://www.ultralytics.com/es/glossary/adam-optimizer},
  organization = {Ultralytics}
}

% [48] RMSProp
@online{kashyap2024rmsprop,
  author       = {Kashyap, Piyush},
  title        = {Understanding RMSProp: A Simple Guide to One of Deep Learning’s Powerful Optimizers},
  year         = {2024, Nov 2},
  url          = {https://medium.com/@piyushkashyap045/understanding-rmsprop-a-simple-guide-to-one-of-deep-learnings-powerful-optimizers-403baeed9922},
  organization = {Medium}
}

% [49] SGD Impl
@online{stackoverflow_sgd_optimizer,
  author       = {{StackOverflow users}},
  title        = {Is SGD optimizer in PyTorch actually does Gradient Descent algorithm?},
  year         = {2022},
  url          = {https://stackoverflow.com/questions/72496224/is-sgd-optimizer-in-pytorch-actually-does-gradient-descent-algorithm},
  organization = {StackOverflow},
  note         = {Answer explaining the relation between SGD, mini-batch, and full gradient descent in PyTorch}
}


% [50] Pytorch lr_scheduler
@online{pytorch_adjust_lr,
  author       = {{PyTorch}},
  title        = {How to adjust learning rate — torch.optim},
  year         = {2025},
  url          = {https://pytorch.org/docs/stable/optim.html\#how-to-adjust-learning-rate},
  organization = {PyTorch Documentation}
}

% [51] CosineAnnealingLR
@online{pytorch_cosineannealinglr,
  author       = {{PyTorch}},
  title        = {CosineAnnealingLR --- torch.optim.lr\_scheduler},
  year         = {2025},
  url          = {https://docs.pytorch.org/docs/stable/generated/torch.optim.lr\_scheduler.CosineAnnealingLR.html},
  organization = {PyTorch Documentation}
}


% [52]CosineAnnealingLR get_lr
@misc{pytorch_lr_scheduler_source,
  author       = {{PyTorch developers}},
  title        = {torch.optim.lr\_scheduler source code in PyTorch v2.8.0},
  url          = {https://github.com/pytorch/pytorch/blob/v2.8.0/torch/optim/lr_scheduler.py\#L1023},
  year         = {2025},
  note         = {Línea de código relevante}
}


% [53]  Lo de ajustar lr durante el entrenamiento
@online{reddit_lr_schedule_choice,
  author       = {{Reddit users}},
  title        = {How does one choose a learning rate schedule for models that take days or weeks to train?},
  year         = {2023},
  url          = {https://www.reddit.com/r/MachineLearning/comments/xeyzf7/d_how_does_one_choose_a_learning_rate_schedule/},
  organization = {Reddit r/MachineLearning}
}


% [54] Baja a 3 capas de decoder y empeiza a funcionar, ademas usa 16 batch size y optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

@online{reddit_transformer_not_learning,
  author       = {{Reddit users}},
  title        = {Why isn’t my Transformer model learning? (thread)},
  year         = {2023},
  url          = {https://www.reddit.com/r/machinetranslation/comments/1dmjbrj/why_isnt_my_transformer_model_learning/},
  organization = {Reddit r/MachineTranslation}
}

% [55] Transformers without Tears: Improving the Normalization of Self-Attention para PRE NORM
@online{doherty2024_hownot,
  author       = {Doherty, Christopher},
  title        = {How Not To Train Your Transformer},
  year         = {2024, Nov 7},
  url          = {https://medium.com/@christopherdoherty14/how-not-to-train-your-transformer-7e63011a16eb},
  organization = {Medium}
}

% [56]
@online{vyas2025transformer_decoder_part3,
  author       = {Vyas, Pratik},
  title        = {Transformer: Decoder Only Generative Model (Part-3)},
  year         = {2025-04-16},
  url          = {https://medium.com/@pratik.vyas\_10544/transformer-decoder-only-generative-model-part-3-0b38c10c2ae9},
  organization = {Medium}
}


% [57]
@online{vishnuraj2025temperature,
  author       = {Vishnu Raj, Madasu},
  title        = {Mathematics Behind the Temperature in LLM},
  year         = {2025, Jul 27},
  url          = {https://medium.com/@madasuvishnuraj/mathematics-behind-the-temperature-in-llm-cfb23120ac62},
  organization = {Medium}
}
% [58]
@online{pytorch_topk,
  author       = {{PyTorch}},
  title        = {torch.topk - PyTorch Documentation},
  year         = {2025},
  url          = {https://pytorch.org/docs/stable/generated/torch.topk.html},
  organization = {PyTorch Documentation}
}

% [59]
@misc{bsantraigi_topk_topp_batched,
  author       = {Santraigi, Bsantraigi},
  title        = {Batched top-k and top-p / nucleus sampling in PyTorch},
  url          = {https://gist.github.com/bsantraigi/5752667525d88d375207f099bd78818b},
  year         = {2025},
  note         = {Gist con implementación en PyTorch de top-k y top-p filtrado}
}

% [60] LSTM
@online{sharmasaravanan2024textgen_lstm,
  author       = {Sharmasaravanan},
  title        = {Text Generation Using LSTM: A Step-by-Step Guide},
  year         = {2024, Oct 15},
  url          = {https://sharmasaravanan.medium.com/text-generation-using-lstm-a-step-by-step-guide-9b787467f9de},
  organization = {Medium}
}

% [61] Vocab size gpt3
@online{paul2025vocabsize,
  author       = {Paul, Rohan},
  title        = {Balancing Vocabulary Size in Modern LLMs (GPT-4, LLaMA, Mistral)},
  year         = {2025, Mar 2},
  url          = {https://www.rohan-paul.com/p/tutorial-balancing-vocabulary-size},
  organization = {Rohan Paul Blog}
}
