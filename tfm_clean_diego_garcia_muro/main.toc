\babel@toc {spanish}{}\relax 
\contentsline {chapter}{Resumen}{\es@scroman {iii}}{chapter*.2}%
\contentsline {chapter}{Abstract}{\es@scroman {v}}{chapter*.3}%
\contentsline {chapter}{\numberline {1}Introducción y antecedentes}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Procesamiento del lenguaje natural}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Modelos previos: RNN y LSTM}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}De modelos secuenciales a mecanismos de atención}{2}{section.1.3}%
\contentsline {chapter}{\numberline {2}Objetivos del proyecto}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Objetivos específicos}{5}{section.2.1}%
\contentsline {chapter}{\numberline {3}Material y métodos}{7}{chapter.3}%
\contentsline {section}{\numberline {3.1}PyTorch y Tensorflow}{7}{section.3.1}%
\contentsline {section}{\numberline {3.2}Flask}{7}{section.3.2}%
\contentsline {section}{\numberline {3.3}Librerías y entorno utilizado}{7}{section.3.3}%
\contentsline {section}{\numberline {3.4}Tokenización Byte Pair Encoding}{8}{section.3.4}%
\contentsline {section}{\numberline {3.5}Embeddings}{10}{section.3.5}%
\contentsline {section}{\numberline {3.6}Arquitectura Transformer}{11}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Positional encoding}{11}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Masked Multi-head Attention}{11}{subsection.3.6.2}%
\contentsline {subsubsection}{Query (Q)}{12}{section*.7}%
\contentsline {subsubsection}{Key (K)}{12}{section*.8}%
\contentsline {subsubsection}{Value (V)}{12}{section*.9}%
\contentsline {subsection}{\numberline {3.6.3}Add \& Norm \blx@tocontentsinit {0}\parencite {sharma2024addnorm}}{13}{subsection.3.6.3}%
\contentsline {subsubsection}{Conexión Residual (\textit {Add})}{13}{section*.10}%
\contentsline {subsubsection}{Normalización (\textit {Layer Norm})}{14}{section*.12}%
\contentsline {subsection}{\numberline {3.6.4}Feed Forward}{15}{subsection.3.6.4}%
\contentsline {subsection}{\numberline {3.6.5}Linear y softmax}{16}{subsection.3.6.5}%
\contentsline {chapter}{\numberline {4}Resultados}{17}{chapter.4}%
\contentsline {section}{\numberline {4.1}Adquisión y análisis de las fuentes de datos}{17}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Tiny Shakespeare}{17}{subsection.4.1.1}%
\contentsline {subsubsection}{EDA}{18}{section*.14}%
\contentsline {subsection}{\numberline {4.1.2}WikiText2}{18}{subsection.4.1.2}%
\contentsline {subsubsection}{EDA}{18}{section*.15}%
\contentsline {section}{\numberline {4.2}Limpieza y tokenización}{18}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Implementación del tokenizador BPE}{18}{subsection.4.2.1}%
\contentsline {section}{\numberline {4.3}Embeddings y Positional Encodding}{20}{section.4.3}%
\contentsline {section}{\numberline {4.4}Mecanismos de atención}{21}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Implementación de las múltiples cabezas de atención}{22}{subsection.4.4.1}%
\contentsline {section}{\numberline {4.5}Normalización \texttt {(Add \& Norm)}}{23}{section.4.5}%
\contentsline {section}{\numberline {4.6}Feed Forward}{24}{section.4.6}%
\contentsline {section}{\numberline {4.7}Entrenamiento}{24}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Configuración de parámetros de entrenamiento}{25}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Elementos regulatorios}{25}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}Función de pérdida}{26}{subsection.4.7.3}%
\contentsline {subsection}{\numberline {4.7.4}Optimizadores}{27}{subsection.4.7.4}%
\contentsline {subsection}{\numberline {4.7.5}Resumen del modelo para Tiny Shakespeare}{28}{subsection.4.7.5}%
\contentsline {section}{\numberline {4.8}LSTM}{29}{section.4.8}%
\contentsline {chapter}{\numberline {5}Conclusiones}{31}{chapter.5}%
\contentsline {section}{\numberline {5.1}Primera aproximación con Tiny Shakespeare}{31}{section.5.1}%
\contentsline {section}{\numberline {5.2}Validación}{33}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Resultados}{34}{subsection.5.2.1}%
\contentsline {subsubsection}{Comparación con LSTM}{36}{section*.38}%
\contentsline {subsubsection}{Validación a través de API}{36}{section*.39}%
\contentsline {chapter}{\numberline {A}Anexos}{41}{appendix.Alph1}%
\contentsline {section}{\numberline {A.1}Preparación del entorno}{41}{section.Alph1.1}%
\contentsline {section}{\numberline {A.2}Moses}{42}{section.Alph1.2}%
\contentsline {section}{\numberline {A.3}GELU}{42}{section.Alph1.3}%
\contentsline {section}{\numberline {A.4}Xavier Uniform}{42}{section.Alph1.4}%
\contentsline {section}{\numberline {A.5}Normal uniform}{42}{section.Alph1.5}%
\contentsline {section}{\numberline {A.6}Entropía}{42}{section.Alph1.6}%
\contentsline {section}{\numberline {A.7}MAUVE}{42}{section.Alph1.7}%
\contentsline {section}{\numberline {A.8}Distinct-n}{42}{section.Alph1.8}%
\contentsline {section}{\numberline {A.9}Resultados obtenidos para Tiny Shakespeare}{42}{section.Alph1.9}%
