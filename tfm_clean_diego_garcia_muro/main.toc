\babel@toc {spanish}{}\relax 
\contentsline {chapter}{Resumen}{\es@scroman {iii}}{chapter*.2}%
\contentsline {chapter}{Abstract}{\es@scroman {v}}{chapter*.3}%
\contentsline {chapter}{\numberline {1}Introducción y antecedentes}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Procesamiento del lenguaje natural}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Modelos previos: RNN y LSTM}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}De modelos secuenciales a mecanismos de atención}{2}{section.1.3}%
\contentsline {chapter}{\numberline {2}Objetivos del proyecto}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Objetivos específicos}{5}{section.2.1}%
\contentsline {chapter}{\numberline {3}Material y métodos}{7}{chapter.3}%
\contentsline {section}{\numberline {3.1}PyTorch y Tensorflow}{7}{section.3.1}%
\contentsline {section}{\numberline {3.2}BPE}{7}{section.3.2}%
\contentsline {section}{\numberline {3.3}Flask}{7}{section.3.3}%
\contentsline {section}{\numberline {3.4}Arquitectura Transformer}{7}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Positional encoding}{7}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Masked Multi-head Attention}{8}{subsection.3.4.2}%
\contentsline {subsubsection}{Query (Q)}{8}{section*.5}%
\contentsline {subsubsection}{Key (K)}{8}{section*.6}%
\contentsline {subsubsection}{Value (V)}{8}{section*.7}%
\contentsline {subsection}{\numberline {3.4.3}Add \& Norm \blx@tocontentsinit {0}\parencite {sharma2024addnorm}}{9}{subsection.3.4.3}%
\contentsline {subsubsection}{Conexión Residual (\textit {Add})}{9}{section*.8}%
\contentsline {subsubsection}{Normalización (\textit {Layer Norm})}{10}{section*.10}%
\contentsline {subsection}{\numberline {3.4.4}Feed Forward}{11}{subsection.3.4.4}%
\contentsline {subsection}{\numberline {3.4.5}Linear y softmax}{12}{subsection.3.4.5}%
\contentsline {chapter}{\numberline {4}Resultados}{13}{chapter.4}%
\contentsline {section}{\numberline {4.1}Preparación del entorno}{13}{section.4.1}%
\contentsline {section}{\numberline {4.2}Análisis de los dataset}{14}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Tiny Shakespeare}{14}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}WikiText2}{14}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}Limpieza y tokenización}{14}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Implementación del tokenizador BPE}{16}{subsection.4.3.1}%
\contentsline {paragraph}{Ejemplo:}{18}{section*.16}%
\contentsline {paragraph}{Agrupación en lotes y padding:}{18}{section*.17}%
\contentsline {section}{\numberline {4.4}Embeddings y Positional Encodding}{19}{section.4.4}%
\contentsline {subsubsection}{Implementación embeddings}{20}{section*.19}%
\contentsline {section}{\numberline {4.5}Mecanismos de atención}{21}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Implementación mecanismo de atención}{21}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Implementación de las múltiples cabezas de atención}{23}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}Normalización \texttt {(Add \& Norm)}}{23}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Implementación Add \& Norm}{24}{subsection.4.6.1}%
\contentsline {section}{\numberline {4.7}Feed Forward}{24}{section.4.7}%
\contentsline {section}{\numberline {4.8}Entrenamiento}{25}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Configuración de parámetros de entrenamiento}{25}{subsection.4.8.1}%
\contentsline {subsection}{\numberline {4.8.2}Regulaciones}{26}{subsection.4.8.2}%
\contentsline {subsection}{\numberline {4.8.3}Función de pérdida}{27}{subsection.4.8.3}%
\contentsline {subsection}{\numberline {4.8.4}Optimizadores}{28}{subsection.4.8.4}%
\contentsline {subsection}{\numberline {4.8.5}Resumen del modelo para Tiny Shakespeare}{29}{subsection.4.8.5}%
\contentsline {section}{\numberline {4.9}LSTM}{30}{section.4.9}%
\contentsline {chapter}{\numberline {5}Conclusiones}{31}{chapter.5}%
\contentsline {section}{\numberline {5.1}Primera aproximación con Tiny Shakespeare}{31}{section.5.1}%
\contentsline {section}{\numberline {5.2}Validación}{33}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Resultados}{34}{subsection.5.2.1}%
\contentsline {subsubsection}{Comparación con LSTM}{44}{section*.38}%
\contentsline {subsubsection}{Validación a través de API}{44}{section*.39}%
\contentsline {chapter}{\numberline {A}Anexos}{49}{appendix.Alph1}%
\contentsline {section}{\numberline {A.1}GELU}{49}{section.Alph1.1}%
\contentsline {section}{\numberline {A.2}Xavier Uniform}{49}{section.Alph1.2}%
\contentsline {section}{\numberline {A.3}Normal uniform}{49}{section.Alph1.3}%
\contentsline {section}{\numberline {A.4}Entropía}{49}{section.Alph1.4}%
\contentsline {section}{\numberline {A.5}MAUVE}{49}{section.Alph1.5}%
\contentsline {section}{\numberline {A.6}Distinct-n}{49}{section.Alph1.6}%
